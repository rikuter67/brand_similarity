import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import os
import warnings
from location_bias_reranking import LocationBasedSimilarityReranker
from integrated_dimensionality_reduction import IntegratedDimensionalityReduction

warnings.filterwarnings('ignore') 

# Configuration constants
USE_ANCHOR_BASED = False  # Set to True to use anchor-based UMAP

class LLMStyleEmbeddingAnalyzer:
    def __init__(self):
        print(f"ğŸš€ Initializing LLM Style Embedding Analyzer")
        print(f"ğŸ’¡ Focus on advanced brand similarity analysis with reranking")
        
        self.df = None
        self.embeddings = None
        self.similarity_matrix = None
        self.location_reranker = None
        self.setup_location_reranker()
    
    def setup_location_reranker(self):
        """ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚«ãƒ¼ã®åˆæœŸåŒ–"""
        try:
            maps_csv_path = "datasets/bline_similarity/maps.csv"
            tenants_csv_path = "datasets/bline_similarity/tenants.csv"
            
            if os.path.exists(maps_csv_path):
                self.location_reranker = LocationBasedSimilarityReranker(
                    maps_csv_path=maps_csv_path,
                    tenants_csv_path=tenants_csv_path if os.path.exists(tenants_csv_path) else None
                )
                print(f"âœ… ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
            else:
                print(f"âš ï¸  ä½ç½®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™")
                self.location_reranker = None
        except Exception as e:
            print(f"âš ï¸  ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚«ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.location_reranker = None
    
    def load_data(self, csv_path):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“Š Loading data from: {csv_path}")
        self.df = pd.read_csv(csv_path)

        if 'ãƒ–ãƒ©ãƒ³ãƒ‰å' in self.df.columns and 'name' not in self.df.columns:
            self.df.rename(columns={'ãƒ–ãƒ©ãƒ³ãƒ‰å': 'name'}, inplace=True)
            print("Renamed 'ãƒ–ãƒ©ãƒ³ãƒ‰å' column to 'name'.")
        
        if 'bline_id' in self.df.columns and 'id' not in self.df.columns:
            self.df.rename(columns={'bline_id': 'id'}, inplace=True)
            print("Renamed 'bline_id' column to 'id'.")
        elif 'id' not in self.df.columns:
            self.df['id'] = range(len(self.df))
            print("Created 'id' column as it was not found.")

        print(f"âœ… Loaded {len(self.df)} brands")
        desc_lengths = self.df['description'].str.len()
        print(f"ğŸ“ˆ Description analysis:")
        print(f"   - Mean length: {desc_lengths.mean():.1f} chars")
        print(f"   - Median length: {desc_lengths.median():.1f} chars")
        print(f"   - Range: {desc_lengths.min()}-{desc_lengths.max()} chars")
        
        return self.df
    
    def load_embeddings(self, embeddings_path=None):
        """ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if embeddings_path is None:
            embeddings_path = "./ruri_embeddings_results/ruri_description_embeddings_v3_raw_hub.npy"
        
        print(f"ğŸ§  Loading embeddings from: {embeddings_path}")
        
        if not os.path.exists(embeddings_path):
            raise FileNotFoundError(f"Embeddings file not found: {embeddings_path}")

        self.embeddings = np.load(embeddings_path)
        
        if self.df is not None and self.embeddings.shape[0] != len(self.df):
            print(f"âš ï¸ Embedding count ({self.embeddings.shape[0]}) != DataFrame count ({len(self.df)})")
            min_count = min(self.embeddings.shape[0], len(self.df))
            self.embeddings = self.embeddings[:min_count]
            self.df = self.df.iloc[:min_count].copy()
            print(f"ğŸ”§ Adjusted to {min_count} items")

        print(f"âœ… Embeddings loaded: {self.embeddings.shape}")
        return self.embeddings
    
    def calculate_similarity_matrix(self):
        """é¡ä¼¼åº¦è¡Œåˆ—ã®è¨ˆç®—"""
        print(f"ğŸ”¢ Calculating similarity matrix...")
        
        if self.embeddings is None:
            print("âŒ Embeddings not loaded. Please load embeddings first.")
            return None

        self.similarity_matrix = cosine_similarity(self.embeddings)
        print(f"âœ… Similarity matrix calculated: {self.similarity_matrix.shape}")
        
        # Statistics
        off_diagonal = self.similarity_matrix[~np.eye(self.similarity_matrix.shape[0], dtype=bool)]
        print(f"ğŸ“Š Mean similarity: {off_diagonal.mean():.4f} Â± {off_diagonal.std():.4f}")
        
        return self.similarity_matrix
    
    def find_similar_brands(self, brand_name, top_k=10, min_similarity=0.0, use_location_rerank=True, location_bias_strength=0.3):
        """é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ï¼ˆãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾å¿œï¼‰"""
        try:
            brand_matches = self.df[self.df['name'] == brand_name]
            if len(brand_matches) == 0:
                print(f"âŒ Brand '{brand_name}' not found")
                similar_names = self.df['name'].str.contains(brand_name, case=False, na=False)
                if similar_names.any():
                    suggestions = self.df[similar_names]['name'].head(3).tolist()
                    print(f"ğŸ’¡ Did you mean: {', '.join(suggestions)}")
                return []
            
            brand_idx = brand_matches.index[0]
            similarities = self.similarity_matrix[brand_idx]
            
            # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢åé›†
            similarity_scores = {}
            for i, sim_score in enumerate(similarities):
                if i != brand_idx and sim_score >= min_similarity:
                    brand_name_i = self.df.iloc[i]['name']
                    similarity_scores[brand_name_i] = sim_score
            
            # ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨
            if use_location_rerank and self.location_reranker is not None:
                print(f"ğŸª Applying location-based reranking...")
                reranked_scores = self.location_reranker.rerank_similarity_with_location_bias(
                    similarity_scores=similarity_scores,
                    query_brand=brand_name,
                    bias_strength=location_bias_strength,
                    location_method='comprehensive',
                    rerank_mode='weighted_average'
                )
                final_scores = reranked_scores
            else:
                final_scores = similarity_scores
            
            # ãƒˆãƒƒãƒ—Kçµæœã‚’è¿”ã™
            sorted_brands = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
            
            results = []
            for rank, (similar_brand_name, final_score) in enumerate(sorted_brands[:top_k]):
                brand_row = self.df[self.df['name'] == similar_brand_name]
                if len(brand_row) == 0:
                    continue
                
                similar_idx = brand_row.index[0]
                original_score = similarity_scores[similar_brand_name]
                
                result = {
                    'rank': rank + 1,
                    'brand_name': similar_brand_name,
                    'brand_id': self.df.iloc[similar_idx]['id'],
                    'similarity_score': final_score,
                    'original_similarity': original_score,
                    'location_boost': final_score - original_score if use_location_rerank else 0.0,
                    'description_preview': self.df.iloc[similar_idx]['description'][:100] + "..."
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ Error in similarity search: {e}")
            return []
    
    def export_results(self, output_path):
        """çµæœã‚’CSVã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if self.df is not None:
            self.df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ Results exported to: {output_path}")
        else:
            print("âŒ No data to export")
    
    def run_simple_analysis(self, csv_path, embeddings_path=None, test_brands=['ã‚·ãƒ£ãƒãƒ«', 'ãƒ¦ãƒ‹ã‚¯ãƒ­']):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        print(f"ğŸš€ Running Simple Brand Reranking Analysis")
        print("=" * 50)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã¨ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿
            self.load_data(csv_path)
            self.load_embeddings(embeddings_path)
            
            # é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—
            self.calculate_similarity_matrix()
            
            # ãƒ†ã‚¹ãƒˆãƒ–ãƒ©ãƒ³ãƒ‰ã§é¡ä¼¼æ¤œç´¢ã‚’å®Ÿè¡Œ
            print(f"\nğŸ” Testing similarity search with reranking...")
            
            for brand in test_brands:
                print(f"\n--- Similar brands to '{brand}' ---")
                
                # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚ã‚Š
                print("ğŸ“ With location reranking:")
                similar_reranked = self.find_similar_brands(
                    brand, top_k=5, use_location_rerank=True, location_bias_strength=0.3
                )
                
                if similar_reranked:
                    for result in similar_reranked:
                        boost_indicator = "ğŸ“" if result['location_boost'] > 0.01 else "  "
                        print(f"{result['rank']:2d}. {boost_indicator} {result['brand_name']:25s} "
                              f"(score: {result['similarity_score']:.4f}, "
                              f"boost: {result['location_boost']:+.3f})")
                
                # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãªã—ï¼ˆæ¯”è¼ƒç”¨ï¼‰
                print("ğŸ§  Without location reranking:")
                similar_basic = self.find_similar_brands(
                    brand, top_k=5, use_location_rerank=False
                )
                
                if similar_basic:
                    for result in similar_basic:
                        print(f"{result['rank']:2d}.    {result['brand_name']:25s} "
                              f"(score: {result['similarity_score']:.4f})")
                else:
                    print(f"âŒ No results found for '{brand}'")
            
            print(f"\nâœ… Simple analysis complete!")
            return True
            
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def create_similarity_heatmap(self, output_dir='./advanced_results'):
        """ãƒ–ãƒ©ãƒ³ãƒ‰é–“é¡ä¼¼åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆ (æ—¥æœ¬èªå¯¾å¿œ)"""
        print("\nğŸ“Š Creating brand-to-brand similarity heatmap (æ—¥æœ¬èªå¯¾å¿œ)...")
        
        if self.similarity_matrix is None:
            print("âŒ Similarity matrix not calculated. Please run calculate_similarity_matrix() first.")
            return

        brand_names = self.df['name'].tolist()
        
        if len(brand_names) > 50: # ãƒ–ãƒ©ãƒ³ãƒ‰æ•°ãŒå¤šã™ãã‚‹ã¨ãƒ©ãƒ™ãƒ«ãŒèª­ã¿ã«ãããªã‚‹ãŸã‚åˆ¶é™
            print("âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰æ•°ãŒå¤šã„ãŸã‚ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è»¸ãƒ©ãƒ™ãƒ«ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ã€‚")
            # è»¸ãƒ©ãƒ™ãƒ«ã‚’ã‚ªãƒ•ã«ã™ã‚‹ã‹ã€ã‚µãƒ–ã‚»ãƒƒãƒˆã§æç”»ã‚’æ¤œè¨
            display_brand_names = False
            figsize = (12, 10) # å°ã•ãã™ã‚‹
        else:
            display_brand_names = True
            figsize = (18, 15)

        similarity_df = pd.DataFrame(self.similarity_matrix, index=brand_names, columns=brand_names)
        
        # åŒ…æ‹¬çš„ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆæ¤œå‡ºã¨è¨­å®š
        def find_japanese_font():
            # ã‚ˆã‚Šå¹…åºƒã„ãƒ•ã‚©ãƒ³ãƒˆå€™è£œ
            japanese_font_candidates = [
                'Noto Sans CJK JP', 'NotoSansCJK-Regular', 'Noto Sans CJK',
                'IPAexGothic', 'IPAPGothic', 'IPA Gothic',
                'TakaoGothic', 'TakaoPGothic', 
                'Yu Gothic', 'YuGothic', 'Yu Gothic Medium',
                'Meiryo', 'Meiryo UI',
                'Hiragino Sans GB', 'Hiragino Sans', 'Hiragino Kaku Gothic Pro',
                'MS Gothic', 'MS UI Gothic', 'MS PGothic',
                'MS Mincho', 'MS PMincho',
                'Liberation Sans', 'DejaVu Sans'
            ]
            
            # matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’å–å¾—
            available_fonts = [f.name for f in fm.fontManager.ttflist]
            
            for candidate in japanese_font_candidates:
                # å®Œå…¨ä¸€è‡´ã¨éƒ¨åˆ†ä¸€è‡´ã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
                if candidate in available_fonts:
                    return candidate
                for available in available_fonts:
                    if candidate.lower() in available.lower() or available.lower() in candidate.lower():
                        return available
            
            # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€CJKã‚’å«ã‚€ãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢
            for font in available_fonts:
                if any(keyword in font.lower() for keyword in ['cjk', 'japanese', 'jp', 'gothic', 'mincho']):
                    return font
                    
            return None

        plt.figure(figsize=figsize)
        
        found_japanese_font = find_japanese_font()
        if found_japanese_font:
            plt.rcParams['font.family'] = [found_japanese_font]
            plt.rcParams['axes.unicode_minus'] = False 
            print(f"âœ… Using Japanese font: {found_japanese_font}")
        else:
            print(f"âš ï¸ Warning: No suitable Japanese font found. Using fallback settings.")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š: Unicodeãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
            plt.rcParams['font.family'] = ['DejaVu Sans', 'Liberation Sans', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False  # Unicodeæ–‡å­—ã®è¡¨ç¤ºã‚’æ”¹å–„ 

        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆã¨æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã®è¨­å®š
        ax = sns.heatmap(similarity_df, annot=False, cmap='viridis', fmt=".2f",
                        xticklabels=display_brand_names, yticklabels=display_brand_names,
                        cbar_kws={'label': 'é¡ä¼¼åº¦'})
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«ã‚’æ˜ç¤ºçš„ã«è¨­å®š
        plt.title('ãƒ–ãƒ©ãƒ³ãƒ‰é–“é¡ä¼¼åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', fontsize=16, pad=20)
        plt.xlabel('ãƒ–ãƒ©ãƒ³ãƒ‰å', fontsize=12, labelpad=10)
        plt.ylabel('ãƒ–ãƒ©ãƒ³ãƒ‰å', fontsize=12, labelpad=10)
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ã€è»¸ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚‚è¨­å®š
        if found_japanese_font:
            ax.set_title('ãƒ–ãƒ©ãƒ³ãƒ‰é–“é¡ä¼¼åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', fontsize=16, fontname=found_japanese_font, pad=20)
            ax.set_xlabel('ãƒ–ãƒ©ãƒ³ãƒ‰å', fontsize=12, fontname=found_japanese_font, labelpad=10)
            ax.set_ylabel('ãƒ–ãƒ©ãƒ³ãƒ‰å', fontsize=12, fontname=found_japanese_font, labelpad=10)
        
        if display_brand_names:
            if found_japanese_font:
                plt.xticks(fontsize=8, rotation=90, fontname=found_japanese_font) 
                plt.yticks(fontsize=8, rotation=0, fontname=found_japanese_font)
            else:
                plt.xticks(fontsize=8, rotation=90) 
                plt.yticks(fontsize=8, rotation=0) 
        else:
            plt.xticks([]) # ãƒ©ãƒ™ãƒ«ã‚’éè¡¨ç¤º
            plt.yticks([]) # ãƒ©ãƒ™ãƒ«ã‚’éè¡¨ç¤º
        
        plt.tight_layout()
        
        heatmap_file = os.path.join(output_dir, 'brand_similarity_heatmap_japanese.png') 
        plt.savefig(heatmap_file, dpi=300)
        plt.close()
        print(f"ğŸ’¾ Similarity heatmap saved: {heatmap_file}")

    def create_static_analysis(self, output_dir):
        """é™çš„åˆ†æãƒ—ãƒ­ãƒƒãƒˆ"""
        print("ğŸ“Š Creating static analysis plots...")
        
        if self.embeddings is None:
            print("âŒ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã« generate_advanced_embeddings() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # Plot 1: Cluster distribution
        cluster_counts = self.df['cluster'].value_counts().sort_index()
        axes[0,0].bar(cluster_counts.index, cluster_counts.values, alpha=0.8, color='skyblue')
        axes[0,0].set_title('Cluster Size Distribution')
        axes[0,0].set_xlabel('Cluster ID')
        axes[0,0].set_ylabel('Number of Brands')
        axes[0,0].grid(True, alpha=0.3)
        
        # Plot 2: Similarity distribution
        off_diagonal = self.similarity_matrix[~np.eye(self.similarity_matrix.shape[0], dtype=bool)]
        axes[0,1].hist(off_diagonal, bins=50, alpha=0.8, color='lightcoral', edgecolor='black')
        axes[0,1].set_title('Pairwise Similarity Distribution')
        axes[0,1].set_xlabel('Cosine Similarity')
        axes[0,1].set_ylabel('Frequency')
        axes[0,1].grid(True, alpha=0.3)
        
        # Plot 3: Embedding magnitude distribution
        embedding_norms = np.linalg.norm(self.embeddings, axis=1)
        axes[0,2].hist(embedding_norms, bins=30, alpha=0.8, color='lightgreen', edgecolor='black')
        axes[0,2].set_title('Embedding Magnitude Distribution')
        axes[0,2].set_xlabel('L2 Norm')
        axes[0,2].set_ylabel('Frequency')
        axes[0,2].grid(True, alpha=0.3)
        
        # Plot 4: Description length vs similarity
        desc_lengths = self.df['description'].str.len()
        mean_similarities = []
        for i in range(len(self.df)):
            mean_sim = np.mean(self.similarity_matrix[i][self.similarity_matrix[i] < 1.0])
            mean_similarities.append(mean_sim)
        
        axes[1,0].scatter(desc_lengths, mean_similarities, alpha=0.6, s=30, color='purple')
        axes[1,0].set_title('Description Length vs Avg Similarity')
        axes[1,0].set_xlabel('Description Length (chars)')
        axes[1,0].set_ylabel('Average Similarity')
        axes[1,0].grid(True, alpha=0.3)
        
        # Plot 5: Cluster visualization (2D projection) - æœ€æ–°ã®æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’è‡ªå‹•é¸æŠ
        available_methods = []
        for method in ['mds', 'tsne', 'umap']:
            if f'{method}_x' in self.df.columns:
                available_methods.append(method)
        
        if available_methods:
            # æœ€å¾Œã«å®Ÿè¡Œã•ã‚ŒãŸæ‰‹æ³•ã‚’ä½¿ç”¨
            selected_method = available_methods[-1]
            unique_clusters = sorted(self.df['cluster'].unique())
            # ã‚ˆã‚Šæ˜ç¢ºãªè‰²åˆ†ã‘ã®ãŸã‚ã«ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’ä½¿ç”¨
            color_map = {
                i: ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12', '#9B59B6',
                    '#1ABC9C', '#E67E22', '#34495E', '#F1C40F', '#E91E63',
                    '#8E44AD', '#27AE60', '#D35400', '#2980B9', '#C0392B'][i % 15]
                for i in range(len(unique_clusters))
            }
            
            for i, cluster in enumerate(unique_clusters):
                cluster_data = self.df[self.df['cluster'] == cluster]
                axes[1,1].scatter(cluster_data[f'{selected_method}_x'], cluster_data[f'{selected_method}_y'], 
                                  c=color_map[i], label=f'C{cluster}', alpha=0.9, s=8, # ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å°ã•ã
                                  edgecolors='black', linewidth=0.5)  # é»’ã„ç¸ã§åŒºåˆ¥ã‚’æ˜ç¢ºã«
            
            axes[1,1].set_title(f'Brand Clusters ({selected_method.upper()})', fontsize=12)
            axes[1,1].set_xlabel(f'{selected_method.upper()} Component 1', fontsize=10)
            axes[1,1].set_ylabel(f'{selected_method.upper()} Component 2', fontsize=10)
            axes[1,1].legend(bbox_to_anchor=(1.05, 1), fontsize=8, frameon=True, fancybox=True, shadow=True)
            axes[1,1].grid(True, alpha=0.3)  # ã‚°ãƒªãƒƒãƒ‰ã‚’è¿½åŠ 
        else:
            axes[1,1].text(0.5, 0.5, 'No 2D projection available', ha='center', va='center', 
                          transform=axes[1,1].transAxes, fontsize=12)
            axes[1,1].set_title('No Visualization Available', fontsize=12)
        
        # Plot 6: Feature importance (semantic features) - Ruri v3ç›´æ¥ãƒ­ãƒ¼ãƒ‰æ™‚ã¯æ„å‘³ãŒãªã„ãŸã‚å‰Šé™¤ã¾ãŸã¯å¤‰æ›´æ¨å¥¨
        # ãŸã ã—ã€ã‚³ãƒ¼ãƒ‰ã®äº’æ›æ€§ã®ãŸã‚ã€ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ã§æ®‹ã™ã‹ã€å‰Šé™¤ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã«ã™ã‚‹ã€‚
        # Ruri v3ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã«ã¯ç›´æ¥çš„ãªã€Œsemantic_featuresã€ã®ã‚ˆã†ãªåˆ†è§£ã•ã‚ŒãŸæ„å‘³ãŒãªã„ãŸã‚ã€
        # ã“ã“ã¯è¡¨ç¤ºã—ãªã„æ–¹ãŒé©åˆ‡ã§ã™ã€‚
        axes[1,2].set_visible(False) # ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤ºã«ã™ã‚‹
        axes[1,2].set_title('Ruri v3 embeddings have no direct semantic features', fontsize=10)
        
        plt.tight_layout()
        
        static_file = os.path.join(output_dir, 'advanced_analysis.png')
        plt.savefig(static_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ’¾ Static analysis saved: {static_file}")
    
    def export_results(self, output_dir='./advanced_results'):
        """çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        print(f"\nğŸ’¾ Exporting results to {output_dir}...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        self.df.to_csv(os.path.join(output_dir, 'brands_advanced_analysis.csv'), 
                       index=False, encoding='utf-8-sig')
        
        np.save(os.path.join(output_dir, 'advanced_embeddings.npy'), self.embeddings)
        np.save(os.path.join(output_dir, 'similarity_matrix.npy'), self.similarity_matrix)
        
        with open(os.path.join(output_dir, 'analysis_summary.txt'), 'w', encoding='utf-8') as f:
            f.write("ğŸš€ Advanced Brand Similarity Analysis Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ğŸ“Š Dataset: {len(self.df)} brands\n")
            f.write(f"ğŸ§  Embedding dimensions: {self.embeddings.shape[1]}\n")
            f.write(f"ğŸ”¢ Similarity matrix: {self.similarity_matrix.shape}\n")
            
            off_diagonal = self.similarity_matrix[~np.eye(self.similarity_matrix.shape[0], dtype=bool)]
            f.write(f"ğŸ“ˆ Mean similarity: {off_diagonal.mean():.4f}\n")
            f.write(f"ğŸ“Š Similarity std: {off_diagonal.std():.4f}\n")
            f.write(f"ğŸ¯ Number of clusters: {self.df['cluster'].nunique()}\n")
            
            f.write(f"\nğŸ† Largest clusters:\n")
            cluster_counts = self.df['cluster'].value_counts().head(5)
            for cluster_id, count in cluster_counts.items():
                sample_brands = self.df[self.df['cluster'] == cluster_id]['name'].head(3).tolist()
                f.write(f"   - Cluster {cluster_id}: {count} brands ({', '.join(sample_brands)}...)\n")
        
        print(f"âœ… Export complete!")
    
    def generate_advanced_embeddings(self):
        """Load advanced embeddings using the default Ruri v3 model"""
        print(f"ğŸ§  Loading Ruri v3 embeddings...")
        return self.load_embeddings()
    
    def perform_intelligent_clustering(self):
        """Perform intelligent clustering on embeddings"""
        print(f"ğŸ¯ Performing intelligent clustering...")
        try:
            from sklearn.cluster import KMeans
            from sklearn.metrics import silhouette_score
            
            if self.embeddings is None:
                print("âŒ No embeddings available for clustering")
                return None, None
            
            # Find optimal number of clusters using silhouette score
            best_k = 2
            best_score = -1
            for k in range(2, min(10, len(self.df) // 5)):
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(self.embeddings)
                score = silhouette_score(self.embeddings, cluster_labels)
                if score > best_score:
                    best_score = score
                    best_k = k
            
            # Apply final clustering
            kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(self.embeddings)
            
            # Add cluster information to dataframe
            self.df['cluster'] = cluster_labels
            
            cluster_info = {
                'n_clusters': best_k,
                'silhouette_score': best_score,
                'cluster_sizes': pd.Series(cluster_labels).value_counts().to_dict()
            }
            
            print(f"âœ… Clustering complete: {best_k} clusters (silhouette: {best_score:.3f})")
            return cluster_labels, cluster_info
            
        except Exception as e:
            print(f"âŒ Clustering failed: {e}")
            # Fallback: assign all to single cluster
            cluster_labels = np.zeros(len(self.df), dtype=int)
            self.df['cluster'] = cluster_labels
            return cluster_labels, {'n_clusters': 1, 'silhouette_score': 0}
    
    def create_advanced_visualization(self, method='umap', use_anchor_based=False, show_brand_names=True):
        """Create advanced visualization using specified method"""
        print(f"ğŸ¨ Creating {method.upper()} visualization...")
        try:
            # This method would create individual visualizations
            # For now, we'll use the integrated system
            print(f"âœ… Visualization created using integrated system")
        except Exception as e:
            print(f"âŒ Visualization failed: {e}")
    
    def run_complete_analysis(self, csv_path, test_brands=['ã‚·ãƒ£ãƒãƒ«', 'ã‚³ãƒ  ãƒ‡ ã‚®ãƒ£ãƒ«ã‚½ãƒ³', 'ãƒ¦ãƒ‹ã‚¯ãƒ­']):
        """å®Œå…¨ãªåˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        print(f"\nğŸš€ Running Complete Advanced Brand Analysis")
        print("=" * 60)
        
        try:
            # Step 1: Load data
            self.load_data(csv_path)
            
            # Step 2: Generate advanced embeddings (Ruri v3 embeddingsã‚’ãƒ­ãƒ¼ãƒ‰)
            self.generate_advanced_embeddings()
            
            # Step 3: Calculate similarities
            self.calculate_similarity_matrix()
            
            # Step 4: Intelligent clustering (å‹•çš„ã‚¯ãƒ©ã‚¹ã‚¿æ•°)
            cluster_labels, cluster_info = self.perform_intelligent_clustering()
            
            # Step 5: çµ±åˆæ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å¯è¦–åŒ–
            print(f"\nğŸ¨ çµ±åˆæ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å¯è¦–åŒ–ã‚’å®Ÿè¡Œ...")
            
            # çµ±åˆæ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
            integrated_reducer = IntegratedDimensionalityReduction(
                embeddings=self.embeddings,
                brand_names=self.df['name'].tolist(),
                descriptions=self.df['description'].tolist()
            )
            
            # å…¨ã¦ã®æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’é©ç”¨
            integrated_results = integrated_reducer.apply_all_methods(
                n_clusters=len(set(cluster_labels)) if cluster_labels is not None else None
            )
            
            # çµ±åˆæ¯”è¼ƒå¯è¦–åŒ–ã‚’ä½œæˆ
            integrated_reducer.create_comparison_visualization(
                clusters=cluster_labels,
                show_brand_names=True
            )
            
            # å…¨åº§æ¨™ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            coords_df = integrated_reducer.export_all_coordinates()
            
            # å¾“æ¥ã®å€‹åˆ¥å¯è¦–åŒ–ã‚‚å®Ÿè¡Œï¼ˆæ—¢å­˜ã‚°ãƒ©ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ä¿æŒï¼‰
            print(f"\nğŸ¨ å¾“æ¥ã®å€‹åˆ¥å¯è¦–åŒ–ã‚‚å®Ÿè¡Œ...")
            
            # è«–æ–‡ã®æ‰‹æ³•ï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹UMAPï¼‰
            if USE_ANCHOR_BASED:
                print(f"ğŸ“ è«–æ–‡æ‰‹æ³•: ã‚¢ãƒ³ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹UMAP")
                self.create_advanced_visualization(method='anchor_umap', use_anchor_based=True, show_brand_names=True)
            
            # æ¯”è¼ƒæ‰‹æ³•1: æ¨™æº–UMAP
            self.create_advanced_visualization(method='umap', show_brand_names=True)
            
            # æ¯”è¼ƒæ‰‹æ³•2: t-SNE
            self.create_advanced_visualization(method='tsne', show_brand_names=True)
            
            # æ¯”è¼ƒæ‰‹æ³•3: MDS
            self.create_advanced_visualization(method='mds', show_brand_names=True)
            
            # Step 6: Test similarity search with multiple brands (ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ä»˜ã)
            print(f"\nğŸ” Testing similarity search with location reranking...")
            
            for brand in test_brands:
                print(f"\n--- ğŸ·ï¸   Similar brands to '{brand}' ---")
                print(f"    ğŸ“ åŸ‹ã‚è¾¼ã¿ + ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ:")
                similar_reranked = self.find_similar_brands(
                    brand, top_k=8, min_similarity=0.1, 
                    use_location_rerank=True, location_bias_strength=0.3
                )
                
                if similar_reranked:
                    for result in similar_reranked:
                        location_indicator = "ğŸ“" if result['location_boost'] > 0.01 else "  "
                        print(f"{result['rank']:2d}. {location_indicator} {result['brand_name']:30s} "
                              f"(æœ€çµ‚: {result['similarity_score']:.4f}, "
                              f"å…ƒ: {result['original_similarity']:.4f}, "
                              f"ãƒ–ãƒ¼ã‚¹ãƒˆ: {result['location_boost']:+.3f})")
                
                # æ¯”è¼ƒã®ãŸã‚ä½ç½®æƒ…å ±ãªã—ã®çµæœã‚‚è¡¨ç¤º
                print(f"    ğŸ§  åŸ‹ã‚è¾¼ã¿ã®ã¿ã®çµæœ:")
                similar_basic = self.find_similar_brands(
                    brand, top_k=5, min_similarity=0.1, 
                    use_location_rerank=False
                )
                
                if similar_basic:
                    for result in similar_basic:
                        print(f"{result['rank']:2d}.    {result['brand_name']:30s} "
                              f"(sim: {result['similarity_score']:.4f})")
                else:
                    print(f"âŒ No results found for '{brand}'")
            
            # Step 7: Export results
            output_dir = './advanced_results'
            self.export_results(output_dir)
            
            print(f"\nğŸ‰ Advanced Analysis Complete!")
            print(f"ğŸ“ Results saved in: {output_dir}")
            print(f"ğŸŒ Open brand_similarity_landscape.html for interactive exploration")
            
            return output_dir
            
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    CSV_PATH = "description.csv"
    
    try:
        print("ğŸš€ Initializing Advanced Brand Similarity Analyzer")
        print("ğŸ’¡ This system uses Ruri v3 embeddings for high-quality analysis")
        print("ğŸ¯ Generating high-quality embeddings without complex feature engineering")
        
        analyzer = LLMStyleEmbeddingAnalyzer()
        
        results_dir = analyzer.run_complete_analysis(CSV_PATH)
        
        if results_dir:
            print(f"\nâœ¨ Success! Advanced Brand Analysis Complete!")
            print(f"ğŸ¯ Results directory: {results_dir}")
            method_name = "Anchor-based UMAP" if USE_ANCHOR_BASED else "Standard UMAP"
            print(f"\nğŸ“‹ Generated files ({method_name}):")
            print(f"   ğŸŒ brand_similarity_landscape.html - Interactive visualization")
            print(f"   ğŸ“Š advanced_analysis.png - Static analysis plots")
            print(f"   ğŸ“„ brands_advanced_analysis.csv - Enhanced dataset")
            print(f"   ğŸ§  advanced_embeddings.npy - Feature vectors (Ruri v3)")
            print(f"   ğŸ”¢ similarity_matrix.npy - Similarity matrix")
            print(f"   ğŸ“ analysis_summary.txt - Summary report")
            print(f"   ğŸ”¥ brand_similarity_heatmap_japanese.png - Similarity Heatmap (æ—¥æœ¬èªå¯¾å¿œ)")
            
            print(f"\nğŸ¨ çµ±åˆæ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ ã®ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:")
            print(f"   ğŸ“Š dimensionality_reduction_comparison.png - 4æ‰‹æ³•æ¯”è¼ƒ (2x2ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ)")
            print(f"   ğŸŒ dimensionality_reduction_dashboard.html - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ¯”è¼ƒãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
            print(f"   ğŸ“„ all_dimensionality_reduction_coordinates.csv - å…¨æ‰‹æ³•ã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿")
            print(f"   ğŸ“Š dimensionality_reduction_statistics.csv - çµ±è¨ˆæ¯”è¼ƒãƒ‡ãƒ¼ã‚¿")
            print(f"   ğŸ“ dimensionality_reduction_report.txt - è©³ç´°æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ")
            
            if USE_ANCHOR_BASED:
                print(f"\nğŸ“‹ å€‹åˆ¥å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ« (å¾“æ¥ã‚°ãƒ©ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ä¿æŒ):")
                print(f"   ğŸŒ brand_similarity_landscape_anchor_umap.html - è«–æ–‡æ‰‹æ³•")
                print(f"   ğŸŒ brand_similarity_landscape_umap.html - æ¨™æº–UMAP")
                print(f"   ğŸŒ brand_similarity_landscape_tsne.html - t-SNE")
                print(f"   ğŸŒ brand_similarity_landscape_mds.html - MDS")
                print(f"\nğŸ¯ çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´:")
                print(f"   â€¢ 4ã¤ã®æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’åŒæ™‚æ¯”è¼ƒ")
                print(f"   â€¢ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
                print(f"   â€¢ çµ±è¨ˆçš„æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
                print(f"   â€¢ æ—¢å­˜ã®è¦‹ã‚„ã™ã„ã‚°ãƒ©ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ä¿æŒ")
                print(f"   â€¢ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±ã®çµ±åˆè¡¨ç¤º")

    except Exception as e:
        print(f"âŒ Failed to run analysis: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()