import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
from tqdm import tqdm
import warnings
import requests
import json
warnings.filterwarnings('ignore')

# Clean visualization settings
plt.rcParams['font.family'] = ['DejaVu Sans']
plt.rcParams['font.size'] = 10

class LLMStyleEmbeddingAnalyzer:
    def __init__(self):
        """
        LLMã‚¹ã‚¿ã‚¤ãƒ« ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°åˆ†æå™¨
        PyTorchã®å•é¡Œã‚’å›é¿ã—ã€é«˜å“è³ªãªç‰¹å¾´æŠ½å‡ºã‚’å®Ÿç¾
        """
        print(f"ğŸš€ Initializing LLM-Style Embedding Analyzer")
        print(f"ğŸ’¡ Using advanced linguistic feature extraction")
        
        # Initialize storage
        self.df = None
        self.embeddings = None
        self.similarity_matrix = None
        
        # Advanced feature extraction setup
        self.setup_advanced_features()
        
    def setup_advanced_features(self):
        """é«˜åº¦ãªç‰¹å¾´æŠ½å‡ºã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        
        # 1. è©³ç´°ãªãƒ–ãƒ©ãƒ³ãƒ‰çŸ¥è­˜ãƒ™ãƒ¼ã‚¹
        self.brand_knowledge = {
            # Luxury tiers
            'ultra_luxury': ['ã‚·ãƒ£ãƒãƒ«', 'ã‚¨ãƒ«ãƒ¡ã‚¹', 'ãƒ«ã‚¤ãƒ»ãƒ´ã‚£ãƒˆãƒ³', 'ãƒ‡ã‚£ã‚ªãƒ¼ãƒ«', 'ã‚°ãƒƒãƒ', 'ãƒ—ãƒ©ãƒ€', 
                           'ãƒ´ã‚¡ãƒ¬ãƒ³ãƒ†ã‚£ãƒ', 'ãƒãƒ¬ãƒ³ã‚·ã‚¢ã‚¬', 'ã‚¤ãƒ´ãƒ»ã‚µãƒ³ãƒ­ãƒ¼ãƒ©ãƒ³', 'ã‚»ãƒªãƒ¼ãƒŒ', 'ãƒ­ã‚¨ãƒ™',
                           'ãƒœãƒƒãƒ†ã‚¬ãƒ»ãƒ´ã‚§ãƒã‚¿', 'ã‚¸ãƒãƒ³ã‚·ã‚£', 'ã‚«ãƒ«ãƒ†ã‚£ã‚¨', 'ãƒ†ã‚£ãƒ•ã‚¡ãƒ‹ãƒ¼', 'ãƒ–ãƒ«ã‚¬ãƒª'],
            'luxury': ['ã‚³ãƒ¼ãƒ', 'ãƒã‚¤ã‚±ãƒ«ãƒ»ã‚³ãƒ¼ã‚¹', 'ã‚±ã‚¤ãƒˆãƒ»ã‚¹ãƒšãƒ¼ãƒ‰', 'ãƒˆãƒªãƒ¼ ãƒãƒ¼ãƒ', 'ãƒ•ãƒ«ãƒ©',
                      'ãƒãƒ¼ã‚¯ ã‚¸ã‚§ã‚¤ã‚³ãƒ–ã‚¹', 'ãƒ€ã‚¤ã‚¢ãƒ³ ãƒ•ã‚©ãƒ³ ãƒ•ã‚¡ã‚¹ãƒ†ãƒ³ãƒãƒ¼ã‚°'],
            'premium': ['ãƒ©ãƒ«ãƒ• ãƒ­ãƒ¼ãƒ¬ãƒ³', 'ã‚«ãƒ«ãƒãƒ³ãƒ»ã‚¯ãƒ©ã‚¤ãƒ³', 'ãƒˆãƒŸãƒ¼ ãƒ’ãƒ«ãƒ•ã‚£ã‚¬ãƒ¼', 'ãƒ©ã‚³ã‚¹ãƒ†',
                       'ãƒãƒ¼ãƒ«ãƒ»ã‚¹ãƒŸã‚¹', 'ã‚¢ãƒ‹ã‚¨ã‚¹ãƒ™ãƒ¼'],
            'fast_fashion': ['ãƒ¦ãƒ‹ã‚¯ãƒ­', 'ã‚¨ã‚¤ãƒï¼†ã‚¨ãƒ ', 'ã‚¶ãƒ©', 'ãƒ•ã‚©ãƒ¼ã‚¨ãƒãƒ¼21', 'ã‚®ãƒ£ãƒƒãƒ—'],
            
            # Japanese brands
            'japanese_avant_garde': ['ã‚³ãƒ  ãƒ‡ ã‚®ãƒ£ãƒ«ã‚½ãƒ³', 'ãƒ¨ã‚¦ã‚¸ãƒ¤ãƒãƒ¢ãƒˆ', 'ã‚¢ãƒ³ãƒ€ãƒ¼ã‚«ãƒãƒ¼', 
                                   'ã‚¸ãƒ¥ãƒ³ãƒ¤ ãƒ¯ã‚¿ãƒŠãƒ™', 'ã‚¤ãƒƒã‚»ã‚¤ãƒŸãƒ¤ã‚±', 'ã‚±ã‚¤ã‚¿ ãƒãƒ«ãƒ¤ãƒ'],
            'japanese_mainstream': ['ã‚±ãƒ³ã‚¾ãƒ¼', 'ã‚¢ã‚·ãƒƒã‚¯ã‚¹', 'ãƒŸã‚ºãƒ', 'ãƒ¯ã‚³ãƒãƒªã‚¢'],
            
            # Country origins
            'french': ['ã‚·ãƒ£ãƒãƒ«', 'ãƒ‡ã‚£ã‚ªãƒ¼ãƒ«', 'ãƒ«ã‚¤ãƒ»ãƒ´ã‚£ãƒˆãƒ³', 'ã‚¤ãƒ´ãƒ»ã‚µãƒ³ãƒ­ãƒ¼ãƒ©ãƒ³', 'ã‚»ãƒªãƒ¼ãƒŒ',
                      'ã‚¸ãƒãƒ³ã‚·ã‚£', 'ãƒãƒ¬ãƒ³ã‚·ã‚¢ã‚¬', 'ã‚½ãƒ‹ã‚¢ ãƒªã‚­ã‚¨ãƒ«', 'ã‚¢ãƒ‹ã‚¨ã‚¹ãƒ™ãƒ¼', 'ã‚±ãƒ³ã‚¾ãƒ¼'],
            'italian': ['ã‚°ãƒƒãƒ', 'ãƒ—ãƒ©ãƒ€', 'ãƒ´ã‚§ãƒ«ã‚µãƒ¼ãƒã‚§', 'ã‚¸ãƒ§ãƒ«ã‚¸ã‚ª ã‚¢ãƒ«ãƒãƒ¼ãƒ‹', 'ãƒ‰ãƒ«ãƒã‚§ï¼†ã‚¬ãƒƒãƒãƒ¼ãƒŠ',
                       'ãƒ•ã‚§ãƒ³ãƒ‡ã‚£', 'ãƒœãƒƒãƒ†ã‚¬ãƒ»ãƒ´ã‚§ãƒã‚¿', 'ãƒãƒ«ãƒ‹', 'ã‚¨ãƒˆãƒ­', 'ãƒãƒƒã‚¯ã‚¹ãƒãƒ¼ãƒ©'],
            'american': ['ãƒ©ãƒ«ãƒ• ãƒ­ãƒ¼ãƒ¬ãƒ³', 'ã‚«ãƒ«ãƒãƒ³ãƒ»ã‚¯ãƒ©ã‚¤ãƒ³', 'ãƒˆãƒŸãƒ¼ ãƒ’ãƒ«ãƒ•ã‚£ã‚¬ãƒ¼', 'ãƒã‚¤ã‚±ãƒ«ãƒ»ã‚³ãƒ¼ã‚¹',
                        'ã‚³ãƒ¼ãƒ', 'ã‚±ã‚¤ãƒˆãƒ»ã‚¹ãƒšãƒ¼ãƒ‰', 'ãƒãƒ¼ã‚¯ ã‚¸ã‚§ã‚¤ã‚³ãƒ–ã‚¹', 'ã‚®ãƒ£ãƒƒãƒ—'],
            'british': ['ãƒãƒ¼ãƒãƒªãƒ¼', 'ãƒãƒ¼ãƒ«ãƒ»ã‚¹ãƒŸã‚¹', 'ãƒ´ã‚£ãƒ´ã‚£ã‚¢ãƒ³ãƒ»ã‚¦ã‚¨ã‚¹ãƒˆã‚¦ãƒƒãƒ‰', 'ã‚¢ãƒ¬ã‚­ã‚µãƒ³ãƒ€ãƒ¼ãƒ»ãƒãƒƒã‚¯ã‚¤ãƒ¼ãƒ³'],
            'german': ['ã‚¸ãƒ« ã‚µãƒ³ãƒ€ãƒ¼', 'ãƒ’ãƒ¥ãƒ¼ã‚´ ãƒœã‚¹', 'ã‚¢ãƒ‡ã‚£ãƒ€ã‚¹', 'ãƒ—ãƒ¼ãƒ'],
            
            # Style categories
            'minimalist': ['ã‚¸ãƒ« ã‚µãƒ³ãƒ€ãƒ¼', 'ã‚»ãƒªãƒ¼ãƒŒ', 'ã‚³ã‚¹', 'ã‚¢ã‚¯ãƒ ã‚¹ãƒˆã‚¥ãƒ‡ã‚£ã‚ªã‚º', 'ãƒ«ãƒ¡ãƒ¼ãƒ«'],
            'avant_garde': ['ã‚³ãƒ  ãƒ‡ ã‚®ãƒ£ãƒ«ã‚½ãƒ³', 'ãƒ¨ã‚¦ã‚¸ãƒ¤ãƒãƒ¢ãƒˆ', 'ãƒªãƒƒã‚¯ãƒ»ã‚ªã‚¦ã‚¨ãƒ³ã‚¹', 'ã‚¢ãƒ³ ãƒ‰ã‚¥ãƒ ãƒ«ãƒ¡ã‚¹ãƒ†ãƒ¼ãƒ«'],
            'street': ['ã‚·ãƒ¥ãƒ—ãƒªãƒ¼ãƒ ', 'ã‚ªãƒ•ãƒ›ãƒ¯ã‚¤ãƒˆ', 'ã‚¢ ãƒ™ã‚¤ã‚·ãƒ³ã‚° ã‚¨ã‚¤ãƒ—', 'ã‚¢ãƒ³ãƒ€ãƒ¼ã‚«ãƒãƒ¼'],
            'sporty': ['ãƒŠã‚¤ã‚­', 'ã‚¢ãƒ‡ã‚£ãƒ€ã‚¹', 'ãƒ—ãƒ¼ãƒ', 'ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¢ãƒ¼ãƒãƒ¼', 'ãƒ«ãƒ«ãƒ¬ãƒ¢ãƒ³']
        }
        
        # 2. é«˜åº¦ãªç‰¹å¾´èªè¾æ›¸
        self.feature_vocabulary = {
            'luxury_indicators': {
                'ultra_high': ['haute couture', 'bespoke', 'artisan', 'heritage', 'maison', 'atelier', 
                             'è·äºº', 'ä¼çµ±', 'æœ€é«˜ç´š', 'ã‚ªãƒ¼ãƒˆã‚¯ãƒãƒ¥ãƒ¼ãƒ«', 'ãƒ¡ã‚¾ãƒ³'],
                'high': ['luxury', 'premium', 'prestige', 'exclusive', 'sophisticated', 'refined',
                        'ãƒ©ã‚°ã‚¸ãƒ¥ã‚¢ãƒªãƒ¼', 'ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ', 'é«˜ç´š', 'ä¸Šè³ª', 'æ´—ç·´', 'å“æ ¼'],
                'mid': ['quality', 'elegant', 'stylish', 'classic', 'timeless',
                       'å“è³ª', 'ã‚¨ãƒ¬ã‚¬ãƒ³ãƒˆ', 'ã‚¯ãƒ©ã‚·ãƒƒã‚¯', 'ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥'],
                'low': ['affordable', 'budget', 'value', 'accessible', 'everyday',
                       'ã‚¢ãƒ•ã‚©ãƒ¼ãƒ€ãƒ–ãƒ«', 'æ‰‹é ƒ', 'ä¾¡å€¤', 'ãŠæ‰‹è»½']
            },
            
            'design_philosophy': {
                'minimalist': ['minimal', 'simple', 'clean', 'understated', 'pure', 'essential',
                              'ãƒŸãƒ‹ãƒãƒ«', 'ã‚·ãƒ³ãƒ—ãƒ«', 'ç°¡æ½”', 'ã‚¯ãƒªãƒ¼ãƒ³', 'æœ¬è³ª'],
                'maximalist': ['ornate', 'elaborate', 'embellished', 'decorative', 'baroque',
                              'è£…é£¾', 'è¯ã‚„ã‹', 'è±ªè¯', 'ã‚´ãƒ¼ã‚¸ãƒ£ã‚¹'],
                'avant_garde': ['experimental', 'innovative', 'conceptual', 'deconstructed', 'radical',
                               'å®Ÿé¨“çš„', 'é©æ–°çš„', 'ã‚³ãƒ³ã‚»ãƒ—ãƒãƒ¥ã‚¢ãƒ«', 'å‰è¡›', 'ã‚¢ãƒãƒ³ã‚®ãƒ£ãƒ«ãƒ‰'],
                'classic': ['traditional', 'timeless', 'heritage', 'classic', 'vintage',
                           'ä¼çµ±çš„', 'ã‚¯ãƒ©ã‚·ãƒƒã‚¯', 'å¤å…¸', 'ãƒ´ã‚£ãƒ³ãƒ†ãƒ¼ã‚¸']
            },
            
            'aesthetic_qualities': {
                'feminine': ['feminine', 'delicate', 'graceful', 'romantic', 'soft', 'flowing',
                            'ãƒ•ã‚§ãƒŸãƒ‹ãƒ³', 'å¥³æ€§ã‚‰ã—ã„', 'å„ªé›…', 'ãƒ­ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯', 'æŸ”ã‚‰ã‹'],
                'masculine': ['masculine', 'strong', 'structured', 'sharp', 'bold', 'geometric',
                             'ãƒã‚¹ã‚­ãƒ¥ãƒªãƒ³', 'ç”·æ€§çš„', 'åŠ›å¼·ã„', 'ã‚·ãƒ£ãƒ¼ãƒ—', 'ãƒœãƒ¼ãƒ«ãƒ‰'],
                'androgynous': ['unisex', 'gender-neutral', 'androgynous', 'fluid',
                               'ãƒ¦ãƒ‹ã‚»ãƒƒã‚¯ã‚¹', 'ä¸­æ€§çš„', 'ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¸ãƒŠã‚¹']
            }
        }
    
    def load_data(self, csv_path):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“Š Loading data from: {csv_path}")
        
        self.df = pd.read_csv(csv_path)
        print(f"âœ… Loaded {len(self.df)} brands")
        
        # Data quality analysis
        desc_lengths = self.df['description'].str.len()
        print(f"ğŸ“ˆ Description analysis:")
        print(f"  - Mean length: {desc_lengths.mean():.1f} chars")
        print(f"  - Median length: {desc_lengths.median():.1f} chars")
        print(f"  - Range: {desc_lengths.min()}-{desc_lengths.max()} chars")
        
        return self.df
    
    def extract_semantic_features(self, description, brand_name):
        """
        ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç‰¹å¾´æŠ½å‡ºï¼ˆLLMã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
        20æ¬¡å…ƒã®é«˜å“è³ªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        """
        features = np.zeros(20)
        desc_lower = description.lower()
        brand_lower = brand_name.lower()
        
        # 1-4: Luxury Level (4 dimensions)
        luxury_levels = ['ultra_luxury', 'luxury', 'premium', 'fast_fashion']
        for i, level in enumerate(luxury_levels):
            if level in self.brand_knowledge:
                if brand_name in self.brand_knowledge[level]:
                    features[i] = 1.0
                elif any(keyword in desc_lower for keyword in 
                        self.feature_vocabulary['luxury_indicators'].get(level.split('_')[0], [])):
                    features[i] = 0.7
        
        # 5-9: Country Origin (5 dimensions)
        countries = ['french', 'italian', 'american', 'british', 'german']
        for i, country in enumerate(countries):
            if country in self.brand_knowledge and brand_name in self.brand_knowledge[country]:
                features[5 + i] = 1.0
            elif any(keyword in desc_lower for keyword in 
                    [country, country.replace('an', 'a'), country[:-2]]):
                features[5 + i] = 0.5
        
        # 10-13: Design Philosophy (4 dimensions)
        philosophies = ['minimalist', 'maximalist', 'avant_garde', 'classic']
        for i, phil in enumerate(philosophies):
            if phil in self.brand_knowledge and brand_name in self.brand_knowledge[phil]:
                features[10 + i] = 1.0
            else:
                phil_keywords = self.feature_vocabulary['design_philosophy'].get(phil, [])
                keyword_count = sum(1 for keyword in phil_keywords if keyword in desc_lower)
                features[10 + i] = min(1.0, keyword_count * 0.3)
        
        # 14-16: Aesthetic Qualities (3 dimensions)
        aesthetics = ['feminine', 'masculine', 'androgynous']
        for i, aesthetic in enumerate(aesthetics):
            aesthetic_keywords = self.feature_vocabulary['aesthetic_qualities'].get(aesthetic, [])
            keyword_count = sum(1 for keyword in aesthetic_keywords if keyword in desc_lower)
            features[14 + i] = min(1.0, keyword_count * 0.4)
        
        # 17: Japanese Elements
        japanese_brands = (self.brand_knowledge.get('japanese_avant_garde', []) + 
                          self.brand_knowledge.get('japanese_mainstream', []))
        if brand_name in japanese_brands:
            features[17] = 1.0
        elif any(keyword in desc_lower for keyword in ['japan', 'japanese', 'æ—¥æœ¬', 'zen', 'å’Œ']):
            features[17] = 0.6
        
        # 18: Street/Casual Elements
        if 'street' in self.brand_knowledge and brand_name in self.brand_knowledge['street']:
            features[18] = 1.0
        elif any(keyword in desc_lower for keyword in ['street', 'urban', 'casual', 'youth']):
            features[18] = 0.5
        
        # 19: Innovation Score
        innovation_keywords = ['innovative', 'experimental', 'cutting-edge', 'revolutionary', 
                              'groundbreaking', 'é©æ–°', 'å®Ÿé¨“', 'å…ˆç«¯']
        innovation_count = sum(1 for keyword in innovation_keywords if keyword in desc_lower)
        features[19] = min(1.0, innovation_count * 0.4)
        
        return features
    
    def generate_advanced_embeddings(self):
        """
        é«˜åº¦ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        TF-IDF + ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç‰¹å¾´ + N-gramåˆ†æ
        """
        print(f"\nğŸ§  Generating advanced embeddings...")
        
        descriptions = self.df['description'].fillna('').tolist()
        brand_names = self.df['name'].tolist()
        
        # 1. High-quality TF-IDF features
        print("ğŸ”¤ Creating TF-IDF vectors...")
        tfidf = TfidfVectorizer(
            max_features=300,
            ngram_range=(1, 3),  # unigrams, bigrams, trigrams
            min_df=2,
            max_df=0.8,
            analyzer='word',
            lowercase=True,
            stop_words=None  # Keep all words for fashion context
        )
        
        tfidf_features = tfidf.fit_transform(descriptions).toarray()
        print(f"  âœ… TF-IDF shape: {tfidf_features.shape}")
        
        # 2. Semantic features
        print("ğŸ¯ Extracting semantic features...")
        semantic_features = []
        for desc, brand_name in tqdm(zip(descriptions, brand_names), desc="Semantic extraction"):
            features = self.extract_semantic_features(desc, brand_name)
            semantic_features.append(features)
        
        semantic_features = np.array(semantic_features)
        print(f"  âœ… Semantic features shape: {semantic_features.shape}")
        
        # 3. Brand name embeddings (character-level features)
        print("ğŸ·ï¸  Creating brand name features...")
        name_features = []
        for brand_name in brand_names:
            # Simple character-level features
            name_vector = np.zeros(10)
            name_len = len(brand_name)
            
            name_vector[0] = min(1.0, name_len / 50.0)  # Normalized length
            name_vector[1] = min(1.0, brand_name.count(' ') / 5.0)  # Word count
            name_vector[2] = sum(1 for c in brand_name if c.isupper()) / max(1, name_len)  # Uppercase ratio
            name_vector[3] = sum(1 for c in brand_name if c.isalpha()) / max(1, name_len)  # Alpha ratio
            name_vector[4] = 1.0 if 'ãƒ»' in brand_name else 0.0  # Japanese separator
            name_vector[5] = 1.0 if any(c.isascii() for c in brand_name) else 0.0  # Has ASCII
            name_vector[6] = 1.0 if any(ord(c) > 127 for c in brand_name) else 0.0  # Has non-ASCII
            
            # Safe vowel counting
            if name_len > 0:
                name_vector[7] = brand_name.lower().count('a') / name_len  # Vowel density
                name_vector[8] = brand_name.lower().count('e') / name_len
                name_vector[9] = brand_name.lower().count('i') / name_len
            
            name_features.append(name_vector)
        
        name_features = np.array(name_features)
        print(f"  âœ… Name features shape: {name_features.shape}")
        
        # 4. Combine all features with optimal weighting
        print("ğŸ”— Combining feature vectors...")
        
        # Weight different feature types
        tfidf_weighted = tfidf_features * 1.0      # Base importance
        semantic_weighted = semantic_features * 3.0  # High importance for brand characteristics
        name_weighted = name_features * 0.5        # Lower importance
        
        # Combine
        self.embeddings = np.concatenate([
            tfidf_weighted, 
            semantic_weighted, 
            name_weighted
        ], axis=1)
        
        print(f"âœ… Final embeddings shape: {self.embeddings.shape}")
        print(f"  - TF-IDF: {tfidf_features.shape[1]} dims")
        print(f"  - Semantic: {semantic_features.shape[1]} dims")
        print(f"  - Name: {name_features.shape[1]} dims")
        print(f"  - Total: {self.embeddings.shape[1]} dims")
        
        return self.embeddings
    
    def calculate_similarity_matrix(self):
        """é«˜å“è³ªé¡ä¼¼åº¦è¡Œåˆ—ã®è¨ˆç®—"""
        print(f"\nğŸ”¢ Calculating similarity matrix...")
        
        # Use cosine similarity for high-dimensional vectors
        self.similarity_matrix = cosine_similarity(self.embeddings)
        
        # Quality metrics
        print(f"âœ… Similarity matrix: {self.similarity_matrix.shape}")
        
        # Exclude diagonal (self-similarity)
        off_diagonal = self.similarity_matrix[~np.eye(self.similarity_matrix.shape[0], dtype=bool)]
        
        print(f"ğŸ“Š Similarity statistics:")
        print(f"  - Mean: {off_diagonal.mean():.4f}")
        print(f"  - Std: {off_diagonal.std():.4f}")
        print(f"  - Min: {off_diagonal.min():.4f}")
        print(f"  - Max: {off_diagonal.max():.4f}")
        print(f"  - Median: {np.median(off_diagonal):.4f}")
        
        return self.similarity_matrix
    
    def find_similar_brands(self, brand_name, top_k=10, min_similarity=0.0):
        """é«˜ç²¾åº¦é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢"""
        try:
            # Find brand
            brand_matches = self.df[self.df['name'] == brand_name]
            if len(brand_matches) == 0:
                print(f"âŒ Brand '{brand_name}' not found")
                similar_names = self.df['name'].str.contains(brand_name, case=False, na=False)
                if similar_names.any():
                    suggestions = self.df[similar_names]['name'].head(5).tolist()
                    print(f"ğŸ’¡ Did you mean: {', '.join(suggestions)}")
                return []
            
            brand_idx = brand_matches.index[0]
            similarities = self.similarity_matrix[brand_idx]
            
            # Filter and sort
            valid_indices = np.where(similarities >= min_similarity)[0]
            valid_similarities = similarities[valid_indices]
            sorted_indices = np.argsort(valid_similarities)[::-1]
            
            results = []
            count = 0
            
            for idx in sorted_indices:
                actual_idx = valid_indices[idx]
                
                # Skip self
                if actual_idx == brand_idx:
                    continue
                
                similarity_score = similarities[actual_idx]
                
                result = {
                    'rank': count + 1,
                    'brand_name': self.df.iloc[actual_idx]['name'],
                    'brand_id': self.df.iloc[actual_idx]['id'],
                    'similarity_score': similarity_score,
                    'description_preview': self.df.iloc[actual_idx]['description'][:120] + "...",
                    'embedding_distance': np.linalg.norm(
                        self.embeddings[brand_idx] - self.embeddings[actual_idx]
                    )
                }
                
                results.append(result)
                count += 1
                
                if count >= top_k:
                    break
            
            return results
            
        except Exception as e:
            print(f"âŒ Error in similarity search: {e}")
            return []
    
    def perform_intelligent_clustering(self, n_clusters=12, random_state=42):
        """çŸ¥çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        print(f"\nğŸ¯ Performing intelligent clustering...")
        
        # Use K-means with multiple initializations for stability
        kmeans = KMeans(
            n_clusters=n_clusters, 
            random_state=random_state, 
            n_init=20,  # Multiple initializations
            max_iter=500
        )
        
        cluster_labels = kmeans.fit_predict(self.embeddings)
        self.df['cluster'] = cluster_labels
        
        # Detailed cluster analysis
        print(f"ğŸ“Š Cluster analysis:")
        
        cluster_info = []
        for cluster_id in range(n_clusters):
            cluster_brands = self.df[self.df['cluster'] == cluster_id]
            
            # Representative brands
            sample_brands = cluster_brands['name'].head(5).tolist()
            
            # Cluster center analysis
            cluster_center = kmeans.cluster_centers_[cluster_id]
            
            info = {
                'cluster_id': cluster_id,
                'size': len(cluster_brands),
                'sample_brands': sample_brands,
                'center_norm': np.linalg.norm(cluster_center),
                'avg_desc_length': cluster_brands['description'].str.len().mean()
            }
            
            cluster_info.append(info)
        
        # Sort by size
        cluster_info.sort(key=lambda x: x['size'], reverse=True)
        
        print(f"ğŸ† Top clusters:")
        for i, info in enumerate(cluster_info[:5]):
            print(f"  {i+1}. Cluster {info['cluster_id']}: {info['size']} brands")
            print(f"     Examples: {', '.join(info['sample_brands'])}")
        
        return cluster_labels, cluster_info
    
    def create_advanced_visualization(self, method='umap', output_dir='./advanced_results'):
        """é«˜åº¦ãªå¯è¦–åŒ–"""
        print(f"\nğŸ¨ Creating advanced visualization using {method.upper()}...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. Dimensionality reduction
        if method.lower() == 'umap':
            reducer = umap.UMAP(
                n_components=2,
                n_neighbors=15,
                min_dist=0.1,
                metric='cosine',
                random_state=42
            )
        else:  # t-SNE
            reducer = TSNE(
                n_components=2,
                perplexity=30,
                random_state=42,
                metric='cosine'
            )
        
        coords_2d = reducer.fit_transform(self.embeddings)
        self.df[f'{method}_x'] = coords_2d[:, 0]
        self.df[f'{method}_y'] = coords_2d[:, 1]
        
        # 2. Interactive visualization
        fig = px.scatter(
            self.df,
            x=f'{method}_x',
            y=f'{method}_y',
            color='cluster',
            hover_data=['name', 'brand_id'],
            hover_name='name',
            title=f'ğŸš€ Advanced Brand Similarity Landscape ({method.upper()})',
            width=1200,
            height=800,
            color_continuous_scale='viridis'
        )
        
        fig.update_traces(marker=dict(size=10, opacity=0.8))
        fig.update_layout(
            title_x=0.5,
            font=dict(size=14),
            hovermode='closest'
        )
        
        # Save interactive plot
        interactive_file = os.path.join(output_dir, 'brand_similarity_landscape.html')
        fig.write_html(interactive_file)
        print(f"ğŸ’¾ Interactive plot saved: {interactive_file}")
        
        # 3. Static analysis plots
        self.create_static_analysis(output_dir)
        
        return coords_2d
    
    def create_static_analysis(self, output_dir):
        """é™çš„åˆ†æãƒ—ãƒ­ãƒƒãƒˆ"""
        print("ğŸ“Š Creating static analysis plots...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # Plot 1: Cluster distribution
        cluster_counts = self.df['cluster'].value_counts().sort_index()
        axes[0,0].bar(cluster_counts.index, cluster_counts.values, alpha=0.8, color='skyblue')
        axes[0,0].set_title('Cluster Size Distribution')
        axes[0,0].set_xlabel('Cluster ID')
        axes[0,0].set_ylabel('Number of Brands')
        axes[0,0].grid(True, alpha=0.3)
        
        # Plot 2: Similarity distribution
        off_diagonal = self.similarity_matrix[~np.eye(self.similarity_matrix.shape[0], dtype=bool)]
        axes[0,1].hist(off_diagonal, bins=50, alpha=0.8, color='lightcoral', edgecolor='black')
        axes[0,1].set_title('Pairwise Similarity Distribution')
        axes[0,1].set_xlabel('Cosine Similarity')
        axes[0,1].set_ylabel('Frequency')
        axes[0,1].grid(True, alpha=0.3)
        
        # Plot 3: Embedding magnitude distribution
        embedding_norms = np.linalg.norm(self.embeddings, axis=1)
        axes[0,2].hist(embedding_norms, bins=30, alpha=0.8, color='lightgreen', edgecolor='black')
        axes[0,2].set_title('Embedding Magnitude Distribution')
        axes[0,2].set_xlabel('L2 Norm')
        axes[0,2].set_ylabel('Frequency')
        axes[0,2].grid(True, alpha=0.3)
        
        # Plot 4: Description length vs similarity
        desc_lengths = self.df['description'].str.len()
        mean_similarities = []
        for i in range(len(self.df)):
            mean_sim = np.mean(self.similarity_matrix[i][self.similarity_matrix[i] < 1.0])
            mean_similarities.append(mean_sim)
        
        axes[1,0].scatter(desc_lengths, mean_similarities, alpha=0.6, s=30, color='purple')
        axes[1,0].set_title('Description Length vs Avg Similarity')
        axes[1,0].set_xlabel('Description Length (chars)')
        axes[1,0].set_ylabel('Average Similarity')
        axes[1,0].grid(True, alpha=0.3)
        
        # Plot 5: Cluster visualization (2D projection)
        if 'umap_x' in self.df.columns:
            unique_clusters = self.df['cluster'].unique()
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))
            
            for i, cluster in enumerate(unique_clusters):
                cluster_data = self.df[self.df['cluster'] == cluster]
                axes[1,1].scatter(cluster_data['umap_x'], cluster_data['umap_y'], 
                                c=[colors[i]], label=f'C{cluster}', alpha=0.7, s=30)
            
            axes[1,1].set_title('Brand Clusters (UMAP)')
            axes[1,1].set_xlabel('UMAP Component 1')
            axes[1,1].set_ylabel('UMAP Component 2')
            axes[1,1].legend(bbox_to_anchor=(1.05, 1), fontsize=8)
        
        # Plot 6: Feature importance (semantic features)
        semantic_start = 300  # After TF-IDF features
        semantic_features = self.embeddings[:, semantic_start:semantic_start+20]
        feature_vars = np.var(semantic_features, axis=0)
        
        feature_names = ['Ultra Lux', 'Luxury', 'Premium', 'Fast Fashion', 
                        'French', 'Italian', 'American', 'British', 'German',
                        'Minimalist', 'Maximalist', 'Avant-garde', 'Classic',
                        'Feminine', 'Masculine', 'Androgynous', 'Japanese', 
                        'Street', 'Innovation', 'Other']
        
        axes[1,2].barh(range(len(feature_vars)), feature_vars, alpha=0.8, color='orange')
        axes[1,2].set_title('Semantic Feature Variance')
        axes[1,2].set_xlabel('Variance')
        axes[1,2].set_ylabel('Feature Index')
        axes[1,2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        static_file = os.path.join(output_dir, 'advanced_analysis.png')
        plt.savefig(static_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ’¾ Static analysis saved: {static_file}")
    
    def export_results(self, output_dir='./advanced_results'):
        """çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        print(f"\nğŸ’¾ Exporting results to {output_dir}...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Save enhanced dataset
        self.df.to_csv(os.path.join(output_dir, 'brands_advanced_analysis.csv'), 
                      index=False, encoding='utf-8-sig')
        
        # Save embeddings and similarity matrix
        np.save(os.path.join(output_dir, 'advanced_embeddings.npy'), self.embeddings)
        np.save(os.path.join(output_dir, 'similarity_matrix.npy'), self.similarity_matrix)
        
        # Summary report
        with open(os.path.join(output_dir, 'analysis_summary.txt'), 'w', encoding='utf-8') as f:
            f.write("ğŸš€ Advanced Brand Similarity Analysis Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ğŸ“Š Dataset: {len(self.df)} brands\n")
            f.write(f"ğŸ§  Embedding dimensions: {self.embeddings.shape[1]}\n")
            f.write(f"ğŸ”¢ Similarity matrix: {self.similarity_matrix.shape}\n")
            
            # Similarity statistics
            off_diagonal = self.similarity_matrix[~np.eye(self.similarity_matrix.shape[0], dtype=bool)]
            f.write(f"ğŸ“ˆ Mean similarity: {off_diagonal.mean():.4f}\n")
            f.write(f"ğŸ“Š Similarity std: {off_diagonal.std():.4f}\n")
            f.write(f"ğŸ¯ Number of clusters: {self.df['cluster'].nunique()}\n")
            
            # Top clusters
            f.write(f"\nğŸ† Largest clusters:\n")
            cluster_counts = self.df['cluster'].value_counts().head(5)
            for cluster_id, count in cluster_counts.items():
                sample_brands = self.df[self.df['cluster'] == cluster_id]['name'].head(3).tolist()
                f.write(f"  - Cluster {cluster_id}: {count} brands ({', '.join(sample_brands)}...)\n")
        
        print(f"âœ… Export complete!")
    
    def run_complete_analysis(self, csv_path, test_brands=['ã‚·ãƒ£ãƒãƒ«', 'ã‚³ãƒ  ãƒ‡ ã‚®ãƒ£ãƒ«ã‚½ãƒ³', 'ãƒ¦ãƒ‹ã‚¯ãƒ­']):
        """å®Œå…¨ãªåˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        print(f"\nğŸš€ Running Complete Advanced Brand Analysis")
        print("=" * 60)
        
        try:
            # Step 1: Load data
            self.load_data(csv_path)
            
            # Step 2: Generate advanced embeddings
            self.generate_advanced_embeddings()
            
            # Step 3: Calculate similarities
            self.calculate_similarity_matrix()
            
            # Step 4: Intelligent clustering
            self.perform_intelligent_clustering(n_clusters=15)
            
            # Step 5: Advanced visualization
            self.create_advanced_visualization(method='umap')
            
            # Step 6: Test similarity search with multiple brands
            print(f"\nğŸ” Testing similarity search...")
            
            for brand in test_brands:
                print(f"\n--- ğŸ·ï¸  Similar brands to '{brand}' ---")
                similar = self.find_similar_brands(brand, top_k=8, min_similarity=0.1)
                
                if similar:
                    for result in similar:
                        print(f"{result['rank']:2d}. {result['brand_name']:35s} "
                              f"(sim: {result['similarity_score']:.4f}) "
                              f"[dist: {result['embedding_distance']:.3f}]")
                else:
                    print(f"âŒ No results found for '{brand}'")
            
            # Step 7: Export results
            output_dir = './advanced_results'
            self.export_results(output_dir)
            
            print(f"\nğŸ‰ Advanced Analysis Complete!")
            print(f"ğŸ“ Results saved in: {output_dir}")
            print(f"ğŸŒ Open brand_similarity_landscape.html for interactive exploration")
            
            return output_dir
            
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # Configuration
    CSV_PATH = "datasets/bline_similarity/blines_updated_desc_validation_20250530055331.csv"
    
    try:
        print("ğŸš€ Initializing Advanced Brand Similarity Analyzer")
        print("ğŸ’¡ This system uses PyTorch-free advanced linguistic analysis")
        print("ğŸ¯ Generating high-quality embeddings without external dependencies")
        
        # Initialize analyzer (no PyTorch required!)
        analyzer = LLMStyleEmbeddingAnalyzer()
        
        # Run complete analysis
        results_dir = analyzer.run_complete_analysis(CSV_PATH)
        
        if results_dir:
            print(f"\nâœ¨ Success! Advanced Brand Analysis Complete!")
            print(f"ğŸ¯ Results directory: {results_dir}")
            print(f"\nğŸ“‹ Generated files:")
            print(f"  ğŸŒ brand_similarity_landscape.html - Interactive visualization")
            print(f"  ğŸ“Š advanced_analysis.png - Static analysis plots")
            print(f"  ğŸ“„ brands_advanced_analysis.csv - Enhanced dataset")
            print(f"  ğŸ§  advanced_embeddings.npy - Feature vectors")
            print(f"  ğŸ”¢ similarity_matrix.npy - Similarity matrix")
            print(f"  ğŸ“ analysis_summary.txt - Summary report")
        
    except Exception as e:
        print(f"âŒ Failed to run analysis: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()