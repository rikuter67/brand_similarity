#!/usr/bin/env python3
"""
çµ±åˆãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ  â†’ Geminièª¬æ˜æ–‡ç”Ÿæˆ â†’ Ruri v3ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ é¡ä¼¼åº¦æ¤œç´¢ â†’ ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚° â†’ å¯è¦–åŒ–
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import requests
import json
import os
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
from brand_similarity import LLMStyleEmbeddingAnalyzer
# from location_bias_reranking import create_reranker_from_streamlit_data
from gpt_oss_direct import GPTOSSDirect
import warnings
warnings.filterwarnings('ignore')
import umap
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, DBSCAN
from sklearn.cluster import AgglomerativeClustering
from sklearn.mixture import GaussianMixture

# Streamlit page configuration
st.set_page_config(
    page_title="çµ±åˆãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    .search-card {
        background-color: #f8f9fa;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 1px solid #dee2e6;
        margin: 1rem 0;
    }
    .result-card {
        background-color: #e8f4fd;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #0066cc;
        margin: 0.5rem 0;
    }
    .new-brand-highlight {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #ffecb3;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

class IntegratedBrandSearchSystem:
    """çµ±åˆãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.base_analyzer = None
        self.ruri_model = None
        self.embeddings = None
        self.brand_names = []
        self.brand_descriptions = []
        # self.reranker = None
        self.added_brands = []  # æ–°è¦è¿½åŠ ã•ã‚ŒãŸãƒ–ãƒ©ãƒ³ãƒ‰
        self.dimensionality_reducer = None
        self.reduction_results = {}
        self.cluster_results = {}
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ç”¨
        self.brand_mapping = {}  # integrated_brand -> maps_brand
        
        # ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ç®¡ç†
        self.maps_data = None  # maps.csvãƒ‡ãƒ¼ã‚¿
        self.brand_genre_mapping = {}  # ãƒ–ãƒ©ãƒ³ãƒ‰å -> genres
        self.genre_brands = {}  # genre -> [brand_names]
        
        # GPT-OSSç›´æ¥å®Ÿè¡Œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        self.gpt_oss = None
        
    @st.cache_resource
    def load_ruri_model(_self):
        """Ruri v3ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            model = SentenceTransformer('cl-nagoya/ruri-v3-310m')
            return model
        except Exception as e:
            st.error(f"Ruri v3ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    @st.cache_data
    def load_base_data(_self):
        """æ—¢å­˜ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        try:
            # æ—¢å­˜ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
            if os.path.exists('integrated_brands.csv'):
                csv_file = 'integrated_brands.csv'
            else:
                csv_file = 'description.csv'
            
            analyzer = LLMStyleEmbeddingAnalyzer()
            analyzer.load_data(csv_file)
            
            # åŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
            embedding_files = [
                "./ruri_embeddings_results/ruri_description_embeddings_v3_raw_hub.npy",
                "./ruri_embeddings_results/ruri_description_embeddings.npy",
                "./ruri_embeddings_results/updated_embeddings.npy"
            ]
            
            for emb_path in embedding_files:
                if os.path.exists(emb_path):
                    try:
                        raw_embeddings = np.load(emb_path)
                        analyzer.embeddings = normalize(raw_embeddings, norm='l2', axis=1)
                        st.success(f"âœ… åŸ‹ã‚è¾¼ã¿èª­ã¿è¾¼ã¿: {os.path.basename(emb_path)}")
                        break
                    except Exception as e:
                        continue
            
            if analyzer.embeddings is None:
                st.warning("æ—¢å­˜åŸ‹ã‚è¾¼ã¿ãªã— - æ–°è¦ç”ŸæˆãŒå¿…è¦")
                
            return analyzer
            
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        self.base_analyzer = self.load_base_data()
        self.ruri_model = self.load_ruri_model()
        
        if self.base_analyzer and self.base_analyzer.embeddings is not None:
            self.embeddings = self.base_analyzer.embeddings.copy()
            # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰åã¨èª¬æ˜ã‚’å¾©å…ƒ
            self.brand_names = self.base_analyzer.df['name'].tolist()
            self.brand_descriptions = self.base_analyzer.df['description'].tolist()
            
            # ãƒªãƒ©ãƒ³ã‚«ãƒ¼åˆæœŸåŒ–
            # try:
            #     self.reranker = create_reranker_from_streamlit_data(self.base_analyzer)
            #     # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åˆæœŸåŒ–
            #     self._initialize_brand_mapping()
            # except:
            #     self.reranker = None
                
            # ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±èª­ã¿è¾¼ã¿
            self.load_genre_data()
            
            # æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            self.initialize_dimensionality_reducer()
            
        
        return self.base_analyzer is not None and self.ruri_model is not None
    
    def initialize_dimensionality_reducer(self):
        """æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            if self.embeddings is not None and len(self.embeddings) > 0:
                # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ¬ãƒ™ãƒ«ã§ã®åŸ‹ã‚è¾¼ã¿é›†ç´„
                brand_embeddings, unique_brand_names, unique_descriptions = self.aggregate_embeddings_by_brand()
                
                if brand_embeddings is not None and len(brand_embeddings) > 0:
                    from integrated_dimensionality_reduction import IntegratedDimensionalityReduction
                    self.dimensionality_reducer = IntegratedDimensionalityReduction(
                        embeddings=brand_embeddings,
                        brand_names=unique_brand_names,
                        descriptions=unique_descriptions
                    )
                    print(f"âœ… æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†: {len(brand_embeddings)}ãƒ–ãƒ©ãƒ³ãƒ‰")
                else:
                    self.dimensionality_reducer = None
            else:
                self.dimensionality_reducer = None
        except Exception as e:
            print(f"æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.dimensionality_reducer = None
    
    def aggregate_embeddings_by_brand(self):
        """ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ¬ãƒ™ãƒ«ã§ã®åŸ‹ã‚è¾¼ã¿é›†ç´„"""
        try:
            if not hasattr(self, 'base_analyzer') or self.base_analyzer is None:
                return None, None, None
                
            df = self.base_analyzer.df
            embeddings = self.embeddings
            
            if len(df) != len(embeddings):
                print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿é•·ä¸ä¸€è‡´: CSV {len(df)} vs åŸ‹ã‚è¾¼ã¿ {len(embeddings)}")
                # ãƒ‡ãƒ¼ã‚¿é•·ãŒé•ã†å ´åˆã§ã‚‚ã€CSVã®ç¯„å›²å†…ã§å‡¦ç†ã‚’ç¶šè¡Œ
                print("ğŸ“ CSVã®ç¯„å›²å†…ã§åŸ‹ã‚è¾¼ã¿é›†ç´„ã‚’å®Ÿè¡Œã—ã¾ã™...")
                max_index = min(len(df), len(embeddings))
                df = df.iloc[:max_index]
                embeddings = embeddings[:max_index]
            
            # ãƒ–ãƒ©ãƒ³ãƒ‰åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å¹³å‡åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—
            brand_groups = df.groupby('name')
            brand_embeddings = []
            unique_brand_names = []
            unique_descriptions = []
            
            for brand_name, group in brand_groups:
                indices = group.index.tolist()
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒembeddingsã®ç¯„å›²å†…ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                valid_indices = [i for i in indices if i < len(embeddings)]
                
                if valid_indices:
                    # è©²å½“ãƒ–ãƒ©ãƒ³ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ã‚’å¹³å‡åŒ–
                    brand_embedding = np.mean(embeddings[valid_indices], axis=0)
                    brand_embeddings.append(brand_embedding)
                    unique_brand_names.append(brand_name)
                    
                    # ä»£è¡¨çš„ãªèª¬æ˜æ–‡ã‚’å–å¾—
                    desc = group.iloc[0].get('description', brand_name)
                    unique_descriptions.append(desc)
            
            if brand_embeddings:
                brand_embeddings = np.array(brand_embeddings)
                print(f"ğŸ“Š ãƒ–ãƒ©ãƒ³ãƒ‰é›†ç´„å®Œäº†: {len(unique_brand_names)}ãƒ–ãƒ©ãƒ³ãƒ‰, åŸ‹ã‚è¾¼ã¿å½¢çŠ¶: {brand_embeddings.shape}")
                return brand_embeddings, unique_brand_names, unique_descriptions
            else:
                return None, None, None
                
        except Exception as e:
            print(f"åŸ‹ã‚è¾¼ã¿é›†ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None, None
    
    def _initialize_brand_mapping(self):
        """ãƒ–ãƒ©ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã®åˆæœŸåŒ–ã€éƒ¨åˆ†ä¸€è‡´ã§è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°"""
        # if not self.reranker or not self.brand_names:
        #     return
        
        # available_brands = set(self.reranker.get_available_brands_for_location_analysis())
        return
        
        # éƒ¨åˆ†ä¸€è‡´ã§ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        for integrated_brand in self.brand_names:
            best_match = None
            best_score = 0.0
            
            integrated_lower = integrated_brand.lower().replace(' ', '').replace('-', '')
            
            for available_brand in available_brands:
                available_lower = available_brand.lower().replace(' ', '').replace('-', '')
                
                # éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆåŒæ–¹å‘ï¼‰
                if integrated_lower in available_lower or available_lower in integrated_lower:
                    # ä¸€è‡´ç‡ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    score = len(set(integrated_lower) & set(available_lower)) / len(set(integrated_lower) | set(available_lower))
                    if score > best_score and score > 0.3:  # 30%ä»¥ä¸Šã®ä¸€è‡´
                        best_match = available_brand
                        best_score = score
            
            if best_match:
                self.brand_mapping[integrated_brand] = best_match
        
        print(f"ğŸ”— ãƒ–ãƒ©ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆæœŸåŒ–: {len(self.brand_mapping)} ä»¶ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ")
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°ä¾‹ã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®5ä»¶ï¼‰
        for i, (integrated, mapped) in enumerate(list(self.brand_mapping.items())[:5]):
            print(f"  {integrated} -> {mapped}")
    
    def load_genre_data(self):
        """maps.csvã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
        try:
            import pandas as pd
            import os
            
            maps_path = "datasets/bline_similarity/maps.csv"
            if not os.path.exists(maps_path):
                st.warning("âš ï¸ maps.csvãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - ã‚¸ãƒ£ãƒ³ãƒ«æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™")
                return
            
            # maps.csvã‚’èª­ã¿è¾¼ã¿
            self.maps_data = pd.read_csv(maps_path)
            st.info(f"âœ… maps.csvã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(self.maps_data)}ä»¶ï¼‰")
            
            # ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’æŠ½å‡ºï¼ˆåº—èˆ—åã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’æ¨å®šï¼‰
            self._extract_brand_genre_mapping()
            
        except Exception as e:
            st.error(f"ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.maps_data = None
    
    def _extract_brand_genre_mapping(self):
        """maps.csvã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰åã¨ã‚¸ãƒ£ãƒ³ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ"""
        if self.maps_data is None:
            return
        
        # ã‚¸ãƒ£ãƒ³ãƒ«ãŒnullã§ãªã„è¡Œã®ã¿å‡¦ç†
        valid_genre_data = self.maps_data.dropna(subset=['genre'])
        
        for _, row in valid_genre_data.iterrows():
            shop_name = str(row['name'])
            genre_str = str(row['genre'])
            
            # ã‚¸ãƒ£ãƒ³ãƒ«æ–‡å­—åˆ—ã‚’åˆ†å‰²
            genres = [g.strip() for g in genre_str.split(',') if g.strip()]
            
            # åº—èˆ—åã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
            brand_name = self._extract_brand_name_from_shop(shop_name)
            
            if brand_name and brand_name in self.brand_names:
                # ãƒ–ãƒ©ãƒ³ãƒ‰ã¨ã‚¸ãƒ£ãƒ³ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                if brand_name not in self.brand_genre_mapping:
                    self.brand_genre_mapping[brand_name] = set()
                
                self.brand_genre_mapping[brand_name].update(genres)
                
                # ã‚¸ãƒ£ãƒ³ãƒ«ã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰ã®é€†ãƒãƒƒãƒ”ãƒ³ã‚°ã‚‚ä½œæˆ
                for genre in genres:
                    if genre not in self.genre_brands:
                        self.genre_brands[genre] = set()
                    self.genre_brands[genre].add(brand_name)
        
        # setã‚’listã«å¤‰æ›
        for brand in self.brand_genre_mapping:
            self.brand_genre_mapping[brand] = list(self.brand_genre_mapping[brand])
        
        for genre in self.genre_brands:
            self.genre_brands[genre] = list(self.genre_brands[genre])
        
        st.info(f"ğŸ¯ ã‚¸ãƒ£ãƒ³ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†: {len(self.brand_genre_mapping)}ãƒ–ãƒ©ãƒ³ãƒ‰ã€{len(self.genre_brands)}ã‚¸ãƒ£ãƒ³ãƒ«")
    
    def _extract_brand_name_from_shop(self, shop_name):
        """åº—èˆ—åã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’æ¨å®š"""
        # åº—èˆ—åã‹ã‚‰å ´æ‰€æƒ…å ±ã‚’é™¤å»ã—ã¦ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’æŠ½å‡º
        shop_parts = shop_name.split()
        if len(shop_parts) == 0:
            return None
        
        # æœ€åˆã®éƒ¨åˆ†ã‚’ãƒ–ãƒ©ãƒ³ãƒ‰åã¨ã—ã¦ä½¿ç”¨ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        potential_brand = shop_parts[0]
        
        # integrated_brands.csvã®ãƒ–ãƒ©ãƒ³ãƒ‰åã¨éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        for brand_name in self.brand_names:
            brand_lower = brand_name.lower().replace(' ', '').replace('-', '')
            potential_lower = potential_brand.lower().replace(' ', '').replace('-', '')
            
            # éƒ¨åˆ†ä¸€è‡´ã¾ãŸã¯åŒ…å«é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
            if (potential_lower in brand_lower or brand_lower in potential_lower) and len(potential_lower) > 2:
                return brand_name
        
        return None
    
    def generate_brand_description_template(self, brand_name):
        """ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèª¬æ˜æ–‡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ"""
        # ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸåŸºæœ¬çš„ãªèª¬æ˜æ–‡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        template = f"""ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰ã€Œ{brand_name}ã€ã¯ã€ç‹¬è‡ªã®ãƒ‡ã‚¶ã‚¤ãƒ³å“²å­¦ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æŒã¤ãƒ–ãƒ©ãƒ³ãƒ‰ã§ã™ã€‚
ã‚³ãƒ³ãƒ†ãƒ³ãƒãƒ©ãƒªãƒ¼ãªãƒ‡ã‚¶ã‚¤ãƒ³ã¨æ©Ÿèƒ½æ€§ã‚’é‡è¦–ã—ãŸè£½å“å±•é–‹ã‚’è¡Œã£ã¦ãŠã‚Šã€
å¹…åºƒã„å¹´é½¢å±¤ã«æ”¯æŒã•ã‚Œã¦ã„ã¾ã™ã€‚é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã‚¯ãƒ©ãƒ•ãƒˆãƒãƒ³ã‚·ãƒƒãƒ—ã‚’
å¤§åˆ‡ã«ã—ãªãŒã‚‰ã€ç¾ä»£çš„ãªãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ã«åˆã‚ã›ãŸå•†å“ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚"""
        return template.strip()
    
    def initialize_gpt_oss(self):
        """GPT-OSSç›´æ¥å®Ÿè¡Œãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        if self.gpt_oss is None:
            with st.spinner("GPT-OSSãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                self.gpt_oss = GPTOSSDirect()
        return self.gpt_oss.is_available()
    
    def check_gpt_oss_connection(self):
        """GPT-OSSæ¥ç¶šçŠ¶æ…‹ç¢ºèª"""
        try:
            if self.gpt_oss is None:
                self.gpt_oss = GPTOSSDirect()
            return self.gpt_oss.is_available()
        except Exception:
            return False
    
    def generate_brand_description(self, api_key, brand_name, fallback_mode="template"):
        """GPT-OSS ãƒ­ãƒ¼ã‚«ãƒ«APIã§ãƒ–ãƒ©ãƒ³ãƒ‰èª¬æ˜æ–‡ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰"""
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        model_available = self.initialize_gpt_oss()
        
        if not model_available:
            st.warning("âš ï¸ GPT-OSSãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“")
            
            if fallback_mode == "template":
                st.info("ğŸ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")
                return self.generate_brand_description_template(brand_name)
            elif fallback_mode == "manual":
                st.info("âœï¸ æ‰‹å‹•å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„")
                return None
            else:
                st.error("âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None
        
        try:
            # ç›´æ¥æ¨è«–å®Ÿè¡Œ
            st.info("ğŸš€ GPT-OSSã§èª¬æ˜æ–‡ç”Ÿæˆä¸­...")
            description = self.gpt_oss.generate_brand_description(brand_name)
            
            if description and not description.startswith("ã‚¨ãƒ©ãƒ¼:"):
                return description.strip()
            else:
                st.error(f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {description}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if fallback_mode == "template":
                    st.info("ğŸ“ ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")
                    return self.generate_brand_description_template(brand_name)
                return None
        except Exception as e:
            st.error(f"èª¬æ˜æ–‡ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            
            # ä¾‹å¤–ç™ºç”Ÿæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if fallback_mode == "template":
                st.info("ğŸ“ ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")
                return self.generate_brand_description_template(brand_name)
            
            return None
    
    def check_brand_exists(self, brand_name, strict=True):
        """ãƒ–ãƒ©ãƒ³ãƒ‰ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        if not self.brand_names:
            return False, None
        
        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if brand_name in self.brand_names:
            return True, brand_name
        
        # éå³å¯†ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰åãƒã‚§ãƒƒã‚¯
        if not strict:
            brand_lower = brand_name.lower().replace(' ', '').replace('-', '')
            for existing_brand in self.brand_names:
                existing_lower = existing_brand.lower().replace(' ', '').replace('-', '')
                if brand_lower == existing_lower or brand_lower in existing_lower or existing_lower in brand_lower:
                    return True, existing_brand
        
        return False, None
    
    def add_new_brand(self, brand_name, description):
        """æ–°ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ç³»çµ±ã«è¿½åŠ """
        if not self.ruri_model:
            st.error("Ruri v3ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        # æ—¢å­˜ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚§ãƒƒã‚¯
        exists, existing_name = self.check_brand_exists(brand_name, strict=True)
        if exists:
            st.warning(f"âš ï¸ '{brand_name}' ã¯æ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ï¼ˆæ—¢å­˜: '{existing_name}'ï¼‰ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return False
        
        try:
            # Ruri v3ã§æ–°ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            new_embedding = self.ruri_model.encode(
                [description], 
                normalize_embeddings=True,
                convert_to_tensor=False
            )
            
            # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆ
            if self.embeddings is not None:
                self.embeddings = np.vstack([self.embeddings, new_embedding])
            else:
                self.embeddings = new_embedding
            
            self.brand_names.append(brand_name)
            self.brand_descriptions.append(description)
            self.added_brands.append({
                'name': brand_name,
                'description': description,
                'index': len(self.brand_names) - 1
            })
            
            # ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ã¨æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°
            self._save_updated_data()
            self._update_dimensionality_reducer()
            
            st.success(f"âœ… {brand_name} ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            return True
            
        except Exception as e:
            st.error(f"ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def process_csv_batch(self, csv_data, api_key="", progress_callback=None, use_existing_descriptions=False, force_regenerate=False, strict_check=True):
        """CSVä¸€æ‹¬å‡¦ç† - ãƒ–ãƒ©ãƒ³ãƒ‰åæ¤œç´¢â†’é‡è¤‡ãƒã‚§ãƒƒã‚¯â†’èª¬æ˜æ–‡ç”Ÿæˆâ†’Ruriv3ãƒ™ã‚¯ãƒˆãƒ«åŒ–â†’è¾æ›¸è¿½åŠ ã®ã‚µã‚¤ã‚¯ãƒ«"""
        if not self.ruri_model:
            st.error("Ruri v3ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return []
        
        results = []
        total_brands = len(csv_data)
        
        for i, row in enumerate(csv_data):
            if progress_callback:
                progress_callback(i, total_brands, f"ãƒ–ãƒ©ãƒ³ãƒ‰åæ¤œç´¢ä¸­: {row['brand_name']}")
            
            try:
                brand_name = row['brand_name']
                
                # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ–ãƒ©ãƒ³ãƒ‰åã«ã‚ˆã‚‹æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                exists, existing_name = self.check_brand_exists(brand_name, strict=strict_check)
                if exists:
                    error_msg = f'æ—¢ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¸ˆã¿ï¼ˆæ—¢å­˜: {existing_name}ï¼‰' if existing_name != brand_name else 'æ—¢ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¸ˆã¿'
                    results.append({
                        'brand_name': brand_name,
                        'status': 'skipped',
                        'error': error_msg,
                        'description': 'ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸ã«å­˜åœ¨',
                        'generated': False,
                        'step': 'duplicate_check'
                    })
                    continue
                
                if progress_callback:
                    progress_callback(i, total_brands, f"èª¬æ˜æ–‡ç”Ÿæˆä¸­: {brand_name}")
                
                # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ–ãƒ©ãƒ³ãƒ‰åã‹ã‚‰èª¬æ˜æ–‡ç”Ÿæˆ
                description = None
                generated = False
                
                if force_regenerate:
                    # å¼·åˆ¶çš„ã«æ–°è¦ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰
                    description = self.generate_brand_description(api_key, brand_name, fallback_mode="template")
                    generated = True
                elif use_existing_descriptions and row.get('description') and str(row['description']).strip():
                    # æ—¢å­˜èª¬æ˜æ–‡ã‚’ä½¿ç”¨
                    description = row['description']
                    generated = False
                else:
                    # GPT-OSSã§èª¬æ˜æ–‡ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰
                    description = self.generate_brand_description(api_key, brand_name, fallback_mode="template")
                    generated = True
                    
                if not description:
                    results.append({
                        'brand_name': brand_name,
                        'status': 'failed',
                        'error': 'èª¬æ˜æ–‡ç”Ÿæˆå¤±æ•—',
                        'description': '',
                        'generated': generated,
                        'step': 'description_generation'
                    })
                    continue
                
                if progress_callback:
                    progress_callback(i, total_brands, f"Ruriv3ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­: {brand_name}")
                
                # ã‚¹ãƒ†ãƒƒãƒ—3: Ruriv3ã«ã‚ˆã‚‹ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                try:
                    new_embedding = self.ruri_model.encode(
                        [description], 
                        normalize_embeddings=True,
                        convert_to_tensor=False
                    )
                except Exception as e:
                    results.append({
                        'brand_name': brand_name,
                        'status': 'failed',
                        'error': f'ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¤±æ•—: {str(e)}',
                        'description': description[:100] + '...',
                        'generated': generated,
                        'step': 'vectorization'
                    })
                    continue
                
                if progress_callback:
                    progress_callback(i, total_brands, f"ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸è¿½åŠ ä¸­: {brand_name}")
                
                # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸ã¸ã®è¿½åŠ 
                try:
                    if self.embeddings is not None:
                        self.embeddings = np.vstack([self.embeddings, new_embedding])
                    else:
                        self.embeddings = new_embedding
                    
                    self.brand_names.append(brand_name)
                    self.brand_descriptions.append(description)
                    self.added_brands.append({
                        'name': brand_name,
                        'description': description,
                        'index': len(self.brand_names) - 1
                    })
                    
                    results.append({
                        'brand_name': brand_name,
                        'status': 'success',
                        'description': description[:100] + '...',
                        'generated': generated,
                        'step': 'completed',
                        'embedding_shape': new_embedding.shape,
                        'vector_index': len(self.brand_names) - 1
                    })
                    
                except Exception as e:
                    results.append({
                        'brand_name': brand_name,
                        'status': 'failed',
                        'error': f'è¾æ›¸è¿½åŠ å¤±æ•—: {str(e)}',
                        'description': description[:100] + '...',
                        'generated': generated,
                        'step': 'dictionary_addition'
                    })
                
            except Exception as e:
                results.append({
                    'brand_name': row.get('brand_name', 'Unknown'),
                    'status': 'error',
                    'error': f'äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}',
                    'description': '',
                    'generated': False,
                    'step': 'unknown_error'
                })
        
        # å‡¦ç†å®Œäº†å¾Œã®çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°
        success_count = sum(1 for r in results if r['status'] == 'success')
        skipped_count = sum(1 for r in results if r['status'] == 'skipped')
        failed_count = len(results) - success_count - skipped_count
        
        # æ–°è¦è¿½åŠ ãŒã‚ã£ãŸå ´åˆã¯ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ã¨æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°
        if success_count > 0:
            self._save_updated_data()
            self._update_dimensionality_reducer()
        
        print(f"ğŸ“Š CSVä¸€æ‹¬å‡¦ç†å®Œäº†: æˆåŠŸ {success_count}, ã‚¹ã‚­ãƒƒãƒ— {skipped_count}, å¤±æ•— {failed_count}")
        
        return results
    
    def _save_updated_data(self):
        """æ›´æ–°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜"""
        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
            if self.base_analyzer and hasattr(self.base_analyzer, 'df'):
                # æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ 
                new_rows = []
                for brand in self.added_brands:
                    new_rows.append({
                        'name': brand['name'],
                        'description': brand['description']
                    })
                
                if new_rows:
                    new_df = pd.DataFrame(new_rows)
                    updated_df = pd.concat([self.base_analyzer.df, new_df], ignore_index=True)
                    updated_df.to_csv('integrated_brands.csv', index=False)
            
            # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜
            if self.embeddings is not None:
                os.makedirs('./ruri_embeddings_results/', exist_ok=True)
                np.save('./ruri_embeddings_results/ruri_description_embeddings_v3_raw_hub.npy', self.embeddings)
                
        except Exception as e:
            st.warning(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _update_dimensionality_reducer(self):
        """æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ ã‚’æ›´æ–°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§å†åˆæœŸåŒ–"""
        try:
            if self.embeddings is not None and len(self.embeddings) > 0:
                # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ¬ãƒ™ãƒ«ã§ã®åŸ‹ã‚è¾¼ã¿é›†ç´„
                brand_embeddings, unique_brand_names, unique_descriptions = self.aggregate_embeddings_by_brand()
                
                if brand_embeddings is not None and len(brand_embeddings) > 0:
                    from integrated_dimensionality_reduction import IntegratedDimensionalityReduction
                    self.dimensionality_reducer = IntegratedDimensionalityReduction(
                        embeddings=brand_embeddings,
                        brand_names=unique_brand_names,
                        descriptions=unique_descriptions
                    )
                    print(f"âœ… æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°å®Œäº†: {len(brand_embeddings)}ãƒ–ãƒ©ãƒ³ãƒ‰")
                else:
                    self.dimensionality_reducer = None
            else:
                self.dimensionality_reducer = None
        except Exception as e:
            st.warning(f"æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            self.dimensionality_reducer = None
    
    def search_similar_brands(self, target_brand, top_k=10, min_similarity=0.1, genre_filter=None, normalize_similarity=False):
        """é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"""
        if self.embeddings is None:
            return []
        
        try:
            # ãƒ–ãƒ©ãƒ³ãƒ‰åæ¤œç´¢
            if target_brand not in self.brand_names:
                st.error(f"ãƒ–ãƒ©ãƒ³ãƒ‰ '{target_brand}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
            
            target_idx = self.brand_names.index(target_brand)
            target_embedding = self.embeddings[target_idx].reshape(1, -1)
            
            # ã‚¸ãƒ£ãƒ³ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®å€™è£œãƒ–ãƒ©ãƒ³ãƒ‰ã‚’æ±ºå®š
            candidate_brands = set(self.brand_names)
            
            if genre_filter and self.brand_genre_mapping:
                # æŒ‡å®šã•ã‚ŒãŸã‚¸ãƒ£ãƒ³ãƒ«ã‚’æŒã¤ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã¿ã«é™å®š
                if isinstance(genre_filter, str):
                    genre_filter = [genre_filter]
                
                filtered_brands = set()
                for genre in genre_filter:
                    if genre in self.genre_brands:
                        filtered_brands.update(self.genre_brands[genre])
                
                if filtered_brands:
                    candidate_brands = filtered_brands
                    st.info(f"ğŸ¯ ã‚¸ãƒ£ãƒ³ãƒ« '{', '.join(genre_filter)}' ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(candidate_brands)}ãƒ–ãƒ©ãƒ³ãƒ‰ãŒå¯¾è±¡")
                else:
                    st.warning(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸã‚¸ãƒ£ãƒ³ãƒ« '{', '.join(genre_filter)}' ã«ãƒ–ãƒ©ãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return []
            
            # é¡ä¼¼åº¦è¨ˆç®—
            similarities = cosine_similarity(target_embedding, self.embeddings)[0]
            
            # çµæœæ•´ç†
            results = []
            for i, sim in enumerate(similarities):
                if i < len(self.brand_names) and i < len(self.brand_descriptions):  # bounds check
                    brand_name = self.brand_names[i]
                    if i != target_idx and sim >= min_similarity and brand_name in candidate_brands:
                        is_new_brand = any(added['index'] == i for added in self.added_brands)
                        brand_genres = self.brand_genre_mapping.get(brand_name, [])
                        
                        results.append({
                            'brand_name': brand_name,
                            'similarity': sim,
                            'description': self.brand_descriptions[i],
                            'is_new': is_new_brand,
                            'genres': brand_genres
                        })
            
            # é¡ä¼¼åº¦ã®æ­£è¦åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if normalize_similarity and results:
                similarities = [r['similarity'] for r in results]
                min_sim = min(similarities)
                max_sim = max(similarities)
                
                # Min-Maxæ­£è¦åŒ–ã§0-1ã®ç¯„å›²ã«å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                if max_sim > min_sim:
                    for result in results:
                        original_sim = result['similarity']
                        normalized_sim = (original_sim - min_sim) / (max_sim - min_sim)
                        result['original_similarity'] = original_sim
                        result['similarity'] = normalized_sim
                        result['normalized'] = True
                else:
                    # å…¨ã¦åŒã˜å€¤ã®å ´åˆã¯æ­£è¦åŒ–ã—ãªã„
                    for result in results:
                        result['original_similarity'] = result['similarity']
                        result['normalized'] = False
            else:
                for result in results:
                    result['original_similarity'] = result['similarity']
                    result['normalized'] = False
            
            # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
            results.sort(key=lambda x: x['similarity'], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    # def apply_fixed_boost_reranking(self, results, target_brand, location_method="comprehensive"):
        """å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆæˆ¦ç•¥ã«ã‚ˆã‚‹ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆåˆ†æçµæœãƒ™ãƒ¼ã‚¹ï¼‰"""
        if not results:
            return results
        
        # å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆå€¤è¨­å®šï¼ˆåˆ†æçµæœã«åŸºã¥ãï¼‰
        boost_tiers = {
            0.8: 0.025,   # 80%ä»¥ä¸Šä¸€è‡´ -> +0.025
            0.5: 0.015,   # 50%ä»¥ä¸Šä¸€è‡´ -> +0.015
            0.2: 0.005,   # 20%ä»¥ä¸Šä¸€è‡´ -> +0.005
            0.0: 0.000    # ä¸€è‡´ãªã— -> +0.000
        }
        
        # åˆæœŸå€¤è¨­å®š
        for result in results:
            result['similarity_score'] = result['similarity']  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜
            result['original_similarity'] = result['similarity']  # å…ƒã®é¡ä¼¼åº¦
            result['final_similarity'] = result['similarity']
            result['location_boost'] = 0.0
            result['location_similarity'] = 0.0
            result['boost_tier'] = '0%'
            result['rerank_method'] = location_method
            result['rerank_mode'] = 'fixed_boost'
        
        if not self.reranker:
            st.warning("ãƒªãƒ©ãƒ³ã‚«ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - å…ƒã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨")
            return results
        
        try:
            # ä½ç½®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆã‚’é©ç”¨
            for result in results:
                brand_name = result['brand_name']
                
                # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ä½ç½®é¡ä¼¼åº¦ã‚’è¨ˆç®—
                mapped_target = self.brand_mapping.get(target_brand, target_brand)
                mapped_brand = self.brand_mapping.get(brand_name, brand_name)
                
                if mapped_target in self.reranker.brand_locations and mapped_brand in self.reranker.brand_locations:
                    location_sim = self.reranker.calculate_location_similarity(
                        mapped_target, mapped_brand, method=location_method
                    )
                    result['location_similarity'] = location_sim
                    
                    # ä½ç½®ä¸€è‡´ç‡ã«å¿œã˜ãŸå›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆã‚’é©ç”¨
                    boost_value = 0.0
                    tier_label = '0%'
                    
                    for threshold in sorted(boost_tiers.keys(), reverse=True):
                        if location_sim >= threshold:
                            boost_value = boost_tiers[threshold]
                            tier_label = f'{int(threshold*100)}%+'
                            break
                    
                    result['location_boost'] = boost_value
                    result['boost_tier'] = tier_label
                    result['final_similarity'] = result['similarity'] + boost_value
                else:
                    result['location_similarity'] = 0.0
                    result['location_boost'] = 0.0
                    result['boost_tier'] = 'ãƒ‡ãƒ¼ã‚¿ãªã—'
                    result['final_similarity'] = result['similarity']
            
            # æœ€çµ‚é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
            results.sort(key=lambda x: x['final_similarity'], reverse=True)
            return results
            
        except Exception as e:
            st.warning(f"å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return results
    
    # def apply_location_reranking(self, results, target_brand, bias_strength=0.015, 
    #                             location_method="comprehensive", rerank_mode="weighted_average"):
        """æ‹¡å¼µã•ã‚ŒãŸä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨ï¼ˆå¾“æ¥æ–¹å¼ï¼‰"""
        if not results:
            return results
        
        # ã¾ãšåˆæœŸå€¤ã‚’è¨­å®š
        for result in results:
            result['similarity_score'] = result['similarity']  # APIçµæœå½¢å¼ã«åˆã‚ã›ã‚‹
            result['original_similarity'] = result['similarity']
            result['final_similarity'] = result['similarity']
            result['location_boost'] = 0.0
            result['location_similarity'] = 0.0
            result['rerank_method'] = location_method
            result['rerank_mode'] = rerank_mode
            result['bias_strength'] = bias_strength
        
        # ãƒªãƒ©ãƒ³ã‚«ãƒ¼ãŒãªã„å ´åˆã¯åˆæœŸå€¤ã®ã¾ã¾è¿”ã™
        if not self.reranker:
            st.warning("ãƒªãƒ©ãƒ³ã‚«ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - å…ƒã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨")
            return results
        
        try:
            # é¡ä¼¼åº¦è¾æ›¸ä½œæˆ
            similarity_dict = {r['brand_name']: r['similarity'] for r in results}
            
            # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Ÿè¡Œï¼ˆãƒ–ãƒ©ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ä»˜ãï¼‰
            reranked_similarities = self.reranker.rerank_similarity_with_location_bias(
                similarity_dict, target_brand, bias_strength, location_method, rerank_mode,
                brand_mapping=self.brand_mapping
            )
            
            # å„ãƒ–ãƒ©ãƒ³ãƒ‰ã®ä½ç½®é¡ä¼¼åº¦ã‚‚è¨ˆç®—
            for result in results:
                brand_name = result['brand_name']
                
                # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ä½ç½®é¡ä¼¼åº¦ã‚’è¨ˆç®—
                mapped_target = self.brand_mapping.get(target_brand, target_brand)
                mapped_brand = self.brand_mapping.get(brand_name, brand_name)
                
                if mapped_target in self.reranker.brand_locations and mapped_brand in self.reranker.brand_locations:
                    location_sim = self.reranker.calculate_location_similarity(
                        mapped_target, mapped_brand, method=location_method
                    )
                    result['location_similarity'] = location_sim
                else:
                    result['location_similarity'] = 0.0
                
                # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å¾Œã®é¡ä¼¼åº¦ã‚’è¨­å®š
                if brand_name in reranked_similarities:
                    result['similarity_score'] = reranked_similarities[brand_name]  # APIçµæœå½¢å¼
                    result['final_similarity'] = reranked_similarities[brand_name]
                    result['location_boost'] = reranked_similarities[brand_name] - result['similarity']
                else:
                    result['similarity_score'] = result['similarity']
                    result['final_similarity'] = result['similarity']
                    result['location_boost'] = 0.0
            
            # æœ€çµ‚é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
            results.sort(key=lambda x: x['final_similarity'], reverse=True)
            return results
            
        except Exception as e:
            st.warning(f"ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚åˆæœŸå€¤ã¯æ—¢ã«è¨­å®šæ¸ˆã¿
            return results
    
    def create_embedding_visualization(self, highlight_brands=None):
        """ç‰¹å¾´é‡ç©ºé–“ã®å¯è¦–åŒ–ï¼ˆãƒ–ãƒ©ãƒ³ãƒ‰ãƒ¬ãƒ™ãƒ«ï¼‰"""
        if self.embeddings is None or len(self.embeddings) < 2:
            st.warning("å¯è¦–åŒ–ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        
        try:
            # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
            brand_embeddings, unique_brand_names, unique_descriptions = self.aggregate_embeddings_by_brand()
            
            if brand_embeddings is None or len(brand_embeddings) < 2:
                st.warning("ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ¬ãƒ™ãƒ«ã®å¯è¦–åŒ–ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return None
            
            # UMAPæ¬¡å…ƒå‰Šæ¸›
            reducer = umap.UMAP(
                n_components=2,
                n_neighbors=min(15, len(brand_embeddings) - 1),
                min_dist=0.1,
                metric='cosine',
                random_state=42
            )
            
            coords_2d = reducer.fit_transform(brand_embeddings)
            
            # è‰²åˆ†ã‘è¨­å®š
            colors = []
            hover_texts = []
            sizes = []
            
            for i, (name, desc) in enumerate(zip(unique_brand_names, unique_descriptions)):
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ãƒã‚§ãƒƒã‚¯
                if i >= len(coords_2d):
                    break
                    
                # æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ–ãƒ©ãƒ³ãƒ‰åãƒ™ãƒ¼ã‚¹ï¼‰
                is_new = any(added['brand_name'] == name for added in self.added_brands)
                # ãƒã‚¤ãƒ©ã‚¤ãƒˆå¯¾è±¡ã‹ãƒã‚§ãƒƒã‚¯
                is_highlight = highlight_brands and name in highlight_brands
                
                if is_new:
                    colors.append('red')
                    sizes.append(12)
                elif is_highlight:
                    colors.append('orange')
                    sizes.append(10)
                else:
                    colors.append('steelblue')
                    sizes.append(8)
                
                # ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                hover_text = f"<b>{name}</b><br>"
                hover_text += f"åº§æ¨™: ({coords_2d[i, 0]:.3f}, {coords_2d[i, 1]:.3f})<br>"
                if is_new:
                    hover_text += "<b>ğŸ†• æ–°è¦è¿½åŠ </b><br>"
                hover_text += f"èª¬æ˜: {desc[:100]}..."
                hover_texts.append(hover_text)
            
            # Plotlyå¯è¦–åŒ–
            fig = go.Figure()
            
            # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’ä¸€è‡´ã•ã›ã‚‹
            max_length = min(len(coords_2d), len(colors), len(sizes), len(hover_texts), len(unique_brand_names))
            
            # ãƒ¡ã‚¤ãƒ³ã®æ•£å¸ƒå›³
            fig.add_trace(go.Scatter(
                x=coords_2d[:max_length, 0],
                y=coords_2d[:max_length, 1],
                mode='markers',
                marker=dict(
                    color=colors[:max_length],
                    size=sizes[:max_length],
                    opacity=0.8,
                    line=dict(width=1, color='white')
                ),
                text=unique_brand_names[:max_length],
                hovertemplate='%{hovertext}<extra></extra>',
                hovertext=hover_texts[:max_length],
                showlegend=False
            ))
            
            # æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ã®å¼·èª¿
            if self.added_brands:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ãƒã‚§ãƒƒã‚¯
                valid_indices = [added['index'] for added in self.added_brands 
                               if added['index'] < len(coords_2d) and added['index'] < len(self.brand_names)]
                if valid_indices:
                    new_coords = coords_2d[valid_indices]
                    new_names = [self.brand_names[i] for i in valid_indices]
                
                    fig.add_trace(go.Scatter(
                        x=new_coords[:, 0],
                        y=new_coords[:, 1],
                        mode='markers+text',
                        marker=dict(
                            color='red',
                            size=15,
                            symbol='star',
                            line=dict(width=2, color='darkred')
                        ),
                        text=new_names,
                        textposition='top center',
                        textfont=dict(size=10, color='darkred'),
                        name='æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰',
                        showlegend=True
                    ))
            
            fig.update_layout(
                title={
                    'text': 'ğŸ§  ãƒ–ãƒ©ãƒ³ãƒ‰ç‰¹å¾´é‡ç©ºé–“ (Ruri v3 + UMAP)',
                    'x': 0.5,
                    'font': {'size': 18}
                },
                xaxis_title='UMAPæ¬¡å…ƒ1',
                yaxis_title='UMAPæ¬¡å…ƒ2',
                width=900,
                height=600,
                hovermode='closest',
                plot_bgcolor='white',
                paper_bgcolor='white'
            )
            
            return fig
            
        except Exception as e:
            st.error(f"å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def create_genre_center_visualization(self, selected_genres=None):
        """ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã®ç‰¹å¾´é‡ç©ºé–“å¯è¦–åŒ–"""
        if self.embeddings is None or len(self.embeddings) < 2:
            st.warning("å¯è¦–åŒ–ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        
        if not self.brand_genre_mapping:
            st.warning("ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None
        
        try:
            # UMAPæ¬¡å…ƒå‰Šæ¸›
            reducer = umap.UMAP(
                n_components=2,
                n_neighbors=min(15, len(self.embeddings) - 1),
                min_dist=0.1,
                metric='cosine',
                random_state=42
            )
            
            coords_2d = reducer.fit_transform(self.embeddings)
            
            # ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
            genre_centers = {}
            genre_coords = {}
            
            for genre, brand_list in self.genre_brands.items():
                if selected_genres and genre not in selected_genres:
                    continue
                
                # ã‚¸ãƒ£ãƒ³ãƒ«ã«å±ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                brand_indices = []
                for brand_name in brand_list:
                    if brand_name in self.brand_names:
                        brand_indices.append(self.brand_names.index(brand_name))
                
                if len(brand_indices) > 0:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ãƒã‚§ãƒƒã‚¯
                    valid_indices = [idx for idx in brand_indices if idx < len(self.embeddings) and idx < len(coords_2d)]
                    if valid_indices:
                        # ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ï¼ˆåŸ‹ã‚è¾¼ã¿ç©ºé–“ï¼‰
                        genre_embeddings = self.embeddings[valid_indices]
                        center_embedding = np.mean(genre_embeddings, axis=0)
                        genre_centers[genre] = center_embedding
                        
                        # 2Dåº§æ¨™ã§ã®ä¸­å¿ƒã‚‚è¨ˆç®—
                        genre_2d_coords = coords_2d[valid_indices]
                        center_2d = np.mean(genre_2d_coords, axis=0)
                        genre_coords[genre] = center_2d
            
            # å¯è¦–åŒ–ä½œæˆ
            fig = go.Figure()
            
            # å…¨ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆè–„ãè¡¨ç¤ºï¼‰
            colors = []
            hover_texts = []
            sizes = []
            
            for i, (name, desc) in enumerate(zip(self.brand_names, self.brand_descriptions)):
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ãƒã‚§ãƒƒã‚¯
                if i >= len(coords_2d):
                    break
                    
                brand_genres = self.brand_genre_mapping.get(name, [])
                
                # é¸æŠã•ã‚ŒãŸã‚¸ãƒ£ãƒ³ãƒ«ã«å±ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if selected_genres:
                    has_selected_genre = any(g in selected_genres for g in brand_genres)
                    if has_selected_genre:
                        colors.append('lightblue')
                        sizes.append(6)
                    else:
                        colors.append('lightgray')
                        sizes.append(3)
                else:
                    colors.append('lightblue')
                    sizes.append(6)
                
                # ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                hover_text = f"<b>{name}</b><br>"
                hover_text += f"ã‚¸ãƒ£ãƒ³ãƒ«: {', '.join(brand_genres) if brand_genres else 'ãªã—'}<br>"
                hover_text += f"åº§æ¨™: ({coords_2d[i, 0]:.3f}, {coords_2d[i, 1]:.3f})<br>"
                hover_text += f"èª¬æ˜: {desc[:100]}..."
                hover_texts.append(hover_text)
            
            # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’ä¸€è‡´ã•ã›ã‚‹
            max_length = min(len(coords_2d), len(colors), len(sizes), len(hover_texts), len(self.brand_names))
            
            # ãƒ–ãƒ©ãƒ³ãƒ‰æ•£å¸ƒå›³
            fig.add_trace(go.Scatter(
                x=coords_2d[:max_length, 0],
                y=coords_2d[:max_length, 1],
                mode='markers',
                marker=dict(
                    color=colors[:max_length],
                    size=sizes[:max_length],
                    opacity=0.6,
                    line=dict(width=0.5, color='white')
                ),
                text=self.brand_names[:max_length],
                hovertemplate='%{hovertext}<extra></extra>',
                hovertext=hover_texts[:max_length],
                name='ãƒ–ãƒ©ãƒ³ãƒ‰',
                showlegend=True
            ))
            
            # ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            center_colors = ['red', 'orange', 'green', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan', 'magenta']
            
            for i, (genre, coord) in enumerate(genre_coords.items()):
                color = center_colors[i % len(center_colors)]
                brand_count = len(self.genre_brands[genre])
                
                fig.add_trace(go.Scatter(
                    x=[coord[0]],
                    y=[coord[1]],
                    mode='markers+text',
                    marker=dict(
                        color=color,
                        size=20,
                        symbol='diamond',
                        line=dict(width=3, color='black')
                    ),
                    text=[f"{genre}"],
                    textposition='top center',
                    textfont=dict(size=12, color='black'),
                    name=f'{genre} ({brand_count}ãƒ–ãƒ©ãƒ³ãƒ‰)',
                    hovertemplate=f"<b>{genre}</b><br>ãƒ–ãƒ©ãƒ³ãƒ‰æ•°: {brand_count}<br>åº§æ¨™: ({coord[0]:.3f}, {coord[1]:.3f})<extra></extra>",
                    showlegend=True
                ))
            
            fig.update_layout(
                title={
                    'text': 'ğŸ¯ ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«å¯è¦–åŒ– (Ruri v3 + UMAP)',
                    'x': 0.5,
                    'font': {'size': 18}
                },
                xaxis_title='UMAPæ¬¡å…ƒ1',
                yaxis_title='UMAPæ¬¡å…ƒ2',
                width=1000,
                height=700,
                hovermode='closest',
                plot_bgcolor='white',
                paper_bgcolor='white',
                legend=dict(
                    yanchor="top",
                    y=0.99,
                    xanchor="left",
                    x=1.01
                )
            )
            
            return fig
            
        except Exception as e:
            st.error(f"ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒå¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    st.markdown('<h1 class="main-header">ğŸ¯ çµ±åˆãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ </h1>', unsafe_allow_html=True)
    st.markdown("**æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ  â†’ Geminièª¬æ˜æ–‡ç”Ÿæˆ â†’ Ruri v3ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ é¡ä¼¼åº¦æ¤œç´¢ â†’ ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚° â†’ å¯è¦–åŒ–**")
    st.markdown("---")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if 'search_system' not in st.session_state:
        with st.spinner("ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
            search_system = IntegratedBrandSearchSystem()
            if search_system.initialize():
                st.session_state.search_system = search_system
                st.success("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            else:
                st.error("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
                return
    
    search_system = st.session_state.search_system
    
    # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ - ã‚¿ãƒ–å½¢å¼
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”ğŸ†• ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ & è¿½åŠ ", "ğŸ“‚ CSVä¸€æ‹¬è¿½åŠ ", "ğŸ§  ç‰¹å¾´é‡ç©ºé–“è§£æ", "ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±"])
    
    with tab1:
        show_integrated_brand_interface(search_system)
    
    with tab2:
        show_csv_batch_interface(search_system)
    
    with tab3:
        show_advanced_visualization(search_system)
    
    with tab4:
        show_system_status(search_system)

def show_integrated_brand_interface(search_system):
    """ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ã¨æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ã®çµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ”ğŸ†• ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ & æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ")
    
    # 2åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    col1, col2 = st.columns([1, 1], gap="large")
    
    with col1:
        st.subheader("ğŸ” ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢")
        show_search_panel(search_system)
    
    with col2:
        st.subheader("â• æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ")
        show_addition_panel(search_system)
    
    # å…±é€šã®çµæœè¡¨ç¤ºã‚¨ãƒªã‚¢
    st.markdown("---")
    show_unified_results(search_system)

def show_search_panel(search_system):
    """æ¤œç´¢ãƒ‘ãƒãƒ«"""
    if not search_system.brand_names:
        st.warning("æ¤œç´¢å¯èƒ½ãªãƒ–ãƒ©ãƒ³ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    with st.container():
        st.markdown('<div class="search-card">', unsafe_allow_html=True)
        
        target_brand = st.selectbox(
            "ğŸ¯ æ¤œç´¢ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰:",
            search_system.brand_names,
            key="search_brand"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            top_k = st.slider("è¡¨ç¤ºä»¶æ•°:", 5, 100, 10, key="search_top_k")
        with col2:
            min_similarity = st.slider("æœ€å°é¡ä¼¼åº¦:", 0.0, 1.0, 0.1, 0.05, key="search_min_sim")
        
        # æ­£è¦åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        normalize_similarity = st.checkbox("ğŸ“Š é¡ä¼¼åº¦ã‚’ã‚®ãƒ£ãƒ©ãƒªå†…ã§æ­£è¦åŒ–ï¼ˆå·®ã‚’å¼·èª¿ï¼‰", 
                                          value=False, 
                                          help="æ¤œç´¢çµæœã®é¡ä¼¼åº¦ã‚’min-maxæ­£è¦åŒ–ã—ã€å·®ã‚’åˆ†ã‹ã‚Šã‚„ã™ãã—ã¾ã™")
        
        # ã‚¸ãƒ£ãƒ³ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
        st.markdown("##### ğŸ¯ ã‚¸ãƒ£ãƒ³ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
        if search_system.genre_brands:
            available_genres = sorted(search_system.genre_brands.keys())
            enable_genre_filter = st.checkbox("ã‚¸ãƒ£ãƒ³ãƒ«é™å®šæ¤œç´¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=False, key="enable_genre_filter")
            
            if enable_genre_filter:
                selected_genres = st.multiselect(
                    "å¯¾è±¡ã‚¸ãƒ£ãƒ³ãƒ«ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:",
                    available_genres,
                    key="selected_genres",
                    help="é¸æŠã•ã‚ŒãŸã‚¸ãƒ£ãƒ³ãƒ«ã®åº—èˆ—ã‚’æŒã¤ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã¿ãŒæ¤œç´¢å¯¾è±¡ã«ãªã‚Šã¾ã™"
                )
            else:
                selected_genres = None
        else:
            enable_genre_filter = False
            selected_genres = None
            st.info("â„¹ï¸ ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
        # st.markdown("##### ğŸª åº—èˆ—ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®š")
        # enable_reranking = st.checkbox("ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=True, key="search_rerank")
        
        # if enable_reranking:
        #     col_bias, col_method = st.columns(2)
        #     with col_bias:
        #         # åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨å€¤: 0.005-0.025
        #         bias_strength = st.slider("ãƒã‚¤ã‚¢ã‚¹å¼·åº¦:", 0.001, 0.050, 0.015, 0.005, key="search_bias", 
        #                                 help="æ¨å¥¨å€¤: 0.005-0.025 (åˆ†æçµæœã‚ˆã‚Š)")
        #     with col_method:
        #         rerank_mode = st.selectbox(
        #             "ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°æ–¹å¼:",
        #             ["weighted_average", "linear_addition", "location_rerank"],
        #             index=0,
        #             key="search_rerank_mode",
        #             help="weighted_average: é‡ã¿ä»˜ãå¹³å‡, linear_addition: ç·šå½¢åŠ ç®—, location_rerank: ä½ç½®é‡è¦–"
        #         )
        #     
        #     location_method = st.selectbox(
        #         "ä½ç½®é¡ä¼¼åº¦è¨ˆç®—æ–¹æ³•:",
        #         ["comprehensive", "tenant", "building", "floor", "geographic", "area"],
        #         index=0,
        #         key="search_location_method",
        #         help="comprehensive: ç·åˆ, tenant: ãƒ†ãƒŠãƒ³ãƒˆ, building: ãƒ“ãƒ«, floor: ãƒ•ãƒ­ã‚¢, geographic: åœ°ç†çš„è·é›¢, area: ã‚¨ãƒªã‚¢"
        #     )
        #     
        #     # å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆæˆ¦ç•¥ã®è¿½åŠ 
        #     use_fixed_boost = st.checkbox("ğŸ¯ å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆæˆ¦ç•¥ã‚’ä½¿ç”¨", value=False, key="search_fixed_boost",
        #                                 help="ä½ç½®ä¸€è‡´ç‡ã«å¿œã˜ãŸæ®µéšçš„å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆå€¤ã‚’é©ç”¨")
        # else:
        #     bias_strength = 0.0
        #     rerank_mode = "weighted_average"
        #     location_method = "comprehensive"
        #     use_fixed_boost = False
        enable_reranking = False
        bias_strength = 0.0
        rerank_mode = "weighted_average"
        location_method = "comprehensive"
        use_fixed_boost = False
        
        if st.button("ğŸš€ æ¤œç´¢å®Ÿè¡Œ", type="primary", key="search_execute"):
            with st.spinner("é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ä¸­..."):
                # ã‚¸ãƒ£ãƒ³ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã—ã¦æ¤œç´¢
                genre_filter = selected_genres if enable_genre_filter and selected_genres else None
                results = search_system.search_similar_brands(target_brand, top_k, min_similarity, genre_filter=genre_filter, normalize_similarity=normalize_similarity)
                
                # if enable_reranking and results:
                #     # å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆæˆ¦ç•¥ã¾ãŸã¯å¾“æ¥æ–¹å¼ã‚’é¸æŠ
                #     if use_fixed_boost:
                #         results = search_system.apply_fixed_boost_reranking(
                #             results, target_brand, location_method
                #         )
                #     else:
                #         results = search_system.apply_location_reranking(
                #             results, target_brand, bias_strength, location_method, rerank_mode
                #         )
                
                st.session_state['search_results'] = results
                st.session_state['search_target'] = target_brand
                
                if results:
                    st.success(f"âœ… {len(results)} ä»¶ã®é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ç™ºè¦‹")
                else:
                    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        st.markdown('</div>', unsafe_allow_html=True)

def show_addition_panel(search_system):
    """æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ãƒ‘ãƒãƒ«"""
    with st.container():
        st.markdown('<div class="search-card">', unsafe_allow_html=True)
        
        api_key = st.text_input(
            "ğŸ”‘ GPT-OSSè¨­å®š (ä»»æ„å…¥åŠ›):",
            type="password",
            help="ãƒ­ãƒ¼ã‚«ãƒ«GPT-OSSã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰",
            key="add_api_key",
            placeholder="ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã®ãŸã‚å…¥åŠ›ä¸è¦"
        )
        
        brand_name = st.text_input(
            "ğŸ·ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰å:", 
            placeholder="ä¾‹: Stone Island, Fear of God",
            key="add_brand_name"
        )
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        st.markdown("##### ğŸ”§ èª¬æ˜æ–‡ç”Ÿæˆè¨­å®š")
        fallback_mode = st.radio(
            "GPT-OSSæ¥ç¶šå¤±æ•—æ™‚ã®å‹•ä½œ:",
            options=["template", "manual"],
            format_func=lambda x: "ğŸ“ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨" if x == "template" else "âœï¸ æ‰‹å‹•å…¥åŠ›",
            index=0,
            key="add_fallback_mode",
            help="template: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨, manual: æ‰‹å‹•ã§èª¬æ˜æ–‡ã‚’å…¥åŠ›"
        )
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯è¨­å®š
        strict_duplicate_check = st.checkbox(
            "ğŸ” å³å¯†ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯",
            value=True,
            key="add_strict_check",
            help="æœ‰åŠ¹: å®Œå…¨ä¸€è‡´ã®ã¿ãƒã‚§ãƒƒã‚¯ / ç„¡åŠ¹: é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰åã‚‚ãƒã‚§ãƒƒã‚¯"
        )
        
        # æ‰‹å‹•å…¥åŠ›ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ï¼ˆæ¡ä»¶ä»˜ãè¡¨ç¤ºï¼‰
        manual_description = None
        if fallback_mode == "manual":
            manual_description = st.text_area(
                "âœï¸ ãƒ–ãƒ©ãƒ³ãƒ‰èª¬æ˜æ–‡ (æ‰‹å‹•å…¥åŠ›):",
                placeholder="ãƒ–ãƒ©ãƒ³ãƒ‰ã®ç‰¹å¾´ã€èµ·æºã€ã‚¹ã‚¿ã‚¤ãƒ«ãªã©ã‚’200-400æ–‡å­—ç¨‹åº¦ã§å…¥åŠ›ã—ã¦ãã ã•ã„",
                height=120,
                key="add_manual_description"
            )
        
        # æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ æ™‚ã®ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
        # st.markdown("##### ğŸª è¿½åŠ å¾Œãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®š")
        # with st.expander("ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è©³ç´°è¨­å®š"):
        #     add_enable_rerank = st.checkbox("è¿½åŠ å¾Œã«ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’é©ç”¨", value=True, key="add_rerank_enable")
        #     if add_enable_rerank:
        #         col_add_bias, col_add_method = st.columns(2)
        #         with col_add_bias:
        #             # æ¨å¥¨å€¤ã«åŸºã¥ãèª¿æ•´
        #             add_bias_strength = st.slider("ãƒã‚¤ã‚¢ã‚¹å¼·åº¦:", 0.001, 0.050, 0.015, 0.005, key="add_bias",
        #                                          help="æ¨å¥¨å€¤: 0.005-0.025")
        #         with col_add_method:
        #             add_rerank_mode = st.selectbox(
        #                 "ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°æ–¹å¼:",
        #                 ["weighted_average", "linear_addition", "location_rerank"],
        #                 index=0,
        #                 key="add_rerank_mode"
        #             )
        #         
        #         add_location_method = st.selectbox(
        #             "ä½ç½®é¡ä¼¼åº¦è¨ˆç®—æ–¹æ³•:",
        #             ["comprehensive", "tenant", "building", "floor", "geographic", "area"],
        #             index=0,
        #             key="add_location_method"
        #         )
        #         
        #         add_use_fixed_boost = st.checkbox("ğŸ¯ å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆæˆ¦ç•¥ã‚’ä½¿ç”¨", value=False, key="add_fixed_boost")
        #     else:
        #         add_bias_strength = 0.015  # æ¨å¥¨å€¤ã«å¤‰æ›´
        #         add_rerank_mode = "weighted_average"  
        #         add_location_method = "comprehensive"
        #         add_use_fixed_boost = False
        add_enable_rerank = False
        add_bias_strength = 0.015
        add_rerank_mode = "weighted_average"
        add_location_method = "comprehensive"
        add_use_fixed_boost = False
        
        # ãƒœã‚¿ãƒ³ã®æœ‰åŠ¹åŒ–åˆ¤å®š
        button_disabled = not brand_name or (fallback_mode == "manual" and not manual_description)
        
        if st.button("ğŸš€ ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ", type="primary", disabled=button_disabled, key="add_execute"):
            
            # æ—¢å­˜ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚§ãƒƒã‚¯
            exists, existing_name = search_system.check_brand_exists(brand_name, strict=strict_duplicate_check)
            if exists:
                if existing_name == brand_name:
                    st.warning(f"âš ï¸ '{brand_name}' ã¯æ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚")
                else:
                    st.warning(f"âš ï¸ '{brand_name}' ã«é¡ä¼¼ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ '{existing_name}' ãŒæ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚")
                return
            
            description = None
            
            # æ‰‹å‹•å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
            if fallback_mode == "manual" and manual_description:
                description = manual_description.strip()
                st.markdown('<div class="new-brand-highlight">', unsafe_allow_html=True)
                st.success("âœ… æ‰‹å‹•å…¥åŠ›ã®èª¬æ˜æ–‡ã‚’ä½¿ç”¨")
                with st.expander("ğŸ“ å…¥åŠ›ã•ã‚ŒãŸèª¬æ˜æ–‡", expanded=True):
                    st.write(description)
                st.markdown('</div>', unsafe_allow_html=True)
            else:
                # Step 1: GPT-OSSã§èª¬æ˜æ–‡ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰
                with st.spinner("GPT-OSSã§èª¬æ˜æ–‡ç”Ÿæˆä¸­..."):
                    description = search_system.generate_brand_description(api_key, brand_name, fallback_mode=fallback_mode)
                    
                if description:
                    st.markdown('<div class="new-brand-highlight">', unsafe_allow_html=True)
                    st.success("âœ… èª¬æ˜æ–‡ç”Ÿæˆå®Œäº†")
                    with st.expander("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡", expanded=True):
                        st.write(description)
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Step 2: Ruri v3ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†è¿½åŠ 
                    with st.spinner("Ruri v3ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­..."):
                        success = search_system.add_new_brand(brand_name, description)
                    
                    if success:
                        st.balloons()
                        st.success(f"ğŸ‰ {brand_name} ã‚’ã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ ã—ã¾ã—ãŸï¼")
                        
                        # å³åº§ã«é¡ä¼¼æ¤œç´¢å®Ÿè¡Œï¼ˆãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨ï¼‰
                        similar_brands = search_system.search_similar_brands(brand_name, top_k=5)
                        
                        # ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨ï¼ˆæ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ æ™‚ï¼‰
                        # if add_enable_rerank and search_system.reranker and similar_brands:
                        #     if add_use_fixed_boost:
                        #         similar_brands = search_system.apply_fixed_boost_reranking(
                        #             similar_brands, brand_name, add_location_method
                        #         )
                        #     else:
                        #         similar_brands = search_system.apply_location_reranking(
                        #             similar_brands, brand_name, bias_strength=add_bias_strength, 
                        #             location_method=add_location_method, rerank_mode=add_rerank_mode
                        #         )
                        
                        st.session_state['add_results'] = similar_brands
                        st.session_state['added_brand'] = brand_name
                        
                        if similar_brands:
                            st.info(f"ğŸ” {brand_name} ã®é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ç¢ºèªã§ãã¾ã™ï¼ˆä¸‹ã®çµæœã‚¨ãƒªã‚¢ã‚’ã”è¦§ãã ã•ã„ï¼‰")
                else:
                    st.error("èª¬æ˜æ–‡ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        st.markdown('</div>', unsafe_allow_html=True)

def show_unified_results(search_system):
    """çµ±åˆçµæœè¡¨ç¤ºã‚¨ãƒªã‚¢"""
    st.subheader("ğŸ“Š æ¤œç´¢ãƒ»è¿½åŠ çµæœ")
    
    # æ¤œç´¢çµæœè¡¨ç¤º
    if 'search_results' in st.session_state and st.session_state['search_results']:
        st.markdown("### ğŸ” æ¤œç´¢çµæœ")
        
        results = st.session_state['search_results']
        target_brand = st.session_state.get('search_target', '')
        
        # è¡¨å½¢å¼è¡¨ç¤º
        display_data = []
        for i, result in enumerate(results, 1):
            brand_indicator = "ğŸ†•" if result.get('is_new', False) else ""
            
            # è¨ˆç®—å¼ã®æ˜ç¢ºåŒ–
            base_sim = result['similarity']
            final_sim = result.get('final_similarity', base_sim)
            boost = result.get('location_boost', 0.0)
            location_sim = result.get('location_similarity', 0.0)
            
            # å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆã®å ´åˆã®ç‰¹åˆ¥è¡¨ç¤º
            rerank_mode = result.get('rerank_mode', 'none')
            boost_info = ''
            if rerank_mode == 'fixed_boost':
                tier = result.get('boost_tier', '0%')
                boost_info = f"{tier} ({boost:+.3f})"
            else:
                boost_info = f"{boost:+.4f}"
            
            display_data.append({
                'é †ä½': f"#{i}",
                'ãƒ–ãƒ©ãƒ³ãƒ‰': f"{brand_indicator} {result['brand_name']}",
                'é¡ä¼¼åº¦': f"{final_sim:.4f}"
            })
        
        df_display = pd.DataFrame(display_data)
        st.dataframe(df_display, use_container_width=True, hide_index=True)
        
        # è©³ç´°æƒ…å ±å±•é–‹
        with st.expander("ğŸ“‹ è©³ç´°æƒ…å ±"):
            for i, result in enumerate(results[:5], 1):
                st.markdown(f"**{i}. {result['brand_name']}**")
                if result.get('normalized', False):
                    st.write(f"é¡ä¼¼åº¦: {result['similarity']:.4f} (æ­£è¦åŒ–å¾Œ)")
                    st.write(f"å…ƒã®é¡ä¼¼åº¦: {result['original_similarity']:.4f}")
                else:
                    st.write(f"é¡ä¼¼åº¦: {result['similarity']:.4f}")
                st.write(f"èª¬æ˜: {result['description'][:200]}...")
                st.markdown("---")
    
    # è¿½åŠ çµæœè¡¨ç¤º
    if 'add_results' in st.session_state and st.session_state['add_results']:
        st.markdown("### ğŸ†• æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ã®é¡ä¼¼åº¦")
        
        results = st.session_state['add_results']
        added_brand = st.session_state.get('added_brand', '')
        
        # è¡¨å½¢å¼è¡¨ç¤º
        display_data = []
        for i, result in enumerate(results, 1):
            brand_indicator = "ğŸ†•" if result.get('is_new', False) else ""
            
            # è¨ˆç®—å¼ã®æ˜ç¢ºåŒ–
            base_sim = result['similarity']
            final_sim = result.get('final_similarity', base_sim)
            boost = result.get('location_boost', 0.0)
            location_sim = result.get('location_similarity', 0.0)
            
            # å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆã®å ´åˆã®ç‰¹åˆ¥è¡¨ç¤º
            rerank_mode = result.get('rerank_mode', 'none')
            boost_info = ''
            if rerank_mode == 'fixed_boost':
                tier = result.get('boost_tier', '0%')
                boost_info = f"{tier} ({boost:+.3f})"
            else:
                boost_info = f"{boost:+.4f}"
            
            display_data.append({
                'é †ä½': f"#{i}",
                'ãƒ–ãƒ©ãƒ³ãƒ‰': f"{brand_indicator} {result['brand_name']}",
                'é¡ä¼¼åº¦': f"{final_sim:.4f}"
            })
        
        df_display = pd.DataFrame(display_data)
        st.dataframe(df_display, use_container_width=True, hide_index=True)
        
        # è©³ç´°æƒ…å ±å±•é–‹
        with st.expander("ğŸ“‹ è©³ç´°æƒ…å ±"):
            for i, result in enumerate(results, 1):
                st.markdown(f"**{i}. {result['brand_name']}**")
                if result.get('normalized', False):
                    st.write(f"é¡ä¼¼åº¦: {result['similarity']:.4f} (æ­£è¦åŒ–å¾Œ)")
                    st.write(f"å…ƒã®é¡ä¼¼åº¦: {result['original_similarity']:.4f}")
                else:
                    st.write(f"é¡ä¼¼åº¦: {result['similarity']:.4f}")
                st.write(f"èª¬æ˜: {result['description'][:200]}...")
                st.markdown("---")
    

def show_system_status(search_system):
    """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º"""
    st.header("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
    
    # çµ±è¨ˆæƒ…å ±
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ç·ãƒ–ãƒ©ãƒ³ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºï¼‰", len(search_system.brand_names))
    with col2:
        st.metric("æ–°è¦è¿½åŠ æ•°", len(search_system.added_brands))
    with col3:
        if search_system.embeddings is not None:
            st.metric("åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ", search_system.embeddings.shape[1])
    with col4:
        # reranker_status = "æœ‰åŠ¹" if search_system.reranker else "ç„¡åŠ¹"
        # st.metric("ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°", reranker_status)
        pass
    
    # åˆ†æçµæœã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
    st.subheader("ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.info("ğŸ¯ **æ¨å¥¨ãƒ–ãƒ¼ã‚¹ãƒˆå€¤**: 0.005-0.025")
    with col2:
        st.info("ğŸ”— **ãƒ–ãƒ©ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°**: è‡ªå‹•éƒ¨åˆ†ä¸€è‡´")
    with col3:
        st.info("âš™ï¸ **å›ºå®šãƒ–ãƒ¼ã‚¹ãƒˆ**: ä½ç½®ä¸€è‡´ç‡ãƒ™ãƒ¼ã‚¹")
    
    # ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸ã®è©³ç´°æƒ…å ±
    st.subheader("ğŸ“š ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸æƒ…å ±")
    col1, col2 = st.columns(2)
    
    with col1:
        if search_system.embeddings is not None:
            st.write(f"**è¾æ›¸ã‚µã‚¤ã‚º**: {search_system.embeddings.shape}")
            st.write(f"**ç·ãƒ™ã‚¯ãƒˆãƒ«æ•°**: {len(search_system.brand_names)}")
            st.write(f"**æ¬¡å…ƒæ•°**: {search_system.embeddings.shape[1] if len(search_system.embeddings.shape) > 1 else 'N/A'}")
        else:
            st.write("ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
    
    with col2:
        if search_system.brand_names:
            st.write(f"**æœ€æ–°ãƒ–ãƒ©ãƒ³ãƒ‰**: {search_system.brand_names[-1] if search_system.brand_names else 'N/A'}")
            st.write(f"**æœ€å¤ãƒ–ãƒ©ãƒ³ãƒ‰**: {search_system.brand_names[0] if search_system.brand_names else 'N/A'}")
            new_brands_count = len(search_system.added_brands)
            st.write(f"**ä»Šå›ã‚»ãƒƒã‚·ãƒ§ãƒ³è¿½åŠ æ•°**: {new_brands_count}")

    # æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è¦§
    if search_system.added_brands:
        st.subheader("ğŸ†• æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ï¼ˆä»Šå›ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰")
        for i, added in enumerate(search_system.added_brands, 1):
            with st.expander(f"**{i}. {added['name']}** (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {added['index']})"):
                st.write(f"**èª¬æ˜:** {added['description']}")
                st.write(f"**ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹:** {added['index']}")
                st.write(f"**å‡¦ç†ã‚µã‚¤ã‚¯ãƒ«:** âœ… å®Œäº†")
    
    # ã‚·ã‚¹ãƒ†ãƒ è¨­å®šæƒ…å ±
    st.subheader("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ è¨­å®š")
    
    col1, col2 = st.columns(2)
    with col1:
        st.write("**ãƒ¢ãƒ‡ãƒ«æƒ…å ±:**")
        st.write("- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: Ruri v3 (cl-nagoya/ruri-v3-310m)")
        st.write("- èª¬æ˜æ–‡ç”Ÿæˆ: GPT-OSS 20B (ãƒ­ãƒ¼ã‚«ãƒ«)")
        st.write("- æ¬¡å…ƒå‰Šæ¸›: UMAP")
        
    with col2:
        st.write("**æ©Ÿèƒ½çŠ¶æ…‹:**")
        st.write(f"- ãƒ™ãƒ¼ã‚¹ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼: {'âœ…' if search_system.base_analyzer else 'âŒ'}")
        st.write(f"- Ruriãƒ¢ãƒ‡ãƒ«: {'âœ…' if search_system.ruri_model else 'âŒ'}")
        # GPT-OSSæ¥ç¶šçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
        server_status = search_system.check_gpt_oss_connection()
        st.write(f"- GPT-OSSã‚µãƒ¼ãƒãƒ¼: {'âœ…' if server_status else 'âŒ'}")
        
        if not server_status:
            st.info("ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¾ãŸã¯æ‰‹å‹•å…¥åŠ›ã§èª¬æ˜æ–‡ç”Ÿæˆå¯èƒ½")

def show_brand_addition(search_system):
    """æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ æ©Ÿèƒ½"""
    st.header("â• æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ")
    
    with st.container():
        st.markdown('<div class="search-card">', unsafe_allow_html=True)
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            api_key = st.text_input(
                "ğŸ”‘ GPT-OSSè¨­å®š (ä»»æ„å…¥åŠ›):",
                type="password",
                help="ãƒ­ãƒ¼ã‚«ãƒ«GPT-OSSã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰",
                placeholder="ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã®ãŸã‚å…¥åŠ›ä¸è¦"
            )
        
        with col2:
            brand_name = st.text_input(
                "ğŸ·ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰å:", 
                placeholder="ä¾‹: Stone Island, Fear of God"
            )
        
        if st.button("ğŸš€ ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ", type="primary", disabled=not brand_name):
            
            # Step 1: GPT-OSSã§èª¬æ˜æ–‡ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰
            with st.spinner("GPT-OSSã§èª¬æ˜æ–‡ç”Ÿæˆä¸­..."):
                description = search_system.generate_brand_description(api_key, brand_name, fallback_mode="template")
                
            if description:
                st.markdown('<div class="new-brand-highlight">', unsafe_allow_html=True)
                st.success("âœ… èª¬æ˜æ–‡ç”Ÿæˆå®Œäº†")
                st.write("**ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡:**")
                st.write(description)
                st.markdown('</div>', unsafe_allow_html=True)
                
                # Step 2: Ruri v3ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†è¿½åŠ 
                with st.spinner("Ruri v3ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­..."):
                    success = search_system.add_new_brand(brand_name, description)
                
                if success:
                    st.balloons()
                    st.success(f"ğŸ‰ {brand_name} ã‚’ã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ ã—ã¾ã—ãŸï¼")
                    
                    # å³åº§ã«é¡ä¼¼æ¤œç´¢å®Ÿè¡Œï¼ˆãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨ï¼‰
                    st.info("ğŸ” è¿½åŠ ã—ãŸãƒ–ãƒ©ãƒ³ãƒ‰ã®é¡ä¼¼åº¦ã‚’ç¢ºèª...")
                    similar_brands = search_system.search_similar_brands(brand_name, top_k=5)
                    
                    # ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨
                    # if search_system.reranker and similar_brands:
                    #     similar_brands = search_system.apply_location_reranking(
                    #         similar_brands, brand_name, bias_strength=0.3
                    #     )
                    
                    if similar_brands:
                        st.write("**é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰:**")
                        for i, result in enumerate(similar_brands, 1):
                            brand_indicator = "ğŸ†•" if result.get('is_new', False) else ""
                            final_sim = result.get('final_similarity', result['similarity'])
                            boost = result.get('location_boost', 0)
                            if boost > 0.001:
                                st.write(f"{i}. {brand_indicator} {result['brand_name']} (é¡ä¼¼åº¦: {result['similarity']:.4f})")
            else:
                st.error("èª¬æ˜æ–‡ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        st.markdown('</div>', unsafe_allow_html=True)

def show_brand_search(search_system):
    """ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢æ©Ÿèƒ½"""
    st.header("ğŸ” ãƒ–ãƒ©ãƒ³ãƒ‰é¡ä¼¼åº¦æ¤œç´¢")
    
    if not search_system.brand_names:
        st.warning("æ¤œç´¢å¯èƒ½ãªãƒ–ãƒ©ãƒ³ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    with st.container():
        st.markdown('<div class="search-card">', unsafe_allow_html=True)
        
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            target_brand = st.selectbox(
                "ğŸ¯ æ¤œç´¢ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰:",
                search_system.brand_names
            )
        
        with col2:
            top_k = st.slider("è¡¨ç¤ºä»¶æ•°:", 5, 20, 10)
        
        with col3:
            min_similarity = st.slider("æœ€å°é¡ä¼¼åº¦:", 0.0, 1.0, 0.1, 0.05)
        
        # æ­£è¦åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        normalize_similarity_simple = st.checkbox("ğŸ“Š é¡ä¼¼åº¦ã‚’ã‚®ãƒ£ãƒ©ãƒªå†…ã§æ­£è¦åŒ–", value=False)
        
        # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
        # enable_reranking = st.checkbox("ğŸª ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°", value=True)
        # if enable_reranking:
        #     bias_strength = st.slider("ãƒã‚¤ã‚¢ã‚¹å¼·åº¦:", 0.0, 1.0, 0.3, 0.1)
        enable_reranking = False
        
        if st.button("ğŸš€ æ¤œç´¢å®Ÿè¡Œ", type="primary"):
            with st.spinner("é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ä¸­..."):
                # åŸºæœ¬æ¤œç´¢
                results = search_system.search_similar_brands(target_brand, top_k, min_similarity, normalize_similarity=normalize_similarity_simple)
                
                # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨
                # if enable_reranking and results:
                #     results = search_system.apply_location_reranking(
                #         results, target_brand, bias_strength
                #     )
                
                # çµæœè¡¨ç¤º
                if results:
                    st.success(f"âœ… {len(results)} ä»¶ã®é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ç™ºè¦‹")
                    
                    # è¡¨å½¢å¼è¡¨ç¤º
                    display_data = []
                    for i, result in enumerate(results, 1):
                        brand_indicator = "ğŸ†•" if result.get('is_new', False) else ""
                        display_data.append({
                            'é †ä½': f"#{i}",
                            'ãƒ–ãƒ©ãƒ³ãƒ‰': f"{brand_indicator} {result['brand_name']}",
                            'é¡ä¼¼åº¦': f"{result['similarity']:.4f}"
                        })
                    
                    df_display = pd.DataFrame(display_data)
                    st.dataframe(df_display, use_container_width=True, hide_index=True)
                    
                    # è©³ç´°æƒ…å ±å±•é–‹
                    with st.expander("ğŸ“‹ è©³ç´°æƒ…å ±"):
                        for i, result in enumerate(results[:5], 1):
                            st.markdown(f"**{i}. {result['brand_name']}**")
                            if result.get('normalized', False):
                                st.write(f"é¡ä¼¼åº¦: {result['similarity']:.4f} (æ­£è¦åŒ–å¾Œ)")
                                st.write(f"å…ƒã®é¡ä¼¼åº¦: {result['original_similarity']:.4f}")
                            else:
                                st.write(f"é¡ä¼¼åº¦: {result['similarity']:.4f}")
                            st.write(f"èª¬æ˜: {result['description'][:200]}...")
                            st.markdown("---")
                else:
                    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        st.markdown('</div>', unsafe_allow_html=True)


def show_advanced_visualization(search_system):
    """ç‰¹å¾´é‡ç©ºé–“è§£æ"""
    st.header("ğŸ§  ç‰¹å¾´é‡ç©ºé–“è§£æ")
    
    if not search_system.dimensionality_reducer:
        st.error("âŒ æ¬¡å…ƒå‰Šæ¸›ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        st.info("ğŸ“ ãƒ’ãƒ³ãƒˆ: ã¾ãšã€Œãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ & è¿½åŠ ã€ã‚¿ãƒ–ã§ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„")
        return
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®š
    st.sidebar.header("âš™ï¸ è§£æè¨­å®š")
    
    # æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•é¸æŠ
    available_methods = search_system.dimensionality_reducer.get_available_methods()
    selected_methods = st.sidebar.multiselect(
        "ğŸ”¬ æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•:",
        available_methods,
        default=['PCA', 'UMAP', 't-SNE']  # è»½é‡ãªæ‰‹æ³•ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
    )
    
    if not selected_methods:
        st.sidebar.warning("âš ï¸ å°‘ãªãã¨ã‚‚1ã¤ã®æ‰‹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•é¸æŠ
    available_clustering = search_system.dimensionality_reducer.get_available_clustering()
    clustering_method = st.sidebar.selectbox(
        "ğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•:",
        available_clustering,
        index=0
    )
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    st.sidebar.subheader("ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°è¨­å®š
    if clustering_method in ['KMeans', 'Hierarchical', 'GMM']:
        n_clusters = st.sidebar.slider("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°:", 2, 15, 5)
    else:
        n_clusters = 5
    
    # DBSCANå›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    if clustering_method == 'DBSCAN':
        eps = st.sidebar.slider("Îµ (eps):", 0.1, 2.0, 0.5, 0.1)
        min_samples = st.sidebar.slider("æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°:", 2, 20, 5)
        clustering_kwargs = {'eps': eps, 'min_samples': min_samples}
    else:
        clustering_kwargs = {}
    
    # å¯è¦–åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    st.sidebar.subheader("ğŸ¨ å¯è¦–åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    show_brand_names = st.sidebar.checkbox("ãƒ–ãƒ©ãƒ³ãƒ‰åè¡¨ç¤º", value=False)
    
    # è§£æã‚¿ã‚¤ãƒ—é¸æŠ
    analysis_type = st.selectbox(
        "è§£æã‚¿ã‚¤ãƒ—:",
        ["ğŸ¨ åŸºæœ¬å¯è¦–åŒ–", "ğŸ¯ ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«", "ğŸ“Š æ¯”è¼ƒå¯è¦–åŒ–", "ğŸ¯ å˜ä¸€æ‰‹æ³•è©³ç´°è§£æ", "ğŸ“ˆ å…¨æ‰‹æ³•ä¸€æ‹¬è§£æ"]
    )
    
    if analysis_type == "ğŸ¨ åŸºæœ¬å¯è¦–åŒ–":
        show_basic_visualization(search_system)
    elif analysis_type == "ğŸ¯ ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«":
        show_genre_center_visualization(search_system)
    elif analysis_type == "ğŸ“Š æ¯”è¼ƒå¯è¦–åŒ–":
        show_comparison_analysis(search_system, selected_methods, clustering_method, 
                               n_clusters, clustering_kwargs, show_brand_names)
    elif analysis_type == "ğŸ¯ å˜ä¸€æ‰‹æ³•è©³ç´°è§£æ":
        show_single_method_analysis(search_system, selected_methods, clustering_method,
                                  n_clusters, clustering_kwargs)
    elif analysis_type == "ğŸ“ˆ å…¨æ‰‹æ³•ä¸€æ‹¬è§£æ":
        show_comprehensive_analysis(search_system, clustering_method, n_clusters, clustering_kwargs)

def show_basic_visualization(search_system):
    """åŸºæœ¬çš„ãªç‰¹å¾´é‡ç©ºé–“å¯è¦–åŒ–"""
    st.subheader("ğŸ¨ åŸºæœ¬ç‰¹å¾´é‡ç©ºé–“å¯è¦–åŒ–")
    
    # ãƒ‡ãƒ¼ã‚¿æƒ…å ±è¡¨ç¤º
    total_brands = len(search_system.brand_names)
    st.info(f"ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ å†…å…¨ **{total_brands}** ãƒ–ãƒ©ãƒ³ãƒ‰ã®ç‰¹å¾´é‡ç©ºé–“ã‚’å¯è¦–åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºï¼‰")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•é¸æŠ
        basic_method = st.selectbox(
            "ğŸ”¬ æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•:",
            ['UMAP', 'PCA', 't-SNE'],
            index=0
        )
    
    with col2:
        # ãƒã‚¤ãƒ©ã‚¤ãƒˆè¨­å®š
        if search_system.brand_names:
            highlight_brands = st.multiselect(
                "ğŸ¯ ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰:",
                search_system.brand_names[:20],  # æœ€åˆã®20ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã¿è¡¨ç¤º
                default=[]
            )
        else:
            highlight_brands = []
    
    if st.button("ğŸ¨ å¯è¦–åŒ–ç”Ÿæˆ", type="primary"):
        with st.spinner(f"{basic_method}æ¬¡å…ƒå‰Šæ¸›å®Ÿè¡Œä¸­..."):
            if basic_method == 'UMAP':
                fig = search_system.create_embedding_visualization(highlight_brands)
            else:
                # ä»–ã®æ‰‹æ³•ã‚‚å¯¾å¿œ
                search_system.dimensionality_reducer.apply_selected_methods([basic_method])
                fig = search_system.dimensionality_reducer.create_comparison_visualization_for_streamlit(show_brand_names=False)
            
        if fig:
            st.plotly_chart(fig, use_container_width=True)
            
            # çµ±è¨ˆæƒ…å ±
            st.subheader("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ç·ãƒ–ãƒ©ãƒ³ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºï¼‰", len(search_system.brand_names))
            with col2:
                st.metric("æ–°è¦è¿½åŠ æ•°", len(search_system.added_brands))
            with col3:
                if search_system.embeddings is not None:
                    st.metric("åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ", search_system.embeddings.shape[1])
            with col4:
                # reranker_status = "æœ‰åŠ¹" if search_system.reranker else "ç„¡åŠ¹"
                # st.metric("ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°", reranker_status)
                pass
            
            # æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è¦§
            if search_system.added_brands:
                st.subheader("ğŸ†• æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰")
                for added in search_system.added_brands:
                    st.markdown(f"- **{added['name']}**")
                    st.write(f"  {added['description'][:100]}...")

def show_genre_center_visualization(search_system):
    """ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«å¯è¦–åŒ–"""
    st.subheader("ğŸ¯ ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«å¯è¦–åŒ–")
    
    # ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    if not search_system.genre_brands:
        st.warning("âš ï¸ ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚maps.csvãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ã‚¸ãƒ£ãƒ³ãƒ«çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    st.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ã‚¸ãƒ£ãƒ³ãƒ«: {len(search_system.genre_brands)} ç¨®é¡")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ã‚¸ãƒ£ãƒ³ãƒ«é¸æŠ
        available_genres = sorted(search_system.genre_brands.keys())
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚ˆãä½¿ã‚ã‚Œãã†ãªã‚¸ãƒ£ãƒ³ãƒ«ã‚’é¸æŠ
        default_genres = []
        common_genres = ['womens', 'mens', 'accessary', 'goods', 'shoes', 'beauty', 'sports_and_outdoor']
        for genre in common_genres:
            if genre in available_genres:
                default_genres.append(genre)
        
        selected_genres = st.multiselect(
            "è¡¨ç¤ºã™ã‚‹ã‚¸ãƒ£ãƒ³ãƒ«ï¼ˆç©º=å…¨ã¦ï¼‰:",
            available_genres,
            default=default_genres[:5] if default_genres else [],  # æœ€åˆã®5ã¤ã ã‘ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ
            help="é¸æŠã—ãŸã‚¸ãƒ£ãƒ³ãƒ«ã®ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’ç‰¹å¾´é‡ç©ºé–“ä¸Šã«ãƒ—ãƒ­ãƒƒãƒˆ"
        )
        
        # å…¨ã‚¸ãƒ£ãƒ³ãƒ«è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
        show_all_genres = st.checkbox("å…¨ã‚¸ãƒ£ãƒ³ãƒ«ã‚’è¡¨ç¤º", value=False)
        if show_all_genres:
            selected_genres = available_genres
    
    with col2:
        # ã‚¸ãƒ£ãƒ³ãƒ«çµ±è¨ˆ
        st.write("**ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ãƒ–ãƒ©ãƒ³ãƒ‰æ•°ï¼ˆä¸Šä½10ï¼‰:**")
        genre_counts = {genre: len(brands) for genre, brands in search_system.genre_brands.items()}
        top_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        for genre, count in top_genres:
            st.write(f"- {genre}: {count}ãƒ–ãƒ©ãƒ³ãƒ‰")
    
    if st.button("ğŸ¯ ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«å¯è¦–åŒ–å®Ÿè¡Œ", type="primary"):
        if not selected_genres and not show_all_genres:
            st.warning("è¡¨ç¤ºã™ã‚‹ã‚¸ãƒ£ãƒ³ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
            return
        
        with st.spinner("ã‚¸ãƒ£ãƒ³ãƒ«ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ãƒ»å¯è¦–åŒ–ä¸­..."):
            # é¸æŠã•ã‚ŒãŸã‚¸ãƒ£ãƒ³ãƒ«ã¾ãŸã¯å…¨ã‚¸ãƒ£ãƒ³ãƒ«
            target_genres = selected_genres if selected_genres else available_genres
            
            # å¯è¦–åŒ–å®Ÿè¡Œ
            fig = search_system.create_genre_center_visualization(target_genres)
        
        if fig:
            st.plotly_chart(fig, use_container_width=True)
            
            # ã‚¸ãƒ£ãƒ³ãƒ«è©³ç´°æƒ…å ±
            st.subheader("ğŸ“Š è¡¨ç¤ºã‚¸ãƒ£ãƒ³ãƒ«è©³ç´°")
            for genre in target_genres:
                if genre in search_system.genre_brands:
                    brand_count = len(search_system.genre_brands[genre])
                    with st.expander(f"ğŸ·ï¸ {genre} ({brand_count}ãƒ–ãƒ©ãƒ³ãƒ‰)"):
                        brands = search_system.genre_brands[genre][:10]  # æœ€åˆã®10ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã¿è¡¨ç¤º
                        st.write(", ".join(brands))
                        if len(search_system.genre_brands[genre]) > 10:
                            st.write(f"...ä»– {len(search_system.genre_brands[genre]) - 10} ãƒ–ãƒ©ãƒ³ãƒ‰")
        else:
            st.error("å¯è¦–åŒ–ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

def show_comparison_analysis(search_system, selected_methods, clustering_method, 
                           n_clusters, clustering_kwargs, show_brand_names):
    """æ¯”è¼ƒå¯è¦–åŒ–è§£æ"""
    if not selected_methods:
        st.warning("æœ€ä½1ã¤ã®æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„")
        return
    
    # ãƒ‡ãƒ¼ã‚¿ç¢ºèª - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å…¨244ãƒ–ãƒ©ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç¾åœ¨ã¯ç´„1769ãƒ–ãƒ©ãƒ³ãƒ‰ï¼‰ã‚’è¡¨ç¤º
    total_brands = len(search_system.brand_names)
    st.info(f"ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ å†…ãƒ–ãƒ©ãƒ³ãƒ‰ç·æ•°: **{total_brands}** ãƒ–ãƒ©ãƒ³ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºå¯¾è±¡ï¼‰")
    st.write(f"ğŸ”¬ é¸æŠæ‰‹æ³•: {', '.join(selected_methods)}")
    st.write(f"ğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: {clustering_method}")
    
    if st.button("ğŸš€ æ¯”è¼ƒè§£æå®Ÿè¡Œ", type="primary"):
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # æ¬¡å…ƒå‰Šæ¸›å®Ÿè¡Œ
            status_text.text("ğŸ”„ æ¬¡å…ƒå‰Šæ¸›ã‚’å®Ÿè¡Œä¸­...")
            progress_bar.progress(30)
            
            results = search_system.dimensionality_reducer.apply_selected_methods(
                methods=selected_methods
            )
            
            if not results:
                st.error("âŒ æ¬¡å…ƒå‰Šæ¸›ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            progress_bar.progress(60)
            status_text.text("ğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            clusters = search_system.dimensionality_reducer.apply_clustering(
                method=clustering_method, 
                n_clusters=n_clusters, 
                **clustering_kwargs
            )
            
            if clusters is None:
                st.error("âŒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            progress_bar.progress(90)
            status_text.text("ğŸ¨ å¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
        
        except Exception as e:
            st.error(f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return
        
        # å¯è¦–åŒ–ä½œæˆãƒ»è¡¨ç¤º
        try:
            # å¯è¦–åŒ–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç›´æ¥å–å¾—
            fig = search_system.dimensionality_reducer.create_comparison_visualization_for_streamlit(
                clusters=clusters,
                show_brand_names=show_brand_names
            )
            
            progress_bar.progress(100)
            status_text.text("âœ… å®Œäº†!")
            
            # çµæœè¡¨ç¤º
            st.success(f"âœ… {len(selected_methods)} æ‰‹æ³•ã®æ¯”è¼ƒå¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆå…¨{total_brands}ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºï¼‰")
            
            # å¯è¦–åŒ–ã‚’ç›´æ¥è¡¨ç¤º
            if fig:
                st.subheader("ğŸ“Š æ¯”è¼ƒå¯è¦–åŒ–çµæœ")
                st.plotly_chart(fig, use_container_width=True)
                
                # æ‰‹æ³•èª¬æ˜ã‚’è¿½åŠ 
                with st.expander("ğŸ”¬ æ‰‹æ³•ã®ç‰¹å¾´"):
                    st.write("**PCA**: ç·šå½¢æ¬¡å…ƒå‰Šæ¸›ã€å…¨ä½“æ§‹é€ ä¿æŒ")
                    st.write("**t-SNE**: éç·šå½¢ã€å±€æ‰€æ§‹é€ å¼·èª¿ã€ã‚¯ãƒ©ã‚¹ã‚¿åˆ†é›¢ã«å„ªç§€")
                    st.write("**UMAP**: å±€æ‰€ãƒ»å¤§åŸŸãƒãƒ©ãƒ³ã‚¹ã€é«˜é€Ÿå‡¦ç†")
                    st.write("**MDS**: è·é›¢é–¢ä¿‚ä¿æŒã€å¤§åŸŸæ§‹é€ é‡è¦–")
                    st.write("**Anchor-UMAP**: ã‚¢ãƒ³ã‚«ãƒ¼ç‚¹ã«ã‚ˆã‚‹å®‰å®šåŒ–")
            else:
                st.error("âŒ å¯è¦–åŒ–ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
            show_cluster_analysis(clusters, search_system.brand_names, clustering_method)
            
        except Exception as e:
            st.error(f"âŒ å¯è¦–åŒ–ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            progress_bar.empty()
            status_text.empty()

def show_single_method_analysis(search_system, selected_methods, clustering_method,
                               n_clusters, clustering_kwargs):
    """å˜ä¸€æ‰‹æ³•è©³ç´°è§£æ"""
    if not selected_methods:
        st.warning("è©³ç´°è§£æã™ã‚‹æ‰‹æ³•ã‚’1ã¤é¸æŠã—ã¦ãã ã•ã„")
        return
    
    target_method = st.selectbox("è©³ç´°è§£æã™ã‚‹æ‰‹æ³•:", selected_methods)
    
    if st.button("ğŸ¯ è©³ç´°è§£æå®Ÿè¡Œ", type="primary"):
        with st.spinner(f"{target_method} + {clustering_method} è©³ç´°è§£æä¸­..."):
            # å¯è¦–åŒ–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç›´æ¥å–å¾—
            fig = search_system.dimensionality_reducer.create_single_method_visualization_for_streamlit(
                method=target_method,
                clustering_method=clustering_method,
                n_clusters=n_clusters,
                **clustering_kwargs
            )
        
        st.success(f"âœ… {target_method} ã®è©³ç´°è§£æãŒå®Œäº†ã—ã¾ã—ãŸ")
        
        # å¯è¦–åŒ–ã‚’ç›´æ¥è¡¨ç¤º
        if fig:
            st.subheader(f"ğŸ“Š {target_method} + {clustering_method} è©³ç´°çµæœ")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.error("å¯è¦–åŒ–ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

def show_comprehensive_analysis(search_system, clustering_method, n_clusters, clustering_kwargs):
    """å…¨æ‰‹æ³•ä¸€æ‹¬è§£æ"""
    # ãƒ‡ãƒ¼ã‚¿æƒ…å ±è¡¨ç¤º
    total_brands = len(search_system.brand_names)
    st.info(f"ğŸ“Š å…¨ **{total_brands}** ãƒ–ãƒ©ãƒ³ãƒ‰ã«å¯¾ã—ã¦å…¨æ‰‹æ³•ä¸€æ‹¬è§£æã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºå¯¾è±¡ï¼‰")
    
    if st.button("ğŸ“ˆ å…¨æ‰‹æ³•ä¸€æ‹¬è§£æå®Ÿè¡Œ", type="primary"):
        with st.spinner("å…¨æ‰‹æ³•è§£æå®Ÿè¡Œä¸­..."):
            # å…¨æ‰‹æ³•é©ç”¨
            results = search_system.dimensionality_reducer.apply_all_methods()
            
            if not results:
                st.error("æ¬¡å…ƒå‰Šæ¸›ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            clusters = search_system.dimensionality_reducer.apply_clustering(
                method=clustering_method,
                n_clusters=n_clusters,
                **clustering_kwargs
            )
            
            if clusters is None:
                st.error("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            # å¯è¦–åŒ–ä½œæˆãƒ»è¡¨ç¤º
            fig = search_system.dimensionality_reducer.create_comparison_visualization_for_streamlit(
                clusters=clusters
            )
        
        st.success(f"âœ… å…¨æ‰‹æ³•ä¸€æ‹¬è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆå…¨{total_brands}ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºï¼‰")
        
        # è©³ç´°çµ±è¨ˆ
        st.subheader("ğŸ“Š è§£æçµ±è¨ˆ")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("å®Ÿè¡Œæ‰‹æ³•æ•°", len(results))
        with col2:
            st.metric("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°", len(set(clusters)) if clusters is not None else 0)
        with col3:
            st.metric("ç·ãƒ–ãƒ©ãƒ³ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºï¼‰", len(search_system.brand_names))
        
        # å¯è¦–åŒ–ã‚’ç›´æ¥è¡¨ç¤º
        if fig:
            st.subheader("ğŸ¨ å…¨æ‰‹æ³•æ¯”è¼ƒå¯è¦–åŒ–")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.error("å¯è¦–åŒ–ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

def show_cluster_analysis(clusters, brand_names, clustering_method):
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœè¡¨ç¤º"""
    if clusters is None:
        return
    
    st.subheader(f"ğŸ¯ {clustering_method} ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆ
    unique_clusters = sorted(set(clusters))
    cluster_counts = {cluster: sum(1 for c in clusters if c == cluster) 
                     for cluster in unique_clusters}
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆè¡¨ç¤º
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆ:**")
        for cluster, count in cluster_counts.items():
            st.write(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ {cluster}: {count} ãƒ–ãƒ©ãƒ³ãƒ‰")
    
    with col2:
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒã‚°ãƒ©ãƒ•
        cluster_df = pd.DataFrame({
            'ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼': list(cluster_counts.keys()),
            'ãƒ–ãƒ©ãƒ³ãƒ‰æ•°': list(cluster_counts.values())
        })
        fig = px.bar(cluster_df, x='ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼', y='ãƒ–ãƒ©ãƒ³ãƒ‰æ•°', 
                    title="ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒ")
        st.plotly_chart(fig, use_container_width=True)
    
    # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è©³ç´°
    with st.expander("ğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è©³ç´°"):
        for cluster in unique_clusters:
            # å®‰å…¨ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆç¯„å›²ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            cluster_brands = [brand_names[i] for i, c in enumerate(clusters) 
                            if c == cluster and i < len(brand_names)]
            st.write(f"**ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ {cluster}** ({len(cluster_brands)} ãƒ–ãƒ©ãƒ³ãƒ‰):")
            st.write(", ".join(cluster_brands))
            st.write("---")

def show_csv_batch_interface(search_system):
    """CSVä¸€æ‹¬è¿½åŠ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ“‚ CSVä¸€æ‹¬ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ")
    st.markdown("**å‡¦ç†ã‚µã‚¤ã‚¯ãƒ«**: ãƒ–ãƒ©ãƒ³ãƒ‰åæ¤œç´¢ â†’ é‡è¤‡ãƒã‚§ãƒƒã‚¯ â†’ èª¬æ˜æ–‡ç”Ÿæˆ â†’ Ruriv3ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ è¾æ›¸è¿½åŠ ")
    
    # å‡¦ç†ãƒ•ãƒ­ãƒ¼ã®å¯è¦–åŒ–
    st.markdown("""
    ```
    ğŸ“ CSVãƒ–ãƒ©ãƒ³ãƒ‰å â†’ ğŸ” æ—¢å­˜è¾æ›¸æ¤œç´¢ â†’ â“ é‡è¤‡ãƒã‚§ãƒƒã‚¯
    â†“ (é‡è¤‡ãªã—)
    ğŸ¤– GPT-OSSèª¬æ˜æ–‡ç”Ÿæˆ â†’ ğŸ§  Ruriv3ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ ğŸ“š ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸è¿½åŠ 
    ```
    """)
    
    # CSVå½¢å¼èª¬æ˜
    with st.expander("ğŸ“‹ CSVå½¢å¼ã«ã¤ã„ã¦"):
        st.markdown("""
        **å¿…è¦ãªåˆ—:**
        - `ãƒ–ãƒ©ãƒ³ãƒ‰å`: è¿½åŠ ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ã®åå‰
        - `èª¬æ˜æ–‡`: ãƒ–ãƒ©ãƒ³ãƒ‰ã®èª¬æ˜æ–‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        **ä¾‹:**
        ```csv
        ãƒ–ãƒ©ãƒ³ãƒ‰å,èª¬æ˜æ–‡
        Stone Island,ã‚¤ã‚¿ãƒªã‚¢ã®é«˜ç´šã‚¹ãƒˆãƒªãƒ¼ãƒˆã‚¦ã‚§ã‚¢ãƒ–ãƒ©ãƒ³ãƒ‰...
        Fear of God,ã‚¢ãƒ¡ãƒªã‚«ç™ºã®é«˜ç´šãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰...
        ```
        
        **é‡è¤‡ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã‚¹ã‚­ãƒƒãƒ—æ©Ÿèƒ½:**
        - æ—¢ã«ç™»éŒ²æ¸ˆã¿ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã¯è‡ªå‹•çš„ã«ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™
        - å³å¯†ãƒã‚§ãƒƒã‚¯: å®Œå…¨ä¸€è‡´ã®ã¿æ¤œå‡º
        - éå³å¯†ãƒã‚§ãƒƒã‚¯: é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰åã‚‚æ¤œå‡ºï¼ˆä¾‹: "Nike" ã¨ "NIKE"ï¼‰
        
        **æ³¨æ„:**
        - èª¬æ˜æ–‡ã®å‡¦ç†ã¯é¸æŠã—ãŸãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦æ±ºã¾ã‚Šã¾ã™
        - ã€Œã™ã¹ã¦æ–°è¦ç”Ÿæˆã€ãƒ¢ãƒ¼ãƒ‰ã§ã¯æ—¢å­˜èª¬æ˜æ–‡ã‚’ç„¡è¦–ã—ã¦å…¨ã¦æ–°è¦ç”Ÿæˆã—ã¾ã™
        - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¯10MBä»¥ä¸‹ã«ã—ã¦ãã ã•ã„
        """)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
        type=['csv'],
        help="ãƒ–ãƒ©ãƒ³ãƒ‰å,èª¬æ˜æ–‡ã®å½¢å¼ã®CSVãƒ•ã‚¡ã‚¤ãƒ«"
    )
    
    # è¨­å®š
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("âš™ï¸ å‡¦ç†è¨­å®š")
        
        api_key = st.text_input(
            "ğŸ”‘ GPT-OSSè¨­å®š (ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ):",
            type="password",
            help="ãƒ­ãƒ¼ã‚«ãƒ«GPT-OSSã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨",
            placeholder="ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã®ãŸã‚å…¥åŠ›ä¸è¦"
        )
        
        description_mode = st.radio(
            "èª¬æ˜æ–‡ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰:",
            options=["æ—¢å­˜ã®èª¬æ˜æ–‡ã‚’å„ªå…ˆ", "ç©ºç™½ã®ã¿ç”Ÿæˆ", "ã™ã¹ã¦æ–°è¦ç”Ÿæˆ"],
            index=0,
            help="æ—¢å­˜å„ªå…ˆ: CSVã«èª¬æ˜æ–‡ãŒã‚ã‚Œã°ä½¿ç”¨ã€ç©ºç™½ãªã‚‰ç”Ÿæˆ / ç©ºç™½ã®ã¿: ç©ºç™½ã®å ´åˆã®ã¿ç”Ÿæˆ / ã™ã¹ã¦æ–°è¦: æ—¢å­˜èª¬æ˜æ–‡ã‚’ç„¡è¦–ã—ã¦å…¨ã¦æ–°è¦ç”Ÿæˆ"
        )
        
        strict_csv_check = st.checkbox(
            "ğŸ” å³å¯†ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆCSVï¼‰",
            value=True,
            help="æœ‰åŠ¹: å®Œå…¨ä¸€è‡´ã®ã¿ãƒã‚§ãƒƒã‚¯ / ç„¡åŠ¹: é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰åã‚‚ãƒã‚§ãƒƒã‚¯"
        )
        
    with col2:
        st.subheader("ğŸ¯ å‡¦ç†å¾Œã®è‡ªå‹•æ¤œç´¢")
        
        auto_search = st.checkbox(
            "è¿½åŠ å¾Œã«é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œ", 
            value=False,
            help="å„ãƒ–ãƒ©ãƒ³ãƒ‰ã®é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’è‡ªå‹•æ¤œç´¢"
        )
        
        if auto_search:
            search_count = st.slider("æ¤œç´¢çµæœæ•°:", 1, 10, 3)
    
    # CSVãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if uploaded_file is not None:
        try:
            # CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = pd.read_csv(uploaded_file)
            
            # åˆ—åã®æ­£è¦åŒ–
            if 'brand_name' not in df.columns:
                if 'ãƒ–ãƒ©ãƒ³ãƒ‰å' in df.columns:
                    df = df.rename(columns={'ãƒ–ãƒ©ãƒ³ãƒ‰å': 'brand_name'})
                elif 'name' in df.columns:
                    df = df.rename(columns={'name': 'brand_name'})
                else:
                    st.error("âŒ 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'brand_name', ã¾ãŸã¯ 'name' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return
            
            if 'description' not in df.columns:
                if 'èª¬æ˜æ–‡' in df.columns:
                    df = df.rename(columns={'èª¬æ˜æ–‡': 'description'})
                else:
                    df['description'] = ''  # ç©ºã®èª¬æ˜æ–‡åˆ—ã‚’è¿½åŠ 
            
            st.subheader("ğŸ“Š CSVãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df.head(10), use_container_width=True)
            
            # çµ±è¨ˆæƒ…å ±
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç·ãƒ–ãƒ©ãƒ³ãƒ‰æ•°", len(df))
            with col2:
                has_desc = sum(1 for desc in df['description'] if desc and str(desc).strip())
                st.metric("èª¬æ˜æ–‡ã‚ã‚Š", has_desc)
            with col3:
                need_generation = len(df) - has_desc
                st.metric("ç”ŸæˆãŒå¿…è¦", need_generation)
            
            # å‡¦ç†å®Ÿè¡Œ
            if st.button("ğŸš€ ä¸€æ‹¬å‡¦ç†å®Ÿè¡Œ", type="primary"):
                
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                csv_data = df.to_dict('records')
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                progress_bar = st.progress(0)
                status_text = st.empty()
                results_container = st.empty()
                
                def update_progress(current, total, message):
                    progress = current / total if total > 0 else 0
                    progress_bar.progress(progress)
                    status_text.text(f"{message} ({current}/{total})")
                
                # å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
                if description_mode == "æ—¢å­˜ã®èª¬æ˜æ–‡ã‚’å„ªå…ˆ":
                    use_existing = True
                    force_regenerate = False
                elif description_mode == "ç©ºç™½ã®ã¿ç”Ÿæˆ":
                    use_existing = False
                    force_regenerate = False
                else:  # "ã™ã¹ã¦æ–°è¦ç”Ÿæˆ"
                    use_existing = False
                    force_regenerate = True
                
                # ä¸€æ‹¬å‡¦ç†å®Ÿè¡Œ
                with st.spinner("CSVä¸€æ‹¬å‡¦ç†ä¸­..."):
                    results = search_system.process_csv_batch(
                        csv_data, 
                        api_key=api_key,
                        progress_callback=update_progress,
                        use_existing_descriptions=use_existing,
                        force_regenerate=force_regenerate,
                        strict_check=strict_csv_check
                    )
                
                # çµæœè¡¨ç¤º
                progress_bar.progress(100)
                status_text.text("âœ… å‡¦ç†å®Œäº†!")
                
                # çµæœçµ±è¨ˆ
                success_count = sum(1 for r in results if r['status'] == 'success')
                skipped_count = sum(1 for r in results if r['status'] == 'skipped')
                failed_count = sum(1 for r in results if r['status'] not in ['success', 'skipped'])
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("å‡¦ç†æ¸ˆã¿", len(results))
                with col2:
                    st.metric("æˆåŠŸ", success_count, delta=success_count)
                with col3:
                    st.metric("ã‚¹ã‚­ãƒƒãƒ—", skipped_count, delta=skipped_count if skipped_count > 0 else None)
                with col4:
                    st.metric("å¤±æ•—", failed_count, delta=-failed_count if failed_count > 0 else 0)
                
                # çµæœè©³ç´°è¡¨ç¤º
                st.subheader("ğŸ“Š å‡¦ç†çµæœ")
                
                # æˆåŠŸãƒ»å¤±æ•—ãƒ»ã‚¹ã‚­ãƒƒãƒ—ã®åˆ†é¡è¡¨ç¤º
                if success_count > 0:
                    st.success(f"âœ… {success_count} ãƒ–ãƒ©ãƒ³ãƒ‰ã®å‡¦ç†ã‚µã‚¤ã‚¯ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    st.markdown("**å®Œäº†ã—ãŸã‚µã‚¤ã‚¯ãƒ«**: ãƒ–ãƒ©ãƒ³ãƒ‰åæ¤œç´¢ â†’ é‡è¤‡ãƒã‚§ãƒƒã‚¯ â†’ èª¬æ˜æ–‡ç”Ÿæˆ â†’ Ruriv3ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ è¾æ›¸è¿½åŠ ")
                    
                    success_results = [r for r in results if r['status'] == 'success']
                    
                    # æˆåŠŸçµæœã®è©³ç´°è¡¨ç¤ºç”¨
                    display_success = []
                    for r in success_results:
                        display_success.append({
                            'ãƒ–ãƒ©ãƒ³ãƒ‰å': r['brand_name'],
                            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'âœ… å®Œäº†',
                            'èª¬æ˜æ–‡ç”Ÿæˆ': 'ğŸ¤– AIç”Ÿæˆ' if r.get('generated', False) else 'ğŸ“ æ—¢å­˜ä½¿ç”¨',
                            'ãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶': f"{r.get('embedding_shape', 'N/A')}",
                            'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹': r.get('vector_index', 'N/A'),
                            'èª¬æ˜æ–‡': r['description']
                        })
                    
                    success_df = pd.DataFrame(display_success)
                    st.dataframe(success_df, use_container_width=True, hide_index=True)
                
                if skipped_count > 0:
                    st.info(f"â­ï¸ {skipped_count} ãƒ–ãƒ©ãƒ³ãƒ‰ã¯æ—¢ã«ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
                    
                    skipped_results = [r for r in results if r['status'] == 'skipped']
                    
                    # ã‚¹ã‚­ãƒƒãƒ—çµæœã®è©³ç´°è¡¨ç¤ºç”¨
                    display_skip = []
                    for r in skipped_results:
                        display_skip.append({
                            'ãƒ–ãƒ©ãƒ³ãƒ‰å': r['brand_name'],
                            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'â­ï¸ ã‚¹ã‚­ãƒƒãƒ—',
                            'ã‚¹ãƒ†ãƒƒãƒ—': r.get('step', 'N/A'),
                            'ç†ç”±': r.get('error', 'N/A')
                        })
                    
                    skipped_df = pd.DataFrame(display_skip)
                    st.dataframe(skipped_df, use_container_width=True, hide_index=True)
                
                if failed_count > 0:
                    st.error(f"âŒ {failed_count} ãƒ–ãƒ©ãƒ³ãƒ‰ã®å‡¦ç†ã‚µã‚¤ã‚¯ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    
                    failed_results = [r for r in results if r['status'] not in ['success', 'skipped']]
                    
                    # å¤±æ•—çµæœã®è©³ç´°è¡¨ç¤ºç”¨
                    display_failed = []
                    for r in failed_results:
                        display_failed.append({
                            'ãƒ–ãƒ©ãƒ³ãƒ‰å': r['brand_name'],
                            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'âŒ å¤±æ•—',
                            'å¤±æ•—ã‚¹ãƒ†ãƒƒãƒ—': r.get('step', 'unknown'),
                            'ã‚¨ãƒ©ãƒ¼è©³ç´°': r.get('error', 'N/A'),
                            'èª¬æ˜æ–‡ç”Ÿæˆ': 'ğŸ¤– AIç”Ÿæˆ' if r.get('generated', False) else 'ğŸ“ æ—¢å­˜ä½¿ç”¨'
                        })
                    
                    failed_df = pd.DataFrame(display_failed)
                    st.dataframe(failed_df, use_container_width=True, hide_index=True)
                
                # è‡ªå‹•æ¤œç´¢å®Ÿè¡Œ
                if auto_search and success_count > 0:
                    st.subheader("ğŸ” è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ã®é¡ä¼¼åº¦æ¤œç´¢")
                    
                    search_results = {}
                    for result in results:
                        if result['status'] == 'success':
                            brand_name = result['brand_name']
                            similar = search_system.search_similar_brands(
                                brand_name, top_k=search_count, min_similarity=0.1
                            )
                            if similar:
                                search_results[brand_name] = similar
                    
                    # æ¤œç´¢çµæœè¡¨ç¤º
                    for brand_name, similar_brands in search_results.items():
                        with st.expander(f"ğŸ¯ {brand_name} ã®é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰"):
                            for i, similar in enumerate(similar_brands, 1):
                                st.write(f"{i}. {similar['brand_name']} (é¡ä¼¼åº¦: {similar['similarity']:.3f})")
                
                # æ¸…æƒ
                progress_bar.empty()
                status_text.empty()
                
        except Exception as e:
            st.error(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãƒ˜ãƒƒãƒ€ãƒ¼ã«'ãƒ–ãƒ©ãƒ³ãƒ‰å'åˆ—ãŒå¿…è¦ã§ã™ã€‚")

if __name__ == "__main__":
    main()