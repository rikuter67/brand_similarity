import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from brand_similarity import LLMStyleEmbeddingAnalyzer
from brand_store_matching import BrandStoreAnalyzer
from location_bias_reranking import create_reranker_from_streamlit_data
import os
from sklearn.metrics.pairwise import cosine_similarity
import requests
import json

# Streamlit page configuration
st.set_page_config(
    page_title="Brand Analysis Dashboard",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .success-box {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #c3e6cb;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_brand_data():
    """ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    try:
        if os.path.exists('integrated_brands.csv'):
            csv_file = 'integrated_brands.csv'
        else:
            csv_file = 'description.csv'
        
        analyzer = LLMStyleEmbeddingAnalyzer()
        analyzer.load_data(csv_file)
        
        # åŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        embedding_files = [
            "./ruri_embeddings_results/ruri_description_embeddings_v3_raw_hub.npy",
            "./ruri_embeddings_results/ruri_description_embeddings.npy",
            "./ruri_embeddings_results/updated_embeddings.npy"
        ]
        
        for emb_path in embedding_files:
            if os.path.exists(emb_path):
                try:
                    raw_embeddings = np.load(emb_path)
                    from sklearn.preprocessing import normalize
                    analyzer.embeddings = normalize(raw_embeddings, norm='l2', axis=1)
                    st.success(f"âœ… åŸ‹ã‚è¾¼ã¿èª­ã¿è¾¼ã¿: {os.path.basename(emb_path)}")
                    break
                except Exception as e:
                    st.warning(f"âš ï¸ åŸ‹ã‚è¾¼ã¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        if analyzer.embeddings is None:
            analyzer.generate_advanced_embeddings()
        
        analyzer.calculate_similarity_matrix()
        return analyzer
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_data
def load_store_analyzer():
    """åº—èˆ—åˆ†æå™¨ã®èª­ã¿è¾¼ã¿"""
    try:
        return BrandStoreAnalyzer("datasets/bline_similarity/maps.csv")
    except Exception as e:
        st.error(f"åº—èˆ—ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_brand_description_gemini(api_key, brand_name):
    """Gemini APIã‚’ä½¿ã£ãŸãƒ–ãƒ©ãƒ³ãƒ‰èª¬æ˜æ–‡ç”Ÿæˆ"""
    try:
        url = f"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key={api_key}"
        
        prompt = f"""
        Please write a comprehensive brand description for "{brand_name}" in Japanese.
        Include information about:
        - Brand history and origin
        - Design philosophy and aesthetic
        - Target demographic
        - Key products or specialties
        - Price range and positioning
        
        Keep it between 200-400 characters.
        """
        
        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 1024
            }
        }
        
        response = requests.post(url, json=payload, headers={"Content-Type": "application/json"}, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            if 'candidates' in result and result['candidates']:
                candidate = result['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    return candidate['content']['parts'][0]['text'].strip()
        else:
            st.error(f"API ã‚¨ãƒ©ãƒ¼: {response.status_code}")
        
        return None
        
    except Exception as e:
        st.error(f"èª¬æ˜æ–‡ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_brand_description_gpt_oss(brand_name):
    """GPT-OSSã‚’ä½¿ã£ãŸãƒ–ãƒ©ãƒ³ãƒ‰èª¬æ˜æ–‡ç”Ÿæˆ"""
    try:
        from gpt_oss_direct import GPTOSSDirect
        
        gpt = GPTOSSDirect()
        if not gpt.is_available():
            st.error("GPT-OSSãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None
        
        description = gpt.generate_brand_description(brand_name)
        if description and len(description.strip()) > 10:
            return description.strip()
        else:
            st.warning("GPT-OSSã‹ã‚‰æœ‰åŠ¹ãªèª¬æ˜æ–‡ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return None
        
    except Exception as e:
        st.error(f"GPT-OSSèª¬æ˜æ–‡ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_brand_description(api_key, brand_name, use_gpt_oss=False):
    """ãƒ–ãƒ©ãƒ³ãƒ‰èª¬æ˜æ–‡ç”Ÿæˆ - GPT-OSSã¾ãŸã¯Gemini"""
    if use_gpt_oss:
        return generate_brand_description_gpt_oss(brand_name)
    else:
        return generate_brand_description_gemini(api_key, brand_name)

def main():
    # Header
    st.markdown('<h1 class="main-header">ğŸ¯ Brand Analysis Dashboard</h1>', unsafe_allow_html=True)
    st.markdown("---")
    
    # Load data
    with st.spinner("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­..."):
        analyzer = load_brand_data()
        store_analyzer = load_store_analyzer()
    
    if analyzer is None:
        st.error("ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return
    
    # Sidebar menu
    st.sidebar.header("ğŸ“Š Analysis Menu")
    
    page = st.sidebar.selectbox(
        "æ©Ÿèƒ½ã‚’é¸æŠ:",
        [
            "ğŸ  ãƒ–ãƒ©ãƒ³ãƒ‰é¡ä¼¼åº¦æ¤œç´¢", 
            "ğŸª åº—èˆ—ä¸€è‡´ç‡åˆ†æ", 
            "â• æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ",
            "ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦³"
        ]
    )
    
    # Main content
    if page == "ğŸ  ãƒ–ãƒ©ãƒ³ãƒ‰é¡ä¼¼åº¦æ¤œç´¢":
        show_brand_similarity_search(analyzer)
    elif page == "ğŸª åº—èˆ—ä¸€è‡´ç‡åˆ†æ":
        show_store_overlap_analysis(store_analyzer)
    elif page == "â• æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ":
        show_new_brand_addition(analyzer)
    elif page == "ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦³":
        show_data_overview(analyzer, store_analyzer)

def show_brand_similarity_search(analyzer):
    """ãƒ–ãƒ©ãƒ³ãƒ‰é¡ä¼¼åº¦æ¤œç´¢"""
    st.header("ğŸ” ãƒ–ãƒ©ãƒ³ãƒ‰é¡ä¼¼åº¦æ¤œç´¢")
    
    if analyzer.df is None:
        st.error("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        target_brand = st.selectbox(
            "æ¤œç´¢ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’é¸æŠ:",
            analyzer.df['name'].tolist()
        )
    
    with col2:
        top_k = st.slider("è¡¨ç¤ºä»¶æ•°:", 5, 20, 10)
        min_similarity = st.slider("æœ€å°é¡ä¼¼åº¦:", 0.0, 1.0, 0.1, 0.05)
    
    # ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
    enable_location_boost = st.checkbox("ğŸª åº—èˆ—æƒ…å ±ã«ã‚ˆã‚‹ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°", value=True)
    
    if enable_location_boost:
        bias_strength = st.slider("åº—èˆ—ãƒã‚¤ã‚¢ã‚¹å¼·åº¦:", 0.0, 1.0, 0.3, 0.1)
    else:
        bias_strength = 0.0
    
    if st.button("ğŸš€ æ¤œç´¢å®Ÿè¡Œ", type="primary"):
        with st.spinner("é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’æ¤œç´¢ä¸­..."):
            # åŸºæœ¬çš„ãªé¡ä¼¼åº¦è¨ˆç®—
            try:
                target_idx = analyzer.df[analyzer.df['name'] == target_brand].index[0]
                target_embedding = analyzer.embeddings[target_idx]
                
                # é¡ä¼¼åº¦è¨ˆç®—
                similarities = cosine_similarity([target_embedding], analyzer.embeddings)[0]
                
                # çµæœæ•´ç†
                results = []
                for i, sim in enumerate(similarities):
                    if i != target_idx and sim >= min_similarity:
                        brand_name = analyzer.df.iloc[i]['name']
                        results.append({
                            'brand_name': brand_name,
                            'similarity': sim,
                            'description': analyzer.df.iloc[i]['description']
                        })
                
                # ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨
                if enable_location_boost and results:
                    try:
                        reranker = create_reranker_from_streamlit_data(analyzer)
                        if reranker and target_brand in reranker.brand_locations:
                            similarity_dict = {r['brand_name']: r['similarity'] for r in results}
                            reranked = reranker.rerank_similarity_with_location_bias(
                                similarity_dict, target_brand, bias_strength
                            )
                            for r in results:
                                if r['brand_name'] in reranked:
                                    r['final_similarity'] = reranked[r['brand_name']]
                                    r['location_boost'] = reranked[r['brand_name']] - r['similarity']
                                else:
                                    r['final_similarity'] = r['similarity']
                                    r['location_boost'] = 0
                            
                            # æœ€çµ‚é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
                            results.sort(key=lambda x: x['final_similarity'], reverse=True)
                        else:
                            for r in results:
                                r['final_similarity'] = r['similarity']
                                r['location_boost'] = 0
                    except Exception as e:
                        st.warning(f"ä½ç½®æƒ…å ±ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                        for r in results:
                            r['final_similarity'] = r['similarity']
                            r['location_boost'] = 0
                else:
                    for r in results:
                        r['final_similarity'] = r['similarity']
                        r['location_boost'] = 0
                    results.sort(key=lambda x: x['similarity'], reverse=True)
                
                # ä¸Šä½Kä»¶ã«çµã‚‹
                top_results = results[:top_k]
                
                if top_results:
                    st.success(f"âœ… {len(top_results)} ä»¶ã®é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                    
                    # çµæœè¡¨ç¤º
                    display_data = []
                    for i, result in enumerate(top_results, 1):
                        display_data.append({
                            'é †ä½': f"#{i}",
                            'ãƒ–ãƒ©ãƒ³ãƒ‰å': result['brand_name'],
                            'ãƒ™ãƒ¼ã‚¹é¡ä¼¼åº¦': f"{result['similarity']:.4f}",
                            'æœ€çµ‚é¡ä¼¼åº¦': f"{result['final_similarity']:.4f}",
                            'ä½ç½®ãƒ–ãƒ¼ã‚¹ãƒˆ': f"+{result['location_boost']:.3f}" if result['location_boost'] > 0.001 else "---"
                        })
                    
                    df_display = pd.DataFrame(display_data)
                    st.dataframe(df_display, use_container_width=True, hide_index=True)
                    
                    # å¯è¦–åŒ–
                    if len(top_results) > 1:
                        brands = [r['brand_name'] for r in top_results[:8]]
                        base_sims = [r['similarity'] for r in top_results[:8]]
                        final_sims = [r['final_similarity'] for r in top_results[:8]]
                        
                        fig = go.Figure()
                        fig.add_trace(go.Bar(
                            name='ãƒ™ãƒ¼ã‚¹é¡ä¼¼åº¦',
                            x=brands,
                            y=base_sims,
                            marker_color='lightblue'
                        ))
                        fig.add_trace(go.Bar(
                            name='æœ€çµ‚é¡ä¼¼åº¦',
                            x=brands,
                            y=final_sims,
                            marker_color='darkblue'
                        ))
                        
                        fig.update_layout(
                            title=f"é¡ä¼¼åº¦æ¯”è¼ƒ: {target_brand}",
                            xaxis_title="ãƒ–ãƒ©ãƒ³ãƒ‰å",
                            yaxis_title="é¡ä¼¼åº¦",
                            barmode='group',
                            height=400,
                            xaxis={'tickangle': 45}
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
            except Exception as e:
                st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

def show_store_overlap_analysis(store_analyzer):
    """åº—èˆ—ä¸€è‡´ç‡åˆ†æ"""
    st.header("ğŸª åº—èˆ—ä¸€è‡´ç‡åˆ†æ")
    
    if store_analyzer is None:
        st.error("åº—èˆ—ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return
    
    # å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
    report = store_analyzer.get_separation_quality_report()
    
    st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ç·åº—èˆ—æ•°", report['ç·ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ•°'])
    with col2:
        st.metric("åˆ†é›¢æˆåŠŸç‡", f"{report['åº—èˆ—ååˆ†é›¢ç‡']:.1%}")
    with col3:
        st.metric("æ¤œå‡ºãƒ–ãƒ©ãƒ³ãƒ‰æ•°", report['æ¤œå‡ºãƒ–ãƒ©ãƒ³ãƒ‰æ•°'])
    with col4:
        st.metric("å¹³å‡åº—èˆ—æ•°/ãƒ–ãƒ©ãƒ³ãƒ‰", f"{report['å¹³å‡åº—èˆ—æ•°']:.1f}")
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»åº—èˆ—ã‚µãƒãƒªãƒ¼
    st.subheader("ğŸ† åº—èˆ—æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    summary = store_analyzer.get_brand_store_summary()
    st.dataframe(summary.head(20), use_container_width=True, hide_index=True)
    
    # é«˜ã„åº—èˆ—ä¸€è‡´ç‡ãƒšã‚¢
    st.subheader("ğŸ”— é«˜ã„åº—èˆ—ä¸€è‡´ç‡ãƒšã‚¢")
    threshold = st.slider("ä¸€è‡´ç‡é–¾å€¤:", 0.1, 1.0, 0.3, 0.05)
    
    high_overlap = store_analyzer.find_high_overlap_brands(threshold=threshold)
    
    if high_overlap:
        st.info(f"ğŸ“ {len(high_overlap)} çµ„ã®ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        
        display_data = []
        for pair in high_overlap[:20]:
            display_data.append({
                'ãƒ–ãƒ©ãƒ³ãƒ‰1': pair['ãƒ–ãƒ©ãƒ³ãƒ‰1'],
                'ãƒ–ãƒ©ãƒ³ãƒ‰2': pair['ãƒ–ãƒ©ãƒ³ãƒ‰2'],
                'åº—èˆ—ä¸€è‡´ç‡': f"{pair['åº—èˆ—ä¸€è‡´ç‡']:.3f}",
                'å…±é€šåº—èˆ—æ•°': pair['å…±é€šåº—èˆ—æ•°'],
                'å…±é€šåº—èˆ—': pair['å…±é€šåº—èˆ—'][:100] + "..." if len(pair['å…±é€šåº—èˆ—']) > 100 else pair['å…±é€šåº—èˆ—']
            })
        
        df_pairs = pd.DataFrame(display_data)
        st.dataframe(df_pairs, use_container_width=True, hide_index=True)
    else:
        st.warning("æŒ‡å®šã•ã‚ŒãŸé–¾å€¤ä»¥ä¸Šã®ä¸€è‡´ç‡ã‚’æŒã¤ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

def show_new_brand_addition(analyzer):
    """æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ """
    st.header("â• æ–°ãƒ–ãƒ©ãƒ³ãƒ‰è¿½åŠ ")
    
    st.markdown("""
    <div class="success-box">
    <h4>ğŸš€ æ–°ãƒ–ãƒ©ãƒ³ãƒ‰åˆ†æ</h4>
    <p>ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’å…¥åŠ›ã—ã€GPT-OSSã¾ãŸã¯Gemini APIã‚’ä½¿ã£ã¦èª¬æ˜æ–‡ã‚’ç”Ÿæˆå¾Œã€æ—¢å­˜ãƒ–ãƒ©ãƒ³ãƒ‰ã¨ã®é¡ä¼¼åº¦ã‚’åˆ†æã—ã¾ã™ã€‚</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Model selection
    model_option = st.radio(
        "ğŸ¤– ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:",
        ["GPT-OSS (ãƒ­ãƒ¼ã‚«ãƒ«)", "Gemini API (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³)"],
        help="GPT-OSSã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œã—ã€Gemini APIã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¥ç¶šãŒå¿…è¦ã§ã™"
    )
    
    use_gpt_oss = model_option.startswith("GPT-OSS")
    
    # API Key input (only for Gemini)
    api_key = None
    if not use_gpt_oss:
        api_key = st.text_input(
            "ğŸ”‘ Gemini API Key:",
            type="password",
            help="Google AI Studioã§APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ãã ã•ã„"
        )
    
    # Brand input
    brand_name = st.text_input(
        "ğŸ·ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰å:", 
        placeholder="ä¾‹: Stone Island, Fear of God, Acne Studios"
    )
    
    # CSVæ‰¹é‡å‡¦ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.markdown("---")
    st.subheader("ğŸ“Š CSVä¸€æ‹¬å‡¦ç†")
    
    uploaded_file = st.file_uploader(
        "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰:",
        type=['csv'],
        help="nameåˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    if uploaded_file is not None:
        try:
            # CSVèª­ã¿è¾¼ã¿
            df = pd.read_csv(uploaded_file)
            st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(df)} è¡Œ")
            
            if 'name' not in df.columns:
                st.error("'name' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return
            
            st.write("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
            st.dataframe(df.head(), use_container_width=True)
            
            # ä¸€æ‹¬å‡¦ç†è¨­å®š
            col1, col2 = st.columns(2)
            with col1:
                batch_size = st.slider("ãƒãƒƒãƒã‚µã‚¤ã‚º:", 1, 20, 5)
            with col2:
                start_from = st.number_input("é–‹å§‹è¡Œ (0ã‹ã‚‰):", 0, len(df)-1, 0)
            
            # ä¸€æ‹¬å‡¦ç†å®Ÿè¡Œ
            if st.button("ğŸš€ ä¸€æ‹¬èª¬æ˜æ–‡ç”Ÿæˆé–‹å§‹", type="primary"):
                progress_bar = st.progress(0)
                status_text = st.empty()
                results = []
                
                # å‡¦ç†å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
                process_df = df[start_from:start_from+batch_size].copy()
                
                for i, row in process_df.iterrows():
                    brand_name_batch = row['name']
                    
                    # é€²æ—æ›´æ–°
                    progress = (i - start_from + 1) / len(process_df)
                    progress_bar.progress(progress)
                    status_text.text(f"å‡¦ç†ä¸­: {brand_name_batch} ({i-start_from+1}/{len(process_df)})")
                    
                    # èª¬æ˜æ–‡ç”Ÿæˆ
                    try:
                        description = generate_brand_description(api_key, brand_name_batch, use_gpt_oss)
                        if description:
                            results.append({
                                'ãƒ–ãƒ©ãƒ³ãƒ‰å': brand_name_batch,
                                'ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡': description,
                                'çŠ¶æ…‹': 'æˆåŠŸ'
                            })
                        else:
                            results.append({
                                'ãƒ–ãƒ©ãƒ³ãƒ‰å': brand_name_batch,
                                'ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡': 'ç”Ÿæˆå¤±æ•—',
                                'çŠ¶æ…‹': 'ã‚¨ãƒ©ãƒ¼'
                            })
                    except Exception as e:
                        results.append({
                            'ãƒ–ãƒ©ãƒ³ãƒ‰å': brand_name_batch,
                            'ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡': f'ã‚¨ãƒ©ãƒ¼: {str(e)}',
                            'çŠ¶æ…‹': 'ã‚¨ãƒ©ãƒ¼'
                        })
                
                # çµæœè¡¨ç¤º
                if results:
                    result_df = pd.DataFrame(results)
                    success_count = len(result_df[result_df['çŠ¶æ…‹'] == 'æˆåŠŸ'])
                    
                    st.success(f"âœ… å‡¦ç†å®Œäº†: {success_count}/{len(results)} ä»¶æˆåŠŸ")
                    st.dataframe(result_df, use_container_width=True)
                    
                    # CSVå‡ºåŠ›
                    csv_output = result_df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ çµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_output,
                        file_name=f"brand_descriptions_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                
                progress_bar.empty()
                status_text.empty()
        except Exception as e:
            st.error(f"CSVå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # å˜ä¸€ãƒ–ãƒ©ãƒ³ãƒ‰å‡¦ç†
    st.markdown("---")
    st.subheader("ğŸ¯ å˜ä¸€ãƒ–ãƒ©ãƒ³ãƒ‰åˆ†æ")
    
    # æ¡ä»¶ãƒã‚§ãƒƒã‚¯
    can_proceed = brand_name and (use_gpt_oss or api_key)
    
    if not can_proceed:
        if use_gpt_oss:
            st.info("ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info("APIã‚­ãƒ¼ã¨ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        return
    
    # Analysis button
    if st.button("ğŸš€ å˜ä¸€ãƒ–ãƒ©ãƒ³ãƒ‰åˆ†æé–‹å§‹", type="primary"):
        try:
            # Step 1: Generate description
            with st.spinner("èª¬æ˜æ–‡ã‚’ç”Ÿæˆä¸­..."):
                description = generate_brand_description(api_key, brand_name, use_gpt_oss)
                if not description:
                    st.error("èª¬æ˜æ–‡ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    return
            
            st.success("âœ… èª¬æ˜æ–‡ç”Ÿæˆå®Œäº†")
            with st.expander("ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡", expanded=True):
                st.write(description)
            
            # Step 2: Calculate similarities (simplified)
            st.info("ğŸ” é¡ä¼¼åº¦è¨ˆç®—ã¯å®Ÿè£…ä¸­ã§ã™ã€‚ç¾åœ¨ã¯èª¬æ˜æ–‡ç”Ÿæˆã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚")
            
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

def show_data_overview(analyzer, store_analyzer):
    """ãƒ‡ãƒ¼ã‚¿æ¦‚è¦³"""
    st.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦³")
    
    if analyzer is None:
        st.error("ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # åŸºæœ¬çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ç·ãƒ–ãƒ©ãƒ³ãƒ‰æ•°", len(analyzer.df))
    with col2:
        if analyzer.embeddings is not None:
            st.metric("åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°", analyzer.embeddings.shape[1])
        else:
            st.metric("åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°", "æœªç”Ÿæˆ")
    with col3:
        avg_desc_len = analyzer.df['description'].str.len().mean()
        st.metric("å¹³å‡èª¬æ˜æ–‡é•·", f"{avg_desc_len:.0f}æ–‡å­—")
    with col4:
        if store_analyzer:
            st.metric("åº—èˆ—ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ©ãƒ³ãƒ‰æ•°", store_analyzer.get_separation_quality_report()['æ¤œå‡ºãƒ–ãƒ©ãƒ³ãƒ‰æ•°'])
        else:
            st.metric("åº—èˆ—ãƒ‡ãƒ¼ã‚¿", "æœªèª­ã¿è¾¼ã¿")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«
    st.subheader("ğŸ“‹ ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«")
    st.dataframe(analyzer.df.head(10), use_container_width=True)

if __name__ == "__main__":
    main()