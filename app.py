import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from transformers import AutoTokenizer, AutoModel
import torch
import google.generativeai as genai
import os
import time
import re
from sklearn.cluster import KMeans
import umap
from anchor_based_embedding import AnchorBasedEmbedding, compare_embedding_methods

genai.configure(api_key="AIzaSyCV5BV5i514ouP0fqp_1lCoMyJKaDoboqA")
gemini_model = genai.GenerativeModel('gemini-1.5-flash')

# Ruri v3ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã¨ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# å‰å›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å ´æ‰€ã«åˆã‚ã›ã¦ './ruri_v3_downloaded_from_hub' ã‚’æŒ‡å®š
RURI_MODEL_PATH = "./ruri_v3_downloaded_from_hub" 
try:
    @st.cache_resource # Streamlitã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
    def load_ruri_model():
        tokenizer = AutoTokenizer.from_pretrained(RURI_MODEL_PATH)
        # float16ã§ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„
        model = AutoModel.from_pretrained(RURI_MODEL_PATH, torch_dtype=torch.float16)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()
        return tokenizer, model, device
    
    ruri_tokenizer, ruri_model, ruri_device = load_ruri_model()
except Exception as e:
    st.error(f"Ruri v3ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Ruriãƒ¢ãƒ‡ãƒ«ãŒ {RURI_MODEL_PATH} ã«å­˜åœ¨ã™ã‚‹ã‹ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# æ—¢å­˜ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
@st.cache_data # Streamlitã§ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
def load_existing_data():
    try:
        df = pd.read_csv("description.csv")
        # åˆ—åã®ãƒªãƒãƒ¼ãƒ å‡¦ç†
        if 'ãƒ–ãƒ©ãƒ³ãƒ‰å' in df.columns and 'name' not in df.columns:
            df.rename(columns={'ãƒ–ãƒ©ãƒ³ãƒ‰å': 'name'}, inplace=True)
        if 'bline_id' in df.columns and 'id' not in df.columns:
            df.rename(columns={'bline_id': 'id'}, inplace=True)
        elif 'id' not in df.columns:
            df['id'] = range(len(df)) # ãƒ€ãƒŸãƒ¼ID

        # Ruri v3 raw ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        embeddings_path = "./ruri_embeddings_results/ruri_description_embeddings_v3_raw_hub.npy"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è­¦å‘Šã‚’å‡ºã—ã¦ç©ºã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿”ã™ã‹ã€ã‚¨ãƒ©ãƒ¼ã§åœæ­¢
        if not os.path.exists(embeddings_path):
            st.warning(f"æ—¢å­˜ã®Ruri v3ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {embeddings_path}ã€‚æ–°ã—ã„ãƒ–ãƒ©ãƒ³ãƒ‰ã®è¿½åŠ ã¯å¯èƒ½ã§ã™ãŒã€æ—¢å­˜ãƒ–ãƒ©ãƒ³ãƒ‰ã®ãƒãƒƒãƒ—ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ã€‚")
            return df, np.array([]) # æ—¢å­˜ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒãªã„å ´åˆã¯ç©ºé…åˆ—ã‚’è¿”ã™

        embeddings = np.load(embeddings_path)

        if embeddings.shape[0] != len(df):
            st.warning(f"æ—¢å­˜ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ•° ({embeddings.shape[0]}) ãŒãƒ–ãƒ©ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿æ•° ({len(df)}) ã¨ä¸€è‡´ã—ã¾ã›ã‚“ã€‚å¯è¦–åŒ–ã«å½±éŸ¿ãŒå‡ºã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            # ä¸ä¸€è‡´ã®å ´åˆã®å‡¦ç†ï¼ˆä¾‹ï¼šå°‘ãªã„æ–¹ã«åˆã‚ã›ã‚‹ãªã©ï¼‰
            min_rows = min(embeddings.shape[0], len(df))
            df = df.iloc[:min_rows].copy()
            embeddings = embeddings[:min_rows]
            
        return df, embeddings
    except Exception as e:
        st.error(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚description.csvã¨ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

df_existing, embeddings_existing = load_existing_data()

# UMAPãƒªãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é«˜é€ŸåŒ–
@st.cache_resource
def get_umap_reducer(embeddings_data, method="Standard UMAP", n_anchors=75, lambda_anchor=0.1):
    if embeddings_data.size == 0: # æ—¢å­˜ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒãªã„å ´åˆã¯UMAPã‚’fitã§ããªã„
        return None
    # é¸æŠã•ã‚ŒãŸå¯è¦–åŒ–æ‰‹æ³•ã«å¿œã˜ã¦ãƒªãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼ã‚’ä½œæˆ
    if method == "ã‚¢ãƒ³ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿":
        reducer = AnchorBasedEmbedding(
            n_anchors=min(n_anchors, embeddings_data.shape[0]//5),
            lambda_anchor=lambda_anchor,
            n_components=2,
            n_neighbors=min(30, embeddings_data.shape[0] -1),
            min_dist=0.05,
            spread=1.5,
            random_state=42,
            n_epochs=300
        )
    else:  # Standard UMAP
        reducer = umap.UMAP(
            n_components=2,
            n_neighbors=min(30, embeddings_data.shape[0] -1),
            min_dist=0.05,
            spread=1.5,
            metric='cosine',
            random_state=42,
            n_epochs=500
        )
    # For AnchorBasedEmbedding, we don't pre-fit since it only has fit_transform
    if isinstance(reducer, AnchorBasedEmbedding):
        pass  # Will use fit_transform when needed
    else:
        reducer.fit(embeddings_data)
    return reducer


def create_visualization_plot(df, brand_name, method_name, x_col='umap_x', y_col='umap_y'):
    """å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    # ã‚ˆã‚ŠåŠ¹æœçš„ãªå¯è¦–åŒ–ã®ãŸã‚ã«ã€å¼·èª¿è¡¨ç¤ºç”¨ã®è‰²ã¨ã‚µã‚¤ã‚ºã‚’å€‹åˆ¥ã«è¨­å®š
    df['marker_size'] = df['is_highlighted'].astype(int) * 6 + 4  # False=4, True=10 (ã‚µã‚¤ã‚ºã‚’å°ã•ã)
    df['cluster_str'] = df['cluster'].astype(str)
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã«å¿œã˜ãŸæœ€é©ãªè‰²ãƒ‘ãƒ¬ãƒƒãƒˆã‚’é¸æŠ
    n_clusters = df['cluster'].nunique()
    
    # ã‚ˆã‚Šæ˜ç¢ºã«åŒºåˆ¥ã§ãã‚‹è‰²ãƒ‘ãƒ¬ãƒƒãƒˆï¼ˆå½©åº¦ã¨æ˜åº¦ã‚’èª¿æ•´ï¼‰
    color_palette = [
        '#E74C3C', '#3498DB', '#2ECC71', '#F39C12', '#9B59B6',
        '#1ABC9C', '#E67E22', '#34495E', '#F1C40F', '#E91E63',
        '#8E44AD', '#27AE60', '#D35400', '#2980B9', '#C0392B',
        '#16A085', '#7F8C8D', '#BDC3C7', '#95A5A6', '#F4F6F7'
    ]
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒè‰²ãƒ‘ãƒ¬ãƒƒãƒˆã‚ˆã‚Šå¤šã„å ´åˆã¯ç¹°ã‚Šè¿”ã—
    if n_clusters > len(color_palette):
        color_palette = color_palette * ((n_clusters // len(color_palette)) + 1)
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼IDã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¦category_ordersã‚’æ˜ç¤ºçš„ã«è¨­å®š
    cluster_ids = sorted(df['cluster_str'].unique())
    
    # åŸºæœ¬ã®Plotlyã§å¯è¦–åŒ– (é›¢æ•£çš„ãªè‰²ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨)
    fig = px.scatter(
        df,
        x=x_col,
        y=y_col,
        color='cluster_str',
        size='marker_size',
        size_max=2,  # æœ€å¤§ã‚µã‚¤ã‚ºã‚’ç¸®å°
        hover_data=['name', 'description_preview'],
        hover_name='name',
        title=f'ãƒ–ãƒ©ãƒ³ãƒ‰é¡ä¼¼åº¦ãƒãƒƒãƒ— ({method_name}) - {brand_name} ã‚’è¿½åŠ ',
        width=1200,  # å¹…ã‚’æ‹¡å¤§ã—ã¦è¦‹ã‚„ã™ã
        height=800,   # é«˜ã•ã‚’æ‹¡å¤§
        color_discrete_sequence=color_palette[:n_clusters],  # é›¢æ•£çš„ãªè‰²ã‚’ä½¿ç”¨
        category_orders={'cluster_str': cluster_ids}  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’é›¢æ•£ã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦æ˜ç¤º
    )
    
    # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ç‰¹åˆ¥ã«å¼·èª¿è¡¨ç¤º
    highlighted_data = df[df['is_highlighted'] == True]
    if not highlighted_data.empty:
        fig.add_trace(
            go.Scatter(
                x=highlighted_data[x_col],
                y=highlighted_data[y_col],
                mode='markers',
                marker=dict(
                    size=15,  # æ–°è¦è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã‚µã‚¤ã‚ºã‚‚èª¿æ•´
                    color='#FF0000',
                    symbol='diamond',  # ã‚ˆã‚Šç›®ç«‹ã¤ã‚·ãƒ³ãƒœãƒ«
                    line=dict(width=2, color='#8B0000')
                ),
                name=f'æ–°è¦è¿½åŠ : {brand_name}',
                text=highlighted_data['name'],
                hovertemplate='<b>%{text}</b><br>' +
                            'X: %{x:.2f}<br>' +
                            'Y: %{y:.2f}<br>' +
                            '<i>æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸãƒ–ãƒ©ãƒ³ãƒ‰</i><extra></extra>'
            )
        )
    
    # ãƒãƒ¼ã‚«ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ”¹å–„
    fig.update_traces(
        marker=dict(
            line=dict(width=0.5, color='white'),  # ãƒãƒ¼ã‚«ãƒ¼ã®ç¸ã‚’ç´°ã
            opacity=0.9,  # ä¸é€æ˜åº¦ã‚’ä¸Šã’ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è‰²ã‚’ã‚ˆã‚Šæ˜ç¢ºã«
            sizemin=3,    # æœ€å°ã‚µã‚¤ã‚ºã‚’è¨­å®š
            sizemode='diameter'  # ã‚µã‚¤ã‚ºãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®š
        )
    )
    
    fig.update_layout(
        title_x=0.5,
        font=dict(size=14),
        hovermode='closest',
        legend=dict(
            title='ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼',
            itemsizing='constant', 
            tracegroupgap=0,
            orientation='v',
            x=1.02,
            y=1,
            bgcolor='rgba(255,255,255,0.8)',
            bordercolor='rgba(0,0,0,0.2)',
            borderwidth=1
        ),
        showlegend=True,
        plot_bgcolor='white',
        paper_bgcolor='white'
    )
    
    return fig


# --- Streamlit UI ---
st.title("ğŸ‘— ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰é¡ä¼¼åº¦æ¢ç´¢ã‚¢ãƒ—ãƒª")
st.markdown("ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’å…¥åŠ›ã—ã¦ã€GeminiãŒç”Ÿæˆã—ãŸèª¬æ˜æ–‡ã‹ã‚‰é¡ä¼¼ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

# å¯è¦–åŒ–æ‰‹æ³•ã®é¸æŠ
st.sidebar.header("ğŸ¨ å¯è¦–åŒ–è¨­å®š")
visualization_method = st.sidebar.selectbox(
    "å¯è¦–åŒ–æ‰‹æ³•ã‚’é¸æŠ",
    ["Standard UMAP", "ã‚¢ãƒ³ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿", "æ‰‹æ³•æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰"],
    index=1
)

# Default values
n_anchors = 75
lambda_anchor = 0.1

if visualization_method == "ã‚¢ãƒ³ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿":
    st.sidebar.subheader("ğŸ¯ ã‚¢ãƒ³ã‚«ãƒ¼è¨­å®š")
    n_anchors = st.sidebar.slider("ã‚¢ãƒ³ã‚«ãƒ¼æ•°", 20, 200, 75, 5)
    lambda_anchor = st.sidebar.slider("ã‚¢ãƒ³ã‚«ãƒ¼å¼•åŠ›é‡ã¿ (Î»)", 0.01, 0.5, 0.1, 0.01)
    
elif visualization_method == "æ‰‹æ³•æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰":
    st.sidebar.info("è¤‡æ•°ã®æ‰‹æ³•ã‚’åŒæ™‚ã«æ¯”è¼ƒè¡¨ç¤ºã—ã¾ã™")

# Initialize UMAP reducer after UI components are defined
umap_reducer = get_umap_reducer(embeddings_existing, visualization_method, n_anchors, lambda_anchor)

# --- ãƒ–ãƒ©ãƒ³ãƒ‰åå…¥åŠ› ---
brand_name_input = st.text_input("åˆ†æã—ãŸã„ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", "ZARA") # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ZARAã«

# --- Geminièª¬æ˜æ–‡ç”Ÿæˆã¨ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã€å¯è¦–åŒ– ---
if st.button("Geminiã§èª¬æ˜æ–‡ã‚’ç”Ÿæˆ & ãƒãƒƒãƒ—ã«è¡¨ç¤º"):
    if not brand_name_input:
        st.warning("ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        generated_description = ""
        gemini_prompt = f"""ã‚ãªãŸã¯ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³æ¥­ç•Œã®å°‚é–€ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰ã€Œ{brand_name_input}ã€ã®ç‰¹å¾´ã‚’300æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ã€ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒ‡ã‚¶ã‚¤ãƒ³å“²å­¦ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã€ä»£è¡¨çš„ãªã‚¢ã‚¤ãƒ†ãƒ ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚
        
        ### è‰¯è³ªãªä¾‹1:
        ãƒ–ãƒ©ãƒ³ãƒ‰å: ã‚³ãƒ  ãƒ‡ ã‚®ãƒ£ãƒ«ã‚½ãƒ³ å‚è€ƒæƒ…å ±: ã‚³ãƒ  ãƒ‡ ã‚®ãƒ£ãƒ«ã‚½ãƒ³(COMME des GARÃ‡ONS)ã¯æ—¥æœ¬ã®ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰ã€‚å‰µæ¥­è€…å·ä¹…ä¿ç²ã€‚é»’ãªã©ãƒ¢ãƒãƒˆãƒ¼ãƒ³ã‚’å¤šæ§˜ã€å­¤é«˜ã®å¥³æ€§ã‚’æã„ãŸã€‚ èª¬æ˜æ–‡: ã‚³ãƒ  ãƒ‡ ã‚®ãƒ£ãƒ«ã‚½ãƒ³ã¯ã€å·ä¹…ä¿ç²ãŒæ‰‹æ›ã‘ã‚‹é©æ–°çš„ãªãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰ã€‚é»’ã‚’åŸºèª¿ã¨ã—ãŸãƒ¢ãƒãƒˆãƒ¼ãƒ³ã®è‰²ä½¿ã„ã€å·¦å³éå¯¾ç§°ãªã‚«ãƒƒãƒ†ã‚£ãƒ³ã‚°ã€èº«ä½“ã®ãƒ©ã‚¤ãƒ³ã‚’æ›–æ˜§ã«ã™ã‚‹ãƒ«ãƒ¼ã‚ºãªã‚·ãƒ«ã‚¨ãƒƒãƒˆãŒç‰¹å¾´ã€‚ä»£è¡¨çš„ãªã‚¢ã‚¤ãƒ†ãƒ ã¯ç‹¬å‰µçš„ãªãƒ•ã‚©ãƒ«ãƒ ã®ã‚¸ãƒ£ã‚±ãƒƒãƒˆã‚„ãƒ‰ãƒ¬ã‚¹ã€ç©´ã‚ããƒ‹ãƒƒãƒˆã€‚ã‚¦ãƒ¼ãƒ«ã‚®ãƒ£ãƒã‚¸ãƒ³ãªã©ç‹¬è‡ªåŠ å·¥ã®ç´ æã‚’å¤šç”¨ã—ã€æµè¡Œã«å·¦å³ã•ã‚Œãšå€‹æ€§ã‚’è¡¨ç¾ã—ãŸã„ã‚¢ãƒ¼ãƒˆå¿—å‘ã®å±¤ã«æ”¯æŒã•ã‚Œã‚‹ã€‚
        ### è‰¯è³ªãªä¾‹2:
        ãƒ–ãƒ©ãƒ³ãƒ‰å: ã‚¸ãƒ« ã‚µãƒ³ãƒ€ãƒ¼ å‚è€ƒæƒ…å ±: ã‚¸ãƒ« ã‚µãƒ³ãƒ€ãƒ¼ã¯Ms.ã‚¸ãƒ«ãƒ»ã‚µãƒ³ãƒ€ãƒ¼ãŒè¨­ç«‹ã€‚æ´—ç·´ã•ã‚Œã¦ç¹Šç´°ã€ã‹ã¤å“è³ªã«ã“ã ã‚ã£ãŸãƒŸãƒ‹ãƒãƒ«ãªãƒ‡ã‚¶ã‚¤ãƒ³ãŒç‰¹å¾´ã€‚ èª¬æ˜æ–‡: ã‚¸ãƒ« ã‚µãƒ³ãƒ€ãƒ¼ã¯ã€ãƒ‰ã‚¤ãƒ„ç™ºã®ãƒŸãƒ‹ãƒãƒªã‚ºãƒ ã‚’ä»£è¡¨ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ã€‚ç´”ç²‹ã•ã¨å“è³ªã‚’è¿½æ±‚ã—ã€ä¸è¦ãªè£…é£¾ã‚’å‰Šãè½ã¨ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ã§æ´—ç·´ã•ã‚ŒãŸãƒ‡ã‚¶ã‚¤ãƒ³ãŒç‰¹å¾´ã€‚ä»£è¡¨çš„ãªã‚¢ã‚¤ãƒ†ãƒ ã¯å®Œç’§ãªã‚«ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®ã‚·ãƒ£ãƒ„ã€æ§‹ç¯‰çš„ã‚·ãƒ«ã‚¨ãƒƒãƒˆã®ã‚³ãƒ¼ãƒˆã€‚ã‚«ã‚·ãƒŸã‚¢ã€ã‚¦ãƒ¼ãƒ«ã€ã‚·ãƒ«ã‚¯ç­‰ã®é«˜å“è³ªãªå¤©ç„¶ç´ æã‚’å¥½ã¿ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã‚«ãƒ©ãƒ¼ã‚’åŸºèª¿ã¨ã—ãŸè½ã¡ç€ã„ãŸãƒ‘ãƒ¬ãƒƒãƒˆã€‚æµè¡Œã«å·¦å³ã•ã‚Œãªã„ã‚¿ã‚¤ãƒ ãƒ¬ã‚¹ãªã‚¨ãƒ¬ã‚¬ãƒ³ã‚¹ã‚’æ±‚ã‚ã‚‹çŸ¥çš„ãªå¤§äººã«æ”¯æŒã•ã‚Œã‚‹ã€‚
        ### å‡¦ç†å¯¾è±¡:
        ãƒ–ãƒ©ãƒ³ãƒ‰å: {brand_name_input}
        èª¬æ˜æ–‡:"""
        
        max_retries = 3 
        for attempt in range(max_retries):
            try:
                with st.spinner(f"Geminiã§èª¬æ˜æ–‡ã‚’ç”Ÿæˆä¸­... (è©¦è¡Œ {attempt + 1}/{max_retries})"):
                    response = gemini_model.generate_content(gemini_prompt)
                    generated_description = response.text.strip()
                st.success("èª¬æ˜æ–‡ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                st.subheader(f"{brand_name_input} ã®ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡:")
                st.write(generated_description)
                break 
            except Exception as e:
                error_message = str(e)
                st.warning(f"'{brand_name_input}' ã®èª¬æ˜ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message}")
                if "quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"" in error_message:
                    st.error("1æ—¥ã®ç„¡æ–™ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¯ã‚©ãƒ¼ã‚¿ã‚’è¶…éã—ã¾ã—ãŸã€‚ç¿Œæ—¥ä»¥é™ã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
                    break 
                elif "retry_delay" in error_message:
                    match = re.search(r'seconds:\s*(\d+)', error_message)
                    delay = int(match.group(1)) if match else 10
                    st.info(f"ä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ã§ã™ã€‚{delay}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™ (æ®‹ã‚Š{max_retries - 1 - attempt}å›)ã€‚")
                    time.sleep(delay + 1)
                else:
                    st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message}")
                    break
        else: 
            st.error(f"'{brand_name_input}' ã®èª¬æ˜ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            generated_description = "" 

        if generated_description:
            with st.spinner(f"{brand_name_input}ã‚’Ruri v3ã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã—ã€ãƒãƒƒãƒ—ã‚’æ›´æ–°ä¸­..."):
                new_inputs = ruri_tokenizer(
                    [generated_description],
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                )
                new_inputs = {k: v.to(ruri_device) for k, v in new_inputs.items()}
                with torch.no_grad():
                    new_outputs = ruri_model(**new_inputs)
                new_embedding = new_outputs.last_hidden_state[:, 0, :].cpu().numpy()

                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã«æ–°ã—ã„ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’è¿½åŠ 
                new_brand_id = df_existing['id'].max() + 1 if 'id' in df_existing.columns and not df_existing.empty else 0
                new_row = pd.DataFrame([{
                    'name': brand_name_input,
                    'description': generated_description, 
                    'id': new_brand_id,
                    'cluster': -1, 
                    'is_highlighted': True # â˜…æ–°ã—ãè¿½åŠ ã—ãŸãƒ–ãƒ©ãƒ³ãƒ‰ã«ãƒ•ãƒ©ã‚°â˜…
                }])
                
                if 'is_highlighted' not in df_existing.columns:
                    df_existing['is_highlighted'] = False
                else:
                    # ä»¥å‰ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã—ãŸã„å ´åˆã¯ False ã«è¨­å®š
                    # df_existing ãŒã‚³ãƒ”ãƒ¼ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€å…ƒã® df_existing ã«ã¯å½±éŸ¿ã—ãªã„
                    # df_combined ä½œæˆæ™‚ã«æ–°ã—ã„ is_highlighted ã‚’ä½¿ã†
                    pass 


                df_combined = pd.concat([df_existing, new_row], ignore_index=True)
                
                # ã“ã“ã§ df_combined ã® 'is_highlighted' åˆ—ãŒæ­£ã—ãåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
                # concatå¾Œã®df_combinedã«ã€is_highlightedãŒå­˜åœ¨ã—ãªã„æ—¢å­˜ã®è¡ŒãŒã‚ã‚‹å ´åˆã€NaNã«ãªã‚‹ã®ã§Falseã§åŸ‹ã‚ã‚‹
                df_combined['is_highlighted'] = df_combined['is_highlighted'].fillna(False)


                embeddings_combined = np.vstack([embeddings_existing, new_embedding])

                # UMAPã§æ¬¡å…ƒå‰Šæ¸›ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ï¼‰
                try:
                    if umap_reducer is None: 
                         st.info("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€æ–°ã—ã„ãƒ–ãƒ©ãƒ³ãƒ‰ã®ã¿ã§åŸ‹ã‚è¾¼ã¿ã‚’å†å­¦ç¿’ã—ã¾ã™ã€‚")
                         current_reducer = get_umap_reducer(embeddings_combined, visualization_method, n_anchors, lambda_anchor)
                         if current_reducer is not None:
                             if isinstance(current_reducer, AnchorBasedEmbedding):
                                 coords_2d_combined = current_reducer.fit_transform(embeddings_combined, df_combined['name'].tolist())
                             else:
                                 coords_2d_combined = current_reducer.fit_transform(embeddings_combined)
                         else:
                             st.error("åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€å¯è¦–åŒ–ã§ãã¾ã›ã‚“ã€‚")
                             st.stop()
                    else:
                        current_reducer = umap_reducer
                        # æ—¢å­˜ã®ãƒªãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
                        if hasattr(umap_reducer, 'transform'):
                            coords_2d_combined = umap_reducer.transform(embeddings_combined)
                        else:
                            # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã®å ´åˆã¯å†å­¦ç¿’ãŒå¿…è¦
                            if isinstance(umap_reducer, AnchorBasedEmbedding):
                                coords_2d_combined = umap_reducer.fit_transform(embeddings_combined, df_combined['name'].tolist())
                            else:
                                coords_2d_combined = umap_reducer.fit_transform(embeddings_combined)
                except Exception as e:
                    st.error(f"æ¬¡å…ƒå‰Šæ¸›å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    st.error("è¨­å®šã‚’èª¿æ•´ã™ã‚‹ã‹ã€åˆ¥ã®å¯è¦–åŒ–æ‰‹æ³•ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
                    st.stop()

                df_combined['umap_x'] = coords_2d_combined[:, 0]
                df_combined['umap_y'] = coords_2d_combined[:, 1]
                
                # UMAPã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å†å®Ÿè¡Œã—ã¦ã€æ–°ã—ã„ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’å«ã‚ã‚‹ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
                # ãƒ–ãƒ©ãƒ³ãƒ‰æ•°ã«å¿œã˜ã¦é©åˆ‡ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’è‡ªå‹•èª¿æ•´
                n_brands = len(df_combined)
                if n_brands < 10:
                    n_clusters = min(3, n_brands - 1)
                elif n_brands < 50:
                    n_clusters = min(8, n_brands // 3)
                elif n_brands < 200:
                    n_clusters = min(15, n_brands // 8)
                else:
                    n_clusters = min(25, n_brands // 12)  # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã‚ˆã‚Šå¤šãã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
                
                if n_clusters > 0:
                    try:
                        kmeans = KMeans(
                            n_clusters=n_clusters, 
                            random_state=42, 
                            n_init=10,  # å¤§é‡ãƒ‡ãƒ¼ã‚¿æ™‚ã¯è¨ˆç®—é‡ã‚’æŠ‘åˆ¶
                            max_iter=300,  # åæŸæ™‚é–“ã‚’çŸ­ç¸®
                            algorithm='elkan' if n_brands > 100 else 'lloyd'  # å¤§é‡ãƒ‡ãƒ¼ã‚¿æ™‚ã¯é«˜é€Ÿã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                        )
                        df_combined['cluster'] = kmeans.fit_predict(embeddings_combined)
                        st.success(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {n_clusters}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«åˆ†é¡ã—ã¾ã—ãŸ")
                    except Exception as e:
                        st.warning(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}. å˜ä¸€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
                        df_combined['cluster'] = 0
                else:
                    df_combined['cluster'] = 0

                # èª¬æ˜æ–‡ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ—ã‚’å†ç”Ÿæˆï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ã«ï¼‰
                try:
                    df_combined['description_preview'] = df_combined['description'].apply(
                        lambda x: (x[:50] + '...') if (isinstance(x, str) and len(x) > 50) else str(x)
                    )
                except Exception as e:
                    st.warning(f"èª¬æ˜æ–‡ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    df_combined['description_preview'] = df_combined['name']  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

                # æ‰‹æ³•æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯è¤‡æ•°ã®å¯è¦–åŒ–ã‚’è¡¨ç¤º
                if visualization_method == "æ‰‹æ³•æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰":
                    st.subheader("ğŸ”¬ è¤‡æ•°æ‰‹æ³•ã®æ¯”è¼ƒ")
                    
                    # è¤‡æ•°æ‰‹æ³•ã§åŸ‹ã‚è¾¼ã¿ã‚’å®Ÿè¡Œ
                    comparison_results = compare_embedding_methods(embeddings_combined, df_combined['name'].tolist())
                    
                    # ã‚¿ãƒ–ã§å„æ‰‹æ³•ã‚’è¡¨ç¤º
                    tabs = st.tabs(list(comparison_results.keys()))
                    
                    for tab, (method_name, result) in zip(tabs, comparison_results.items()):
                        with tab:
                            embedding = result['embedding']
                            df_temp = df_combined.copy()
                            df_temp['x'] = embedding[:, 0]
                            df_temp['y'] = embedding[:, 1]
                            
                            # å€‹åˆ¥ã®å¯è¦–åŒ–
                            fig_comp = create_visualization_plot(
                                df_temp, brand_name_input, method_name, 
                                x_col='x', y_col='y'
                            )
                            st.plotly_chart(fig_comp, use_container_width=True)
                            
                            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                            if 'metrics' in result:
                                metrics = result['metrics']
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("PCAåˆ†æ•£å¯„ä¸ç‡", f"{metrics.get('pca_variance_ratio', 0):.3f}")
                                with col2:
                                    st.metric("å¹³å‡ã‚¢ãƒ³ã‚«ãƒ¼é–“è·é›¢", f"{metrics.get('anchor_distance_mean', 0):.3f}")
                                with col3:
                                    st.metric("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…åˆ†æ•£", f"{metrics.get('intra_cluster_variance_mean', 0):.3f}")
                else:
                    # å˜ä¸€æ‰‹æ³•ã§ã®å¯è¦–åŒ–
                    fig = create_visualization_plot(
                        df_combined, brand_name_input, visualization_method
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã®å ´åˆã¯è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
                    if visualization_method == "ã‚¢ãƒ³ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿" and hasattr(current_reducer, 'get_anchor_info'):
                        anchor_info = current_reducer.get_anchor_info()
                        if anchor_info:
                            with st.expander("ğŸ“ ã‚¢ãƒ³ã‚«ãƒ¼æƒ…å ±", expanded=False):
                                anchor_df = pd.DataFrame({
                                    'ãƒ–ãƒ©ãƒ³ãƒ‰å': anchor_info['names'],
                                    'Xåº§æ¨™': anchor_info['coords'][:, 0],
                                    'Yåº§æ¨™': anchor_info['coords'][:, 1]
                                })
                                st.dataframe(anchor_df, use_container_width=True)
                                
                                st.markdown("**ã‚¢ãƒ³ã‚«ãƒ¼ã¨ã¯**: åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§ã®åŸºæº–ç‚¹ã¨ã—ã¦é¸ã°ã‚ŒãŸãƒ–ãƒ©ãƒ³ãƒ‰ã§ã™ã€‚")
                    
                    # åˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤º
                    if hasattr(current_reducer, 'get_analysis_metrics'):
                        metrics = current_reducer.get_analysis_metrics(embeddings_combined, coords_2d_combined)
                        if metrics:
                            st.subheader("ğŸ“Š åˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹")
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("PCAåˆ†æ•£å¯„ä¸ç‡", f"{metrics.get('pca_variance_ratio', 0):.3f}")
                            with col2:
                                st.metric("å¹³å‡ã‚¢ãƒ³ã‚«ãƒ¼é–“è·é›¢", f"{metrics.get('anchor_distance_mean', 0):.3f}")
                            with col3:
                                st.metric("ã‚¢ãƒ³ã‚«ãƒ¼é–“è·é›¢æ¨™æº–åå·®", f"{metrics.get('anchor_distance_std', 0):.3f}")
                            with col4:
                                st.metric("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…åˆ†æ•£", f"{metrics.get('intra_cluster_variance_mean', 0):.3f}")
                
                st.success(f"{brand_name_input}ãŒ{visualization_method}ãƒãƒƒãƒ—ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚")
        else:
            st.error("èª¬æ˜æ–‡ãŒç”Ÿæˆã•ã‚Œãªã‹ã£ãŸãŸã‚ã€ãƒãƒƒãƒ—ã«è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")



# # --- ã‚¢ãƒ—ãƒªã®å®Ÿè¡Œæ–¹æ³• ---
# st.markdown("---")
# st.subheader("ã‚¢ãƒ—ãƒªã®å®Ÿè¡Œæ–¹æ³•")
# st.code("""
# 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™:
#    pip install streamlit google-generativeai transformers torch umap-learn scikit-learn pandas tqdm

# 2. `GOOGLE_API_KEY` ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™:
#    export GOOGLE_API_KEY="YOUR_GEMINI_API_KEY_HERE"

# 3. Ruri v3ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ (ã‚‚ã—æœªå®Ÿæ–½ãªã‚‰):
#    git clone https://huggingface.co/cl-nagoya/ruri-v3-310m ./ruri_v3_downloaded_from_hub

# 4. Ruri v3ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç”Ÿæˆã—ã¾ã™ (åˆå›ã®ã¿):
#    python embedding.py

# 5. ã“ã® `app.py` ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§Streamlitã‚’å®Ÿè¡Œã—ã¾ã™:
#    streamlit run app.py
# """)